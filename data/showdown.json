{
	"meta": {
		"version": "2026.01.12",
		"last_update": "2026-01-12T03:50:27Z",
		"schema_version": "1.0"
	},
	"models": [
		{
			"id": "claude-opus-4-5-20251101",
			"name": "Claude Opus 4.5",
			"aka": ["claude-opus-4-5", "claude-opus-4.5", "claude-4.5-opus", "opus-4.5"],
			"provider": "Anthropic",
			"type": "proprietary",
			"release_date": "2025-11-24",
			"pricing": {
				"input_per_1m": 5,
				"output_per_1m": 25,
				"average_per_1m": 15
			},
			"performance": {
				"output_speed_tps": 65,
				"latency_ttft_ms": 3750,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Anthropic's most intelligent model. Excels at complex reasoning and advanced coding with extended autonomous operation. New pricing makes it more competitive.",
			"benchmark_scores": {
				"aime": 48.1,
				"arc_agi_2": 7.8,
				"bfcl": 82.5,
				"frontiermath": 20.7,
				"gpqa_diamond": 80.7,
				"humanity_last_exam": 14.16,
				"live_code_bench": 72.3,
				"livebench": 65.01,
				"lmarena_coding_elo": 1516,
				"lmarena_creative_elo": 1458,
				"lmarena_en_elo": 1477,
				"lmarena_hard_elo": 1494,
				"lmarena_if_elo": 1479,
				"lmarena_math_elo": 1467,
				"lmarena_vision_elo": 1188,
				"lmarena_zh_elo": 1484,
				"math_500": 88.5,
				"mathvista": 72.8,
				"mmlu_pro": 82,
				"mmmlu": 90.8,
				"simpleqa": 30,
				"mmmu": 80.7,
				"mmmu_pro": 80.7,
				"osworld": 61.4,
				"swe_bench": 74.6,
				"tau_bench": 64.2,
				"terminal_bench": 58.5,
				"webdev_arena_elo": 1479,
				"livebench_reasoning": 53.21,
				"livebench_coding": 78.51,
				"livebench_agentic_coding": 63.33,
				"livebench_math": 86.09,
				"livebench_data_analysis": 67.19,
				"livebench_language": 78.66,
				"livebench_if": 28.11
			},
			"imputed_metadata": {
				"simpleqa": {
					"original_value": null,
					"imputed_value": 30,
					"method": "estimated",
					"imputed_date": "2025-12-27",
					"note": "Estimated as ~70% of thinking variant (41.8)"
				}
			}
		},
		{
			"id": "claude-opus-4-5-20251101-thinking-32k",
			"name": "Claude Opus 4.5 Thinking",
			"aka": ["claude-opus-4.5-thinking", "claude-opus-4-5-thinking", "opus-4.5-thinking"],
			"superior_of": "claude-opus-4-5-20251101",
			"provider": "Anthropic",
			"type": "proprietary",
			"release_date": "2025-11-24",
			"pricing": {
				"input_per_1m": 5,
				"output_per_1m": 25,
				"average_per_1m": 15
			},
			"performance": {
				"output_speed_tps": 35,
				"latency_ttft_ms": 8500,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Extended thinking variant of Opus 4.5. Excels at complex reasoning tasks with visible chain-of-thought. Won LMArena Triple Crown (Expert, WebDev, Math). Higher latency due to deliberation.",
			"benchmark_scores": {
				"aime": 86.1,
				"arc_agi_2": 37.6,
				"bfcl": 80.9,
				"frontiermath": 20.7,
				"gpqa_diamond": 86,
				"humanity_last_exam": 25.2,
				"live_code_bench": 75.5,
				"livebench": 75.61,
				"lmarena_coding_elo": 1542,
				"lmarena_creative_elo": 1452,
				"lmarena_en_elo": 1486,
				"lmarena_hard_elo": 1504,
				"lmarena_if_elo": 1480,
				"lmarena_math_elo": 1478,
				"lmarena_vision_elo": 1210,
				"lmarena_zh_elo": 1481,
				"math_500": 94.5,
				"mathvista": 78.5,
				"mmlu_pro": 87.3,
				"mmmlu": 90.8,
				"simpleqa": 41.8,
				"mmmu": 83,
				"mmmu_pro": 80.7,
				"osworld": 66.3,
				"swe_bench": 80.9,
				"tau_bench": 93.6,
				"terminal_bench": 59.3,
				"webdev_arena_elo": 1511,
				"livebench_reasoning": 80.09,
				"livebench_coding": 79.65,
				"livebench_agentic_coding": 63.33,
				"livebench_math": 90.39,
				"livebench_data_analysis": 71.98,
				"livebench_language": 81.26,
				"livebench_if": 62.55
			}
		},
		{
			"id": "claude-sonnet-4-5-20250929",
			"name": "Claude Sonnet 4.5",
			"aka": ["claude-sonnet-4-5", "claude-sonnet-4.5", "sonnet-4.5"],
			"provider": "Anthropic",
			"type": "proprietary",
			"release_date": "2025-09-29",
			"pricing": {
				"input_per_1m": 3,
				"output_per_1m": 15,
				"average_per_1m": 9
			},
			"performance": {
				"output_speed_tps": 77,
				"latency_ttft_ms": 1200,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Best-in-class for software engineering. State-of-the-art on SWE-bench with excellent balance of speed and capability.",
			"benchmark_scores": {
				"aime": 35.6,
				"arc_agi_2": 3.8,
				"bfcl": 80.8,
				"frontiermath": 9.3,
				"gpqa_diamond": 73.7,
				"humanity_last_exam": 30,
				"live_code_bench": 74.5,
				"livebench": 82,
				"lmarena_coding_elo": 1506,
				"lmarena_creative_elo": 1439,
				"lmarena_en_elo": 1461,
				"lmarena_hard_elo": 1473,
				"lmarena_if_elo": 1451,
				"lmarena_math_elo": 1429,
				"lmarena_vision_elo": 1295,
				"lmarena_zh_elo": 1463,
				"math_500": 82.3,
				"mathvista": 68.5,
				"mmlu_pro": 78,
				"mmmlu": 82.8,
				"simpleqa": 13,
				"mmmu": 66.2,
				"mmmu_pro": null,
				"osworld": 44,
				"swe_bench": 77.2,
				"tau_bench": 58.5,
				"terminal_bench": 62.1,
				"webdev_arena_elo": 1387,
				"livebench_reasoning": 42.29,
				"livebench_coding": 76.07,
				"livebench_agentic_coding": 48.33,
				"livebench_math": 80.83,
				"livebench_data_analysis": 67.34,
				"livebench_language": 76,
				"livebench_if": 23.52
			}
		},
		{
			"id": "claude-sonnet-4-5-20250929-thinking-32k",
			"name": "Claude Sonnet 4.5 Thinking",
			"aka": ["claude-sonnet-4.5-thinking", "claude-sonnet-4-5-thinking", "sonnet-4.5-thinking"],
			"superior_of": "claude-sonnet-4-5-20250929",
			"provider": "Anthropic",
			"type": "proprietary",
			"release_date": "2025-09-29",
			"pricing": {
				"input_per_1m": 3,
				"output_per_1m": 15,
				"average_per_1m": 9
			},
			"performance": {
				"output_speed_tps": 45,
				"latency_ttft_ms": 5500,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Extended thinking variant of Sonnet 4.5. Excels at complex reasoning tasks with visible chain-of-thought. Higher latency due to deliberation but improved math and coding benchmarks.",
			"benchmark_scores": {
				"aime": 77.8,
				"arc_agi_2": 13.6,
				"bfcl": 83.5,
				"frontiermath": 15.2,
				"gpqa_diamond": 81.7,
				"humanity_last_exam": 35,
				"live_code_bench": 77.2,
				"livebench": 70.31,
				"lmarena_coding_elo": 1524,
				"lmarena_creative_elo": 1440,
				"lmarena_en_elo": 1468,
				"lmarena_hard_elo": 1485,
				"lmarena_if_elo": 1462,
				"lmarena_math_elo": 1462,
				"lmarena_vision_elo": 1204,
				"lmarena_zh_elo": 1460,
				"math_500": 90.5,
				"mathvista": 74.5,
				"mmlu_pro": 87.4,
				"mmmlu": 84.5,
				"simpleqa": 23.6,
				"mmmu": 70.2,
				"mmmu_pro": null,
				"osworld": 55.8,
				"swe_bench": 77.2,
				"tau_bench": 62.5,
				"terminal_bench": 63.5,
				"webdev_arena_elo": 1393,
				"livebench_reasoning": 77.59,
				"livebench_coding": 80.36,
				"livebench_agentic_coding": 53.33,
				"livebench_math": 79.31,
				"livebench_data_analysis": 71.76,
				"livebench_language": 76.45,
				"livebench_if": 53.35
			}
		},
		{
			"id": "claude-opus-4-1-20250805",
			"name": "Claude Opus 4.1",
			"provider": "Anthropic",
			"type": "proprietary",
			"release_date": "2025-08-05",
			"pricing": {
				"input_per_1m": 15,
				"output_per_1m": 75,
				"average_per_1m": 45
			},
			"performance": {
				"output_speed_tps": 52,
				"latency_ttft_ms": 4200,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Previous flagship with excellent sustained autonomous operation up to 30 hours. Strong at complex multi-step reasoning but higher cost.",
			"benchmark_scores": {
				"aime": 40,
				"arc_agi_2": 8.1,
				"bfcl": 78.5,
				"frontiermath": 5.9,
				"gpqa_diamond": 73.2,
				"humanity_last_exam": null,
				"live_code_bench": 68.2,
				"livebench": null,
				"lmarena_coding_elo": 1499,
				"lmarena_creative_elo": 1435,
				"lmarena_en_elo": 1455,
				"lmarena_hard_elo": 1472,
				"lmarena_if_elo": 1448,
				"lmarena_math_elo": 1434,
				"lmarena_vision_elo": 1285,
				"lmarena_zh_elo": 1451,
				"math_500": 80.2,
				"mathvista": 68.2,
				"mmlu_pro": null,
				"mmmlu": 81.5,
				"simpleqa": 34.8,
				"mmmu": 65.5,
				"mmmu_pro": null,
				"osworld": 58.2,
				"swe_bench": 74.5,
				"tau_bench": 62.8,
				"terminal_bench": 54.2,
				"webdev_arena_elo": 1386,
				"livebench_reasoning": 40.89,
				"livebench_coding": 76.07,
				"livebench_agentic_coding": 53.33,
				"livebench_math": 80.44,
				"livebench_data_analysis": 66.95,
				"livebench_language": 76.75,
				"livebench_if": 25.92
			}
		},
		{
			"id": "gpt-4o-2024-05-13",
			"name": "GPT-4o",
			"provider": "OpenAI",
			"type": "proprietary",
			"release_date": "2024-05-13",
			"pricing": {
				"input_per_1m": 2.5,
				"output_per_1m": 10,
				"average_per_1m": 6.25
			},
			"performance": {
				"output_speed_tps": 110,
				"latency_ttft_ms": 320,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Excellent multimodal capabilities with very fast inference. Good value for general-purpose tasks but superseded by newer models on benchmarks.",
			"benchmark_scores": {
				"aime": 6.3,
				"arc_agi_2": 5.5,
				"bfcl": 76.2,
				"frontiermath": 0.3,
				"gpqa_diamond": 48.9,
				"humanity_last_exam": 26.6,
				"live_code_bench": 48.2,
				"livebench": null,
				"lmarena_coding_elo": 1366,
				"lmarena_creative_elo": 1336,
				"lmarena_en_elo": 1335,
				"lmarena_hard_elo": 1335,
				"lmarena_if_elo": 1320,
				"lmarena_math_elo": 1307,
				"lmarena_vision_elo": 1162,
				"lmarena_zh_elo": 1331,
				"math_500": 76.6,
				"mathvista": 63.8,
				"mmlu_pro": 72.6,
				"mmmlu": 78.2,
				"simpleqa": 8,
				"mmmu": 58.5,
				"mmmu_pro": null,
				"osworld": 32.5,
				"swe_bench": 30.8,
				"tau_bench": 45.5,
				"terminal_bench": 28.5,
				"webdev_arena_elo": 1253,
				"livebench_reasoning": null,
				"livebench_coding": null,
				"livebench_agentic_coding": null,
				"livebench_math": null,
				"livebench_data_analysis": null,
				"livebench_language": null,
				"livebench_if": null
			},
			"imputed_metadata": {
				"simpleqa": {
					"original_value": null,
					"imputed_value": 8,
					"method": "estimated",
					"imputed_date": "2025-12-27",
					"note": "Estimated based on older model performance"
				}
			}
		},
		{
			"id": "gpt-5-high",
			"name": "GPT-5 High",
			"aka": ["gpt-5-high", "gpt-5-high-effort", "gpt5-high"],
			"superior_of": "gpt-4o-2024-05-13",
			"provider": "OpenAI",
			"type": "proprietary",
			"release_date": "2025-10-15",
			"pricing": {
				"input_per_1m": 1.25,
				"output_per_1m": 10,
				"average_per_1m": 5.625
			},
			"performance": {
				"output_speed_tps": 40,
				"latency_ttft_ms": 1600,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "High-reasoning variant of GPT-5. Excellent performance on AIME 2025 (94.6%) and GPQA Diamond (85.7%).",
			"benchmark_scores": {
				"aime": 94.6,
				"arc_agi_2": 9.9,
				"bfcl": null,
				"frontiermath": 26.3,
				"gpqa_diamond": 85.7,
				"humanity_last_exam": 26.3,
				"live_code_bench": null,
				"livebench": null,
				"lmarena_coding_elo": 1470,
				"lmarena_creative_elo": 1381,
				"lmarena_en_elo": 1441,
				"lmarena_hard_elo": 1448,
				"lmarena_if_elo": 1411,
				"lmarena_math_elo": 1439,
				"lmarena_vision_elo": 1210,
				"lmarena_zh_elo": 1455,
				"math_500": null,
				"mathvista": null,
				"mmlu_pro": null,
				"mmmlu": null,
				"simpleqa": null,
				"mmmu": 84.2,
				"mmmu_pro": null,
				"osworld": null,
				"swe_bench": 78.7,
				"tau_bench": null,
				"terminal_bench": null,
				"webdev_arena_elo": 1398,
				"livebench_reasoning": null,
				"livebench_coding": null,
				"livebench_agentic_coding": null,
				"livebench_math": null,
				"livebench_data_analysis": null,
				"livebench_language": null,
				"livebench_if": null
			}
		},
		{
			"id": "gpt-5.1-high",
			"name": "GPT 5.1 High",
			"aka": ["gpt-5.1-thinking", "gpt-5-1-high", "gpt5.1-high", "gpt-5.1-high-effort"],
			"superior_of": "gpt-5-high",
			"provider": "OpenAI",
			"type": "proprietary",
			"release_date": "2025-11-12",
			"pricing": {
				"input_per_1m": 15,
				"output_per_1m": 120,
				"average_per_1m": 67.5
			},
			"performance": {
				"output_speed_tps": 40,
				"latency_ttft_ms": 1600,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "AKA GPT-5.1 Thinking (high effort). Extended thinking variant with high reasoning effort. Scores 69 on Artificial Analysis Index. Higher latency and cost but excels on complex reasoning tasks.",
			"benchmark_scores": {
				"aime": 88.6,
				"arc_agi_2": 17.6,
				"bfcl": 84.5,
				"frontiermath": 31,
				"gpqa_diamond": 87.6,
				"humanity_last_exam": 25.32,
				"live_code_bench": 78,
				"livebench": 72.39,
				"lmarena_coding_elo": 1492,
				"lmarena_creative_elo": 1436,
				"lmarena_en_elo": 1465,
				"lmarena_hard_elo": 1474,
				"lmarena_if_elo": 1446,
				"lmarena_math_elo": 1473,
				"lmarena_vision_elo": 1247,
				"lmarena_zh_elo": 1493,
				"math_500": 92,
				"mathvista": 72.5,
				"mmlu_pro": 86.9,
				"mmmlu": 91,
				"simpleqa": 48.9,
				"mmmu": 85.4,
				"mmmu_pro": 76,
				"osworld": null,
				"swe_bench": 76.3,
				"tau_bench": null,
				"terminal_bench": 47.6,
				"webdev_arena_elo": 1392,
				"livebench_reasoning": 78.79,
				"livebench_coding": 72.49,
				"livebench_agentic_coding": 53.33,
				"livebench_math": 86.9,
				"livebench_data_analysis": 72.07,
				"livebench_language": 79.26,
				"livebench_if": 63.9
			}
		},
		{
			"id": "gpt-5.2-high",
			"name": "GPT 5.2 High",
			"aka": ["gpt-5.2-thinking", "gpt-5.2-high-effort", "gpt-5-2-thinking", "gpt-5-2-high"],
			"superior_of": "gpt-5.1-high",
			"provider": "OpenAI",
			"type": "proprietary",
			"release_date": "2025-12-11",
			"pricing": {
				"input_per_1m": 1.75,
				"output_per_1m": 14,
				"average_per_1m": 7.875
			},
			"performance": {
				"output_speed_tps": 45,
				"latency_ttft_ms": 1200,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "AKA GPT-5.2 Thinking (high effort). Extended thinking variant with 100% on AIME 2025. Top reasoning performance with 43.3% ARC-AGI-2 Verified. Achieves 80% on SWE-Bench and 55.6% on SWE-Bench Pro.",
			"benchmark_scores": {
				"aime": 96.1,
				"arc_agi_2": 52.9,
				"bfcl": 88.5,
				"frontiermath": 40.3,
				"gpqa_diamond": 92.4,
				"humanity_last_exam": 34.5,
				"live_code_bench": 78.5,
				"livebench": 74.07,
				"lmarena_coding_elo": 1494,
				"lmarena_creative_elo": 1386,
				"lmarena_en_elo": 1451,
				"lmarena_hard_elo": 1461,
				"lmarena_if_elo": 1436,
				"lmarena_math_elo": 1501,
				"lmarena_vision_elo": 1280,
				"lmarena_zh_elo": 1468,
				"math_500": 94.5,
				"mathvista": 78.5,
				"mmlu_pro": 86.2,
				"mmmlu": 89.5,
				"simpleqa": 38.2,
				"mmmu": 76.5,
				"mmmu_pro": 86.5,
				"osworld": 58.5,
				"swe_bench": 76.2,
				"tau_bench": 98.7,
				"terminal_bench": 52.5,
				"webdev_arena_elo": 1481,
				"livebench_reasoning": 83.21,
				"livebench_coding": 76.07,
				"livebench_agentic_coding": 51.67,
				"livebench_math": 93.17,
				"livebench_data_analysis": 72.79,
				"livebench_language": 79.81,
				"livebench_if": 61.77
			}
		},
		{
			"id": "gpt-5.2-pro",
			"name": "GPT 5.2 Pro",
			"aka": ["gpt-5.2-xhigh", "gpt-5.2-correlates", "gpt-5-2-pro", "gpt-5-2-xhigh"],
			"superior_of": "gpt-5.2-high",
			"disabled": true,
			"provider": "OpenAI",
			"type": "proprietary",
			"release_date": "2025-12-11",
			"pricing": {
				"input_per_1m": 15,
				"output_per_1m": 120,
				"average_per_1m": 67.5
			},
			"performance": {
				"output_speed_tps": 28,
				"latency_ttft_ms": 2800,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "AKA GPT-5.2 xhigh (correlates). Premium tier with highest accuracy. First model to score 93.2% GPQA Diamond and 54.2% ARC-AGI-2 Verified. Peak performance for high-stakes scenarios. 70.9% match on GDPval knowledge tasks.",
			"benchmark_scores": {
				"aime": 96.1,
				"arc_agi_2": 54.2,
				"bfcl": 90.5,
				"frontiermath": 40.7,
				"gpqa_diamond": 93.2,
				"humanity_last_exam": 36.6,
				"live_code_bench": 82,
				"livebench": 73.82,
				"lmarena_coding_elo": null,
				"lmarena_creative_elo": null,
				"lmarena_en_elo": null,
				"lmarena_hard_elo": null,
				"lmarena_if_elo": null,
				"lmarena_math_elo": null,
				"lmarena_vision_elo": null,
				"lmarena_zh_elo": null,
				"math_500": 96.5,
				"mathvista": 82.5,
				"mmlu_pro": null,
				"mmmlu": 91.5,
				"simpleqa": 38.9,
				"mmmu": 80.5,
				"mmmu_pro": 86.5,
				"osworld": 62.5,
				"swe_bench": 82.5,
				"tau_bench": 99.2,
				"terminal_bench": 55.5,
				"webdev_arena_elo": 1484,
				"livebench_reasoning": 81.69,
				"livebench_coding": 72.11,
				"livebench_agentic_coding": 51.67,
				"livebench_math": 94.22,
				"livebench_data_analysis": 72.42,
				"livebench_language": 80.69,
				"livebench_if": 63.96
			}
		},
		{
			"id": "gemini-3-pro",
			"name": "Gemini 3 Pro",
			"aka": ["gemini-3.0-pro", "gemini3-pro"],
			"provider": "Google",
			"type": "proprietary",
			"release_date": "2025-11-18",
			"pricing": {
				"input_per_1m": 2,
				"output_per_1m": 12,
				"average_per_1m": 7
			},
			"performance": {
				"output_speed_tps": 128,
				"latency_ttft_ms": 680,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "First model to break 1500 Elo on LMArena. Exceptional at algorithmic/competitive programming with breakthrough reasoning performance.",
			"benchmark_scores": {
				"aime": 91.4,
				"arc_agi_2": 31.1,
				"bfcl": 82.8,
				"frontiermath": 37.6,
				"gpqa_diamond": 91.9,
				"humanity_last_exam": 37.5,
				"live_code_bench": 78.5,
				"livebench": 73.46,
				"lmarena_coding_elo": 1519,
				"lmarena_creative_elo": 1489,
				"lmarena_en_elo": 1492,
				"lmarena_hard_elo": 1504,
				"lmarena_if_elo": 1475,
				"lmarena_math_elo": 1478,
				"lmarena_vision_elo": 1302,
				"lmarena_zh_elo": 1518,
				"math_500": 92.8,
				"mathvista": 78.5,
				"mmlu_pro": 90.1,
				"mmmlu": 91.8,
				"simpleqa": 72.9,
				"mmmu": 75.2,
				"mmmu_pro": 81,
				"osworld": 48.5,
				"swe_bench": 79.2,
				"tau_bench": 85.4,
				"terminal_bench": 54.2,
				"webdev_arena_elo": 1468,
				"livebench_reasoning": 77.42,
				"livebench_coding": 74.6,
				"livebench_agentic_coding": 55,
				"livebench_math": 94.12,
				"livebench_data_analysis": 74.91,
				"livebench_language": 84.62,
				"livebench_if": 65.85
			}
		},
		{
			"id": "gemini-3-flash",
			"name": "Gemini 3 Flash",
			"aka": ["gemini-3.0-flash", "gemini3-flash", "gemini-3-flash-non-thinking"],
			"provider": "Google",
			"type": "proprietary",
			"release_date": "2025-12-17",
			"pricing": {
				"input_per_1m": 0.5,
				"output_per_1m": 3,
				"average_per_1m": 1.75
			},
			"performance": {
				"output_speed_tps": 218,
				"latency_ttft_ms": 200,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Fastest intelligent model at 218 TPS. Minimal thinking mode. 3x faster than 2.5 Pro with 30% fewer tokens.",
			"benchmark_scores": {
				"aime": 92.8,
				"arc_agi_2": 33.6,
				"bfcl": null,
				"frontiermath": 35.6,
				"gpqa_diamond": 90.4,
				"humanity_last_exam": 33.7,
				"live_code_bench": null,
				"livebench": 72.4,
				"lmarena_coding_elo": 1499,
				"lmarena_creative_elo": 1466,
				"lmarena_en_elo": 1464,
				"lmarena_hard_elo": 1483,
				"lmarena_if_elo": 1445,
				"lmarena_math_elo": 1473,
				"lmarena_vision_elo": 1274,
				"lmarena_zh_elo": 1530,
				"math_500": null,
				"mathvista": null,
				"mmlu_pro": 88.6,
				"mmmlu": 91.8,
				"simpleqa": 67.4,
				"mmmu": 81.2,
				"mmmu_pro": 81.2,
				"osworld": null,
				"swe_bench": 70.2,
				"tau_bench": 90.2,
				"terminal_bench": null,
				"webdev_arena_elo": 1455,
				"livebench_reasoning": 74.55,
				"livebench_coding": 73.9,
				"livebench_agentic_coding": 40,
				"livebench_math": 93.57,
				"livebench_data_analysis": 74.74,
				"livebench_language": 84.56,
				"livebench_if": 74.86
			}
		},
		{
			"id": "gemini-3-flash-thinking",
			"name": "Gemini 3 Flash Thinking",
			"aka": ["gemini-3-flash-thinking-mode", "gemini3-flash-thinking", "gemini-3-flash-preview"],
			"superior_of": "gemini-3-flash",
			"provider": "Google",
			"type": "proprietary",
			"release_date": "2025-12-17",
			"pricing": {
				"input_per_1m": 0.5,
				"output_per_1m": 3,
				"average_per_1m": 1.75
			},
			"performance": {
				"output_speed_tps": 180,
				"latency_ttft_ms": 400,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Thinking mode of Gemini 3 Flash. Outperforms even Gemini 3 Pro on SWE-Bench (78%) at quarter the cost. Uses thinking_level parameter for controlled reasoning. #2 on LMArena Overall.",
			"benchmark_scores": {
				"aime": 92.8,
				"arc_agi_2": 36,
				"bfcl": null,
				"frontiermath": 35.6,
				"gpqa_diamond": 83.2,
				"humanity_last_exam": 43.5,
				"live_code_bench": null,
				"livebench": 73.74,
				"lmarena_coding_elo": 1504,
				"lmarena_creative_elo": 1466,
				"lmarena_en_elo": 1473,
				"lmarena_hard_elo": 1488,
				"lmarena_if_elo": 1452,
				"lmarena_math_elo": 1482,
				"lmarena_vision_elo": 1264,
				"lmarena_zh_elo": 1530,
				"math_500": null,
				"mathvista": null,
				"mmlu_pro": 88.6,
				"mmmlu": null,
				"simpleqa": 67.4,
				"mmmu": 82,
				"mmmu_pro": null,
				"osworld": null,
				"swe_bench": 74.0,
				"tau_bench": 92,
				"terminal_bench": null,
				"webdev_arena_elo": 1455,
				"livebench_reasoning": 74.55,
				"livebench_coding": 73.9,
				"livebench_agentic_coding": 40,
				"livebench_math": 93.57,
				"livebench_data_analysis": 74.74,
				"livebench_language": 84.56,
				"livebench_if": 74.86
			},
			"imputed_metadata": {
				"simpleqa": {
					"original_value": null,
					"imputed_value": 67.4,
					"method": "estimated",
					"imputed_date": "2025-12-27",
					"note": "Using Gemini 3 Flash base value"
				}
			}
		},
		{
			"id": "gemini-2.5-pro",
			"name": "Gemini 2.5 Pro",
			"provider": "Google",
			"type": "proprietary",
			"release_date": "2025-03-25",
			"pricing": {
				"input_per_1m": 1.25,
				"output_per_1m": 5,
				"average_per_1m": 3.13
			},
			"performance": {
				"output_speed_tps": 165,
				"latency_ttft_ms": 520,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Excellent value with 1M token context window. Strong reasoning at lower cost but superseded by Gemini 3 on cutting-edge tasks.",
			"benchmark_scores": {
				"aime": 84.2,
				"arc_agi_2": 4.9,
				"bfcl": 79.5,
				"frontiermath": 14.1,
				"gpqa_diamond": 85.3,
				"humanity_last_exam": 26.9,
				"live_code_bench": 65.5,
				"livebench": 78,
				"lmarena_coding_elo": 1469,
				"lmarena_creative_elo": 1451,
				"lmarena_en_elo": 1454,
				"lmarena_hard_elo": 1462,
				"lmarena_if_elo": 1444,
				"lmarena_math_elo": 1453,
				"lmarena_vision_elo": 1249,
				"lmarena_zh_elo": 1497,
				"math_500": 85.5,
				"mathvista": 72.5,
				"mmlu_pro": 86.7,
				"mmmlu": 84.2,
				"simpleqa": 56,
				"mmmu": 68.8,
				"mmmu_pro": null,
				"osworld": 42.5,
				"swe_bench": 63.8,
				"tau_bench": 52.8,
				"terminal_bench": 45.2,
				"webdev_arena_elo": 1209,
				"livebench_reasoning": 70.81,
				"livebench_coding": 75.69,
				"livebench_agentic_coding": 33.33,
				"livebench_math": 83.09,
				"livebench_data_analysis": 71.5,
				"livebench_language": 75.5,
				"livebench_if": 33.07
			}
		},
		{
			"id": "deepseek-v3.1",
			"name": "DeepSeek V3.1",
			"provider": "DeepSeek",
			"type": "open-source",
			"release_date": "2025-08-15",
			"pricing": {
				"input_per_1m": 0.27,
				"output_per_1m": 1.1,
				"average_per_1m": 0.69
			},
			"performance": {
				"output_speed_tps": 120,
				"latency_ttft_ms": 450,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Outstanding value as open-source model. Strong coding performance at fraction of proprietary model costs. Excellent for tool use.",
			"benchmark_scores": {
				"aime": 87.8,
				"arc_agi_2": 8.5,
				"bfcl": 80.5,
				"frontiermath": 22.1,
				"gpqa_diamond": 74.9,
				"humanity_last_exam": 30.0,
				"live_code_bench": 56.4,
				"livebench": 74.8,
				"lmarena_coding_elo": 1447,
				"lmarena_creative_elo": 1393,
				"lmarena_en_elo": 1430,
				"lmarena_hard_elo": 1431,
				"lmarena_if_elo": 1401,
				"lmarena_math_elo": 1418,
				"lmarena_vision_elo": 1265,
				"lmarena_zh_elo": 1463,
				"math_500": 82.8,
				"mathvista": 65.5,
				"mmlu_pro": 83.7,
				"mmmlu": 82.5,
				"simpleqa": 27.5,
				"mmmu": 62.5,
				"mmmu_pro": 83.7,
				"osworld": 38.5,
				"swe_bench": 66.0,
				"tau_bench": 55.2,
				"terminal_bench": 31.0,
				"webdev_arena_elo": 1285,
				"livebench_reasoning": null,
				"livebench_coding": null,
				"livebench_agentic_coding": null,
				"livebench_math": null,
				"livebench_data_analysis": null,
				"livebench_language": null,
				"livebench_if": null
			}
		},
		{
			"id": "deepseek-v3.1-thinking",
			"name": "DeepSeek V3.1 Thinking",
			"superior_of": "deepseek-v3.1",
			"provider": "DeepSeek",
			"type": "open-source",
			"release_date": "2025-08-15",
			"pricing": {
				"input_per_1m": 0.27,
				"output_per_1m": 1.1,
				"average_per_1m": 0.69
			},
			"performance": {
				"output_speed_tps": 60,
				"latency_ttft_ms": 1000,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Thinking variant of DeepSeek V3.1 with enhanced reasoning capabilities.",
			"benchmark_scores": {
				"aime": 87.8,
				"arc_agi_2": 8.5,
				"bfcl": 80.5,
				"frontiermath": 22.1,
				"gpqa_diamond": 80.1,
				"humanity_last_exam": 30.0,
				"live_code_bench": 56.4,
				"livebench": 74.8,
				"lmarena_coding_elo": 1437,
				"lmarena_creative_elo": 1400,
				"lmarena_en_elo": 1418,
				"lmarena_hard_elo": 1440,
				"lmarena_if_elo": 1410,
				"lmarena_math_elo": 1425,
				"lmarena_vision_elo": 1265,
				"lmarena_zh_elo": 1470,
				"math_500": 78.5,
				"mathvista": 65.5,
				"mmlu_pro": 84.8,
				"mmmlu": 83.5,
				"simpleqa": 27.5,
				"mmmu": 62.5,
				"mmmu_pro": 83.7,
				"osworld": 38.5,
				"swe_bench": 66.0,
				"tau_bench": 55.2,
				"terminal_bench": 31.0,
				"webdev_arena_elo": 1290,
				"livebench_reasoning": null,
				"livebench_coding": null,
				"livebench_agentic_coding": null,
				"livebench_math": null,
				"livebench_data_analysis": null,
				"livebench_language": null,
				"livebench_if": null
			}
		},
		{
			"id": "deepseek-r1",
			"name": "DeepSeek R1",
			"provider": "DeepSeek",
			"type": "open-source",
			"release_date": "2025-05-28",
			"pricing": {
				"input_per_1m": 0.55,
				"output_per_1m": 2.19,
				"average_per_1m": 1.37
			},
			"performance": {
				"output_speed_tps": 85,
				"latency_ttft_ms": 1200,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Reasoning-focused model with excellent math performance. Competitive with o3 on GPQA at fraction of cost.",
			"benchmark_scores": {
				"aime": 87.5,
				"arc_agi_2": 1.3,
				"bfcl": 75.8,
				"frontiermath": 1.7,
				"gpqa_diamond": 81.0,
				"humanity_last_exam": 17.7,
				"live_code_bench": 73.3,
				"livebench": 73.3,
				"lmarena_coding_elo": 1443,
				"lmarena_creative_elo": 1373,
				"lmarena_en_elo": 1412,
				"lmarena_hard_elo": 1416,
				"lmarena_if_elo": 1392,
				"lmarena_math_elo": 1413,
				"lmarena_vision_elo": 1275,
				"lmarena_zh_elo": 1419,
				"math_500": 97.3,
				"mathvista": 68.2,
				"mmlu_pro": 85.0,
				"mmmlu": 84,
				"simpleqa": 27.8,
				"mmmu": 64.5,
				"mmmu_pro": 85,
				"osworld": 35.2,
				"swe_bench": 57.6,
				"tau_bench": 58.7,
				"terminal_bench": 42.5,
				"webdev_arena_elo": 1265,
				"livebench_reasoning": null,
				"livebench_coding": null,
				"livebench_agentic_coding": null,
				"livebench_math": null,
				"livebench_data_analysis": null,
				"livebench_language": null,
				"livebench_if": null
			}
		},
		{
			"id": "grok-4.1",
			"name": "Grok 4.1",
			"aka": ["grok-4-1", "grok4.1"],
			"provider": "xAI",
			"type": "proprietary",
			"release_date": "2025-11-17",
			"pricing": {
				"input_per_1m": 3,
				"output_per_1m": 15,
				"average_per_1m": 9
			},
			"performance": {
				"output_speed_tps": 95,
				"latency_ttft_ms": 850,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "xAI's flagship successor to Grok 4. #2 on LMArena Overall (1465 Elo). 3x reduction in hallucinations vs Grok 4. Excels at emotional intelligence and conversation.",
			"benchmark_scores": {
				"aime": 84,
				"arc_agi_2": 15.9,
				"bfcl": 83.5,
				"frontiermath": 19.7,
				"gpqa_diamond": 87,
				"humanity_last_exam": 44.4,
				"live_code_bench": 70.5,
				"livebench": null,
				"lmarena_coding_elo": 1463,
				"lmarena_creative_elo": 1425,
				"lmarena_en_elo": 1463,
				"lmarena_hard_elo": 1478,
				"lmarena_if_elo": 1431,
				"lmarena_math_elo": 1435,
				"lmarena_vision_elo": null,
				"lmarena_zh_elo": 1511,
				"math_500": 97.2,
				"mathvista": 76.5,
				"mmlu_pro": 85,
				"mmmlu": 87.5,
				"simpleqa": 47.9,
				"mmmu": 73,
				"mmmu_pro": 72.7,
				"osworld": 48,
				"swe_bench": 76.5,
				"tau_bench": 58.5,
				"terminal_bench": 54,
				"webdev_arena_elo": 1335,
				"livebench_reasoning": null,
				"livebench_coding": null,
				"livebench_agentic_coding": null,
				"livebench_math": null,
				"livebench_data_analysis": null,
				"livebench_language": null,
				"livebench_if": null
			}
		},
		{
			"id": "grok-4.1-thinking",
			"name": "Grok 4.1 Thinking",
			"aka": ["grok-4.1-think", "grok-4-1-thinking", "grok4.1-thinking"],
			"superior_of": "grok-4.1",
			"provider": "xAI",
			"type": "proprietary",
			"release_date": "2025-11-17",
			"pricing": {
				"input_per_1m": 3,
				"output_per_1m": 15,
				"average_per_1m": 9
			},
			"performance": {
				"output_speed_tps": 45,
				"latency_ttft_ms": 2200,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Extended thinking variant of Grok 4.1. #1 on LMArena Overall (1483 Elo). Deep reasoning with chain-of-thought. Top-tier emotional intelligence (1586 EQ-Bench Elo).",
			"benchmark_scores": {
				"aime": 84,
				"arc_agi_2": 26,
				"bfcl": 85.5,
				"frontiermath": 19.7,
				"gpqa_diamond": 87,
				"humanity_last_exam": 50.7,
				"live_code_bench": 74.5,
				"livebench": null,
				"lmarena_coding_elo": 1508,
				"lmarena_creative_elo": 1437,
				"lmarena_en_elo": 1483,
				"lmarena_hard_elo": 1490,
				"lmarena_if_elo": 1437,
				"lmarena_math_elo": 1455,
				"lmarena_vision_elo": null,
				"lmarena_zh_elo": 1500,
				"math_500": 98,
				"mathvista": 79.5,
				"mmlu_pro": 88,
				"mmmlu": 88.5,
				"simpleqa": 47.9,
				"mmmu": 76,
				"mmmu_pro": null,
				"osworld": 52.5,
				"swe_bench": 75.8,
				"tau_bench": 62.5,
				"terminal_bench": 56.5,
				"webdev_arena_elo": 1202,
				"livebench_reasoning": null,
				"livebench_coding": null,
				"livebench_agentic_coding": null,
				"livebench_math": null,
				"livebench_data_analysis": null,
				"livebench_language": null,
				"livebench_if": null
			},
			"imputed_metadata": {
				"simpleqa": {
					"original_value": null,
					"imputed_value": 47.9,
					"method": "estimated",
					"imputed_date": "2025-12-27",
					"note": "Using Grok 4.1 base value"
				}
			}
		},
		{
			"id": "llama-4-maverick-17b-128e-instruct",
			"name": "Llama 4 Maverick",
			"provider": "Meta",
			"type": "open-source",
			"release_date": "2025-04-05",
			"pricing": {
				"input_per_1m": 0,
				"output_per_1m": 0,
				"average_per_1m": 0
			},
			"performance": {
				"output_speed_tps": 155,
				"latency_ttft_ms": 380,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Open-weights model with strong coding. Excellent LiveCodeBench performance. Free for self-hosting, competitive with closed models.",
			"benchmark_scores": {
				"aime": 20.6,
				"arc_agi_2": 7.8,
				"bfcl": 77.6,
				"frontiermath": null,
				"gpqa_diamond": 67,
				"humanity_last_exam": 20,
				"live_code_bench": 43.4,
				"livebench": null,
				"lmarena_coding_elo": 1371,
				"lmarena_creative_elo": 1307,
				"lmarena_en_elo": 1342,
				"lmarena_hard_elo": 1338,
				"lmarena_if_elo": 1310,
				"lmarena_math_elo": 1324,
				"lmarena_vision_elo": 1146,
				"lmarena_zh_elo": 1310,
				"math_500": 61.2,
				"mathvista": 58.5,
				"mmlu_pro": 62.9,
				"mmmlu": 78.5,
				"simpleqa": 15,
				"mmmu": 55.2,
				"mmmu_pro": null,
				"osworld": 32.5,
				"swe_bench": 58.5,
				"tau_bench": 48.5,
				"terminal_bench": 38.5,
				"webdev_arena_elo": 1255,
				"livebench_reasoning": null,
				"livebench_coding": null,
				"livebench_agentic_coding": null,
				"livebench_math": null,
				"livebench_data_analysis": null,
				"livebench_language": null,
				"livebench_if": null
			},
			"imputed_metadata": {
				"simpleqa": {
					"original_value": null,
					"imputed_value": 15,
					"method": "estimated",
					"imputed_date": "2025-12-27",
					"note": "Estimated for open-source model"
				}
			}
		},
		{
			"id": "llama-4-scout-17b-16e-instruct",
			"name": "Llama 4 Scout",
			"provider": "Meta",
			"type": "open-source",
			"release_date": "2025-04-05",
			"pricing": {
				"input_per_1m": 0,
				"output_per_1m": 0,
				"average_per_1m": 0
			},
			"performance": {
				"output_speed_tps": 2600,
				"latency_ttft_ms": 120,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Blazing fast with 10M token context. Exceptional speed at 2600 tokens/s. Ideal for high-throughput applications.",
			"benchmark_scores": {
				"aime": 7.8,
				"arc_agi_2": 5.2,
				"bfcl": 72.5,
				"frontiermath": null,
				"gpqa_diamond": 51.8,
				"humanity_last_exam": null,
				"live_code_bench": 35.5,
				"livebench": null,
				"lmarena_coding_elo": 1361,
				"lmarena_creative_elo": 1290,
				"lmarena_en_elo": 1342,
				"lmarena_hard_elo": 1328,
				"lmarena_if_elo": 1296,
				"lmarena_math_elo": 1316,
				"lmarena_vision_elo": 1125,
				"lmarena_zh_elo": 1305,
				"math_500": 52.5,
				"mathvista": 48.5,
				"mmlu_pro": 58.2,
				"mmmlu": 72.5,
				"simpleqa": 10,
				"mmmu": 45.5,
				"mmmu_pro": null,
				"osworld": 25.5,
				"swe_bench": 42.5,
				"tau_bench": 38.5,
				"terminal_bench": 28.5,
				"webdev_arena_elo": 1205,
				"livebench_reasoning": null,
				"livebench_coding": null,
				"livebench_agentic_coding": null,
				"livebench_math": null,
				"livebench_data_analysis": null,
				"livebench_language": null,
				"livebench_if": null
			},
			"imputed_metadata": {
				"simpleqa": {
					"original_value": null,
					"imputed_value": 10,
					"method": "estimated",
					"imputed_date": "2025-12-27",
					"note": "Estimated for smaller open-source model"
				}
			}
		},
		{
			"id": "qwen3-235b-a22b-instruct-2507",
			"name": "Qwen3 235B",
			"provider": "Alibaba",
			"type": "open-source",
			"release_date": "2025-04-29",
			"pricing": {
				"input_per_1m": 0,
				"output_per_1m": 0,
				"average_per_1m": 0
			},
			"performance": {
				"output_speed_tps": 75,
				"latency_ttft_ms": 850,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Flagship open-source MoE model. Outperforms DeepSeek-R1 on 17/23 benchmarks. State-of-the-art reasoning among open models.",
			"benchmark_scores": {
				"aime": 86.7,
				"arc_agi_2": 9.5,
				"bfcl": 82.5,
				"frontiermath": 8.5,
				"gpqa_diamond": 80.1,
				"humanity_last_exam": null,
				"live_code_bench": 68.5,
				"livebench": 77.1,
				"lmarena_coding_elo": 1469,
				"lmarena_creative_elo": 1379,
				"lmarena_en_elo": 1432,
				"lmarena_hard_elo": 1449,
				"lmarena_if_elo": 1415,
				"lmarena_math_elo": 1428,
				"lmarena_vision_elo": 1211,
				"lmarena_zh_elo": 1465,
				"math_500": 85.5,
				"mathvista": 68.5,
				"mmlu_pro": 78,
				"mmmlu": 85.5,
				"simpleqa": 50.1,
				"mmmu": 65.5,
				"mmmu_pro": null,
				"osworld": 38.5,
				"swe_bench": 62.5,
				"tau_bench": 52.5,
				"terminal_bench": 45.5,
				"webdev_arena_elo": 1295,
				"livebench_reasoning": null,
				"livebench_coding": null,
				"livebench_agentic_coding": null,
				"livebench_math": null,
				"livebench_data_analysis": null,
				"livebench_language": null,
				"livebench_if": null
			}
		},
		{
			"id": "qwen3-32b",
			"name": "Qwen3 32B",
			"provider": "Alibaba",
			"type": "open-source",
			"release_date": "2025-04-29",
			"pricing": {
				"input_per_1m": 0,
				"output_per_1m": 0,
				"average_per_1m": 0
			},
			"performance": {
				"output_speed_tps": 145,
				"latency_ttft_ms": 320,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Largest dense Qwen3 model. Outperforms QwQ-32B while more efficient. Great for local deployment.",
			"benchmark_scores": {
				"aime": 68.5,
				"arc_agi_2": 6.8,
				"bfcl": 75.5,
				"frontiermath": null,
				"gpqa_diamond": 65.5,
				"humanity_last_exam": null,
				"live_code_bench": 55.5,
				"livebench": 74.9,
				"lmarena_coding_elo": 1405,
				"lmarena_creative_elo": 1304,
				"lmarena_en_elo": 1305,
				"lmarena_hard_elo": 1403,
				"lmarena_if_elo": 1345,
				"lmarena_math_elo": 1403,
				"lmarena_vision_elo": 1327,
				"lmarena_zh_elo": 1365,
				"math_500": 72.5,
				"mathvista": 58.5,
				"mmlu_pro": null,
				"mmmlu": 79.5,
				"simpleqa": 25,
				"mmmu": 55.5,
				"mmmu_pro": 68.1,
				"osworld": 32.5,
				"swe_bench": 52.5,
				"tau_bench": 45.5,
				"terminal_bench": 35.5,
				"webdev_arena_elo": 1245,
				"livebench_reasoning": null,
				"livebench_coding": null,
				"livebench_agentic_coding": null,
				"livebench_math": null,
				"livebench_data_analysis": null,
				"livebench_language": null,
				"livebench_if": null
			},
			"imputed_metadata": {
				"simpleqa": {
					"original_value": null,
					"imputed_value": 25,
					"method": "estimated",
					"imputed_date": "2025-12-27",
					"note": "Estimated as ~50% of Qwen3 235B (50.1)"
				}
			}
		},
		{
			"id": "qwen3-max-preview",
			"name": "Qwen3 Max Preview",
			"provider": "Alibaba",
			"type": "proprietary",
			"release_date": "2025-12-01",
			"pricing": {
				"input_per_1m": 1.2,
				"output_per_1m": 6,
				"average_per_1m": 3.6
			},
			"performance": {
				"output_speed_tps": 85,
				"latency_ttft_ms": 750,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Alibaba's flagship proprietary model in preview. Top 3 on LMArena. Strong coding (SWE-Bench 69.6% verified) and math performance. Thinking variant achieves 100% on AIME 2025.",
			"benchmark_scores": {
				"aime": 73.3,
				"arc_agi_2": null,
				"bfcl": 80.5,
				"frontiermath": null,
				"gpqa_diamond": 72.6,
				"humanity_last_exam": null,
				"live_code_bench": 66.9,
				"livebench": null,
				"lmarena_coding_elo": 1463,
				"lmarena_creative_elo": 1396,
				"lmarena_en_elo": 1423,
				"lmarena_hard_elo": 1444,
				"lmarena_if_elo": 1427,
				"lmarena_math_elo": 1436,
				"lmarena_vision_elo": 1305,
				"lmarena_zh_elo": 1489,
				"math_500": 82.5,
				"mathvista": 68.5,
				"mmlu_pro": null,
				"mmmlu": 83.5,
				"simpleqa": 67.5,
				"mmmu": 65.5,
				"mmmu_pro": null,
				"osworld": null,
				"swe_bench": 69.6,
				"tau_bench": 74.8,
				"terminal_bench": 36.3,
				"webdev_arena_elo": 1305,
				"livebench_reasoning": null,
				"livebench_coding": null,
				"livebench_agentic_coding": null,
				"livebench_math": null,
				"livebench_data_analysis": null,
				"livebench_language": null,
				"livebench_if": null
			}
		},
		{
			"id": "o3-2025-04-16",
			"name": "OpenAI o3",
			"provider": "OpenAI",
			"type": "proprietary",
			"release_date": "2025-04-16",
			"pricing": {
				"input_per_1m": 10,
				"output_per_1m": 40,
				"average_per_1m": 25
			},
			"performance": {
				"output_speed_tps": 35,
				"latency_ttft_ms": 2500,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Premier reasoning model with extended thinking. Excels at complex multi-step problems. Higher latency due to deliberation.",
			"benchmark_scores": {
				"aime": 83.9,
				"arc_agi_2": 6.5,
				"bfcl": 78.5,
				"frontiermath": 18.7,
				"gpqa_diamond": 81.8,
				"humanity_last_exam": 24,
				"live_code_bench": 72.5,
				"livebench": 79,
				"lmarena_coding_elo": 1457,
				"lmarena_creative_elo": 1384,
				"lmarena_en_elo": 1442,
				"lmarena_hard_elo": 1440,
				"lmarena_if_elo": 1399,
				"lmarena_math_elo": 1455,
				"lmarena_vision_elo": 1217,
				"lmarena_zh_elo": 1459,
				"math_500": 95.8,
				"mathvista": 78.5,
				"mmlu_pro": 86.4,
				"mmmlu": 85.5,
				"simpleqa": 53,
				"mmmu": 74.5,
				"mmmu_pro": 76.4,
				"osworld": 48.5,
				"swe_bench": 71.7,
				"tau_bench": 55.5,
				"terminal_bench": 58.5,
				"webdev_arena_elo": 1315,
				"livebench_reasoning": null,
				"livebench_coding": null,
				"livebench_agentic_coding": null,
				"livebench_math": null,
				"livebench_data_analysis": null,
				"livebench_language": null,
				"livebench_if": null
			}
		},
		{
			"id": "gemini-2.5-flash",
			"name": "Gemini 2.5 Flash",
			"provider": "Google",
			"type": "proprietary",
			"release_date": "2025-04-17",
			"pricing": {
				"input_per_1m": 0.15,
				"output_per_1m": 0.6,
				"average_per_1m": 0.38
			},
			"performance": {
				"output_speed_tps": 372,
				"latency_ttft_ms": 180,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Fastest major model at 372 tokens/s. Incredible value for speed-critical applications. Good reasoning at ultra-low cost.",
			"benchmark_scores": {
				"aime": 65.5,
				"arc_agi_2": 6.5,
				"bfcl": 74.5,
				"frontiermath": null,
				"gpqa_diamond": 72.5,
				"humanity_last_exam": 11,
				"live_code_bench": 52.5,
				"livebench": null,
				"lmarena_coding_elo": 1422,
				"lmarena_creative_elo": 1398,
				"lmarena_en_elo": 1412,
				"lmarena_hard_elo": 1415,
				"lmarena_if_elo": 1401,
				"lmarena_math_elo": 1413,
				"lmarena_vision_elo": 1213,
				"lmarena_zh_elo": 1437,
				"math_500": 78.5,
				"mathvista": 65.5,
				"mmlu_pro": null,
				"mmmlu": 80.5,
				"simpleqa": 45,
				"mmmu": 62.5,
				"mmmu_pro": null,
				"osworld": 35.5,
				"swe_bench": 48.5,
				"tau_bench": 45.5,
				"terminal_bench": 32.5,
				"webdev_arena_elo": 1313,
				"livebench_reasoning": null,
				"livebench_coding": null,
				"livebench_agentic_coding": null,
				"livebench_math": null,
				"livebench_data_analysis": null,
				"livebench_language": null,
				"livebench_if": null
			},
			"imputed_metadata": {
				"simpleqa": {
					"original_value": null,
					"imputed_value": 45,
					"method": "estimated",
					"imputed_date": "2025-12-27",
					"note": "Estimated as ~80% of Gemini 2.5 Pro (56.0)"
				}
			}
		},
		{
			"id": "minimax-m2",
			"name": "MiniMax M2",
			"provider": "MiniMax",
			"type": "open-source",
			"release_date": "2025-10-27",
			"pricing": {
				"input_per_1m": 0.3,
				"output_per_1m": 1.2,
				"average_per_1m": 0.75
			},
			"performance": {
				"output_speed_tps": 100,
				"latency_ttft_ms": 450,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "MIT-licensed 230B MoE model excelling at coding tasks. Tops Multi-SWE-Bench and WebDev Arena. Fast inference at 100 TPS with excellent value for agentic workflows.",
			"benchmark_scores": {
				"aime": null,
				"arc_agi_2": null,
				"bfcl": null,
				"frontiermath": null,
				"gpqa_diamond": 78,
				"humanity_last_exam": null,
				"live_code_bench": 83.0,
				"livebench": null,
				"lmarena_coding_elo": 1376,
				"lmarena_creative_elo": 1332,
				"lmarena_en_elo": 1372,
				"lmarena_hard_elo": 1366,
				"lmarena_if_elo": 1333,
				"lmarena_math_elo": 1363,
				"lmarena_vision_elo": 1372,
				"lmarena_zh_elo": 1390,
				"math_500": null,
				"mathvista": null,
				"mmlu_pro": null,
				"mmmlu": null,
				"simpleqa": 18,
				"mmmu": null,
				"mmmu_pro": null,
				"osworld": null,
				"swe_bench": 69.4,
				"tau_bench": null,
				"terminal_bench": null,
				"webdev_arena_elo": 1313,
				"livebench_reasoning": null,
				"livebench_coding": null,
				"livebench_agentic_coding": null,
				"livebench_math": null,
				"livebench_data_analysis": null,
				"livebench_language": null,
				"livebench_if": null
			},
			"imputed_metadata": {
				"simpleqa": {
					"original_value": null,
					"imputed_value": 18,
					"method": "estimated",
					"imputed_date": "2025-12-27",
					"note": "Estimated for open-source MoE model"
				}
			}
		},
		{
			"id": "kimi-k2-0905-preview",
			"name": "Kimi K2",
			"aka": ["kimi-k2", "kimi-k2-preview", "k2-0905"],
			"provider": "Moonshot AI",
			"type": "open-source",
			"release_date": "2025-09-05",
			"pricing": {
				"input_per_1m": 0.6,
				"output_per_1m": 2.5,
				"average_per_1m": 1.55
			},
			"performance": {
				"output_speed_tps": 85,
				"latency_ttft_ms": 650,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Open-source MoE model (1T total, 32B active) with strong agentic performance. Competitive with Claude Sonnet 4 on SWE-bench at fraction of cost. 256K context window.",
			"benchmark_scores": {
				"aime": 49.5,
				"arc_agi_2": 2.5,
				"bfcl": 59.6,
				"frontiermath": null,
				"gpqa_diamond": 71.3,
				"humanity_last_exam": null,
				"live_code_bench": 53.7,
				"livebench": 76.4,
				"lmarena_coding_elo": 1465,
				"lmarena_creative_elo": 1381,
				"lmarena_en_elo": null,
				"lmarena_hard_elo": 1435,
				"lmarena_if_elo": 1382,
				"lmarena_math_elo": 1417,
				"lmarena_vision_elo": null,
				"lmarena_zh_elo": 1444,
				"math_500": 97.4,
				"mathvista": null,
				"mmlu_pro": null,
				"mmmlu": null,
				"simpleqa": 20,
				"mmmu": null,
				"mmmu_pro": null,
				"osworld": null,
				"swe_bench": 69.2,
				"tau_bench": null,
				"terminal_bench": 35.7,
				"webdev_arena_elo": null,
				"livebench_reasoning": null,
				"livebench_coding": null,
				"livebench_agentic_coding": null,
				"livebench_math": null,
				"livebench_data_analysis": null,
				"livebench_language": null,
				"livebench_if": null
			},
			"imputed_metadata": {
				"simpleqa": {
					"original_value": null,
					"imputed_value": 20,
					"method": "estimated",
					"imputed_date": "2025-12-27",
					"note": "Estimated as ~65% of Kimi K2 Thinking (31.6)"
				}
			}
		},
		{
			"id": "kimi-k2-thinking-turbo",
			"name": "Kimi K2 Thinking",
			"aka": ["kimi-k2-thinking", "k2-thinking", "kimi-k2-think", "kimi-k2-thinking-turbo"],
			"superior_of": "kimi-k2-0905-preview",
			"provider": "Moonshot AI",
			"type": "open-source",
			"release_date": "2025-11-06",
			"pricing": {
				"input_per_1m": 0.6,
				"output_per_1m": 2.5,
				"average_per_1m": 1.55
			},
			"performance": {
				"output_speed_tps": 45,
				"latency_ttft_ms": 1800,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Thinking agent with multi-step reasoning and tool use. Sets open-source records on SWE-bench (71.3%) and Humanity's Last Exam (44.9% w/tools). Can execute 200-300 sequential tool calls.",
			"benchmark_scores": {
				"aime": 83.1,
				"arc_agi_2": 2.5,
				"bfcl": 90,
				"frontiermath": 21.4,
				"gpqa_diamond": 84.2,
				"humanity_last_exam": 23.9,
				"live_code_bench": 83,
				"livebench": null,
				"lmarena_coding_elo": 1484,
				"lmarena_creative_elo": 1399,
				"lmarena_en_elo": 1446,
				"lmarena_hard_elo": 1453,
				"lmarena_if_elo": 1419,
				"lmarena_math_elo": 1439,
				"lmarena_vision_elo": null,
				"lmarena_zh_elo": 1456,
				"math_500": 97.4,
				"mathvista": null,
				"mmlu_pro": 89.8,
				"mmmlu": 87.7,
				"simpleqa": 31.6,
				"mmmu": null,
				"mmmu_pro": null,
				"osworld": null,
				"swe_bench": 71.3,
				"tau_bench": 93,
				"terminal_bench": 47.1,
				"webdev_arena_elo": 1337,
				"livebench_reasoning": null,
				"livebench_coding": null,
				"livebench_agentic_coding": null,
				"livebench_math": null,
				"livebench_data_analysis": null,
				"livebench_language": null,
				"livebench_if": null
			}
		},
		{
			"id": "longcat-flash-chat",
			"name": "Longcat Flash Chat",
			"provider": "Meituan",
			"type": "open-source",
			"release_date": "2025-12-01",
			"pricing": {
				"input_per_1m": 0.15,
				"output_per_1m": 0.75,
				"average_per_1m": 0.45
			},
			"performance": {
				"output_speed_tps": 100,
				"latency_ttft_ms": 350,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Meituan's MoE model (560B total, 27B active) optimized for agentic and conversational tasks. 128K context with strong tool use and Chinese language performance.",
			"benchmark_scores": {
				"aime": 68.3,
				"arc_agi_2": null,
				"bfcl": 82,
				"frontiermath": null,
				"gpqa_diamond": 73.23,
				"humanity_last_exam": null,
				"live_code_bench": 48.02,
				"livebench": null,
				"lmarena_coding_elo": 1474,
				"lmarena_creative_elo": 1388,
				"lmarena_en_elo": 1426,
				"lmarena_hard_elo": 1425,
				"lmarena_if_elo": 1388,
				"lmarena_math_elo": 1429,
				"lmarena_vision_elo": null,
				"lmarena_zh_elo": 1412,
				"math_500": 96.4,
				"mathvista": null,
				"mmlu_pro": null,
				"mmmlu": null,
				"simpleqa": 15,
				"mmmu": null,
				"mmmu_pro": null,
				"osworld": null,
				"swe_bench": 60.4,
				"tau_bench": 67.7,
				"terminal_bench": 39.51,
				"webdev_arena_elo": 1332,
				"livebench_reasoning": null,
				"livebench_coding": null,
				"livebench_agentic_coding": null,
				"livebench_math": null,
				"livebench_data_analysis": null,
				"livebench_language": null,
				"livebench_if": null
			},
			"imputed_metadata": {
				"simpleqa": {
					"original_value": null,
					"imputed_value": 15,
					"method": "estimated",
					"imputed_date": "2025-12-27",
					"note": "Estimated for open-source MoE model"
				}
			}
		},
		{
			"id": "mistral-large-3",
			"name": "Mistral Large 3",
			"provider": "Mistral",
			"type": "open-source",
			"release_date": "2025-12-02",
			"pricing": {
				"input_per_1m": 0.5,
				"output_per_1m": 1.5,
				"average_per_1m": 1
			},
			"performance": {
				"output_speed_tps": 90,
				"latency_ttft_ms": 450,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Mistral's flagship MoE model (675B total, 41B active). Top open-source coding model on LMArena. Excels at structured reasoning and math with competitive pricing.",
			"benchmark_scores": {
				"aime": 53.3,
				"arc_agi_2": null,
				"bfcl": null,
				"frontiermath": null,
				"gpqa_diamond": 43.9,
				"humanity_last_exam": null,
				"live_code_bench": null,
				"livebench": null,
				"lmarena_coding_elo": 1450,
				"lmarena_creative_elo": 1364,
				"lmarena_en_elo": 1428,
				"lmarena_hard_elo": 1440,
				"lmarena_if_elo": 1407,
				"lmarena_math_elo": 1425,
				"lmarena_vision_elo": null,
				"lmarena_zh_elo": 1426,
				"math_500": 93.6,
				"mathvista": null,
				"mmlu_pro": 73.1,
				"mmmlu": null,
				"simpleqa": 12,
				"mmmu": null,
				"mmmu_pro": null,
				"osworld": null,
				"swe_bench": 55,
				"tau_bench": null,
				"terminal_bench": 23.8,
				"webdev_arena_elo": 1222,
				"livebench_reasoning": null,
				"livebench_coding": null,
				"livebench_agentic_coding": null,
				"livebench_math": null,
				"livebench_data_analysis": null,
				"livebench_language": null,
				"livebench_if": null
			},
			"imputed_metadata": {
				"simpleqa": {
					"original_value": null,
					"imputed_value": 12,
					"method": "estimated",
					"imputed_date": "2025-12-27",
					"note": "Estimated for open-source MoE model"
				}
			}
		}
	],
	"categories": [
		{
			"id": "coding",
			"name": "Coding",
			"emoji": "",
			"weight": 0.25,
			"description": "Measures ability to write, edit, and debug code across multiple programming languages. Includes real-world GitHub issue resolution and competitive programming.",
			"benchmarks": [
				{
					"id": "swe_bench",
					"name": "SWE-Bench Verified",
					"type": "percentage",
					"weight": 0.46,
					"url": "https://swebench.com",
					"description": "Real-world GitHub issue resolution from popular Python repositories"
				},
				{
					"id": "terminal_bench",
					"name": "Terminal-Bench",
					"type": "percentage",
					"weight": 0.05,
					"url": "https://github.com/terminal-bench/terminal-bench",
					"description": "Terminal and command-line task completion"
				},
				{
					"id": "lmarena_coding_elo",
					"name": "LMArena Coding",
					"type": "elo",
					"weight": 0.22,
					"url": "https://lmarena.ai/leaderboard/text/coding",
					"description": "Human preference votes on coding tasks",
					"elo_range": {
						"min": 1100,
						"max": 1600
					}
				},
				{
					"id": "live_code_bench",
					"name": "LiveCodeBench",
					"type": "percentage",
					"weight": 0.16,
					"url": "https://livecodebench.github.io",
					"description": "Competitive programming problems from recent contests"
				},
				{
					"id": "livebench_coding",
					"name": "LiveBench Coding",
					"type": "percentage",
					"weight": 0.11,
					"url": "https://livebench.ai",
					"description": "Monthly-updated coding tasks resistant to contamination (LeetCode-style problems)"
				}
			]
		},
		{
			"id": "reasoning",
			"name": "Reasoning",
			"emoji": "",
			"weight": 0.25,
			"description": "Complex problem-solving and PhD-level questions in science, mathematics, and logical reasoning.",
			"benchmarks": [
				{
					"id": "gpqa_diamond",
					"name": "GPQA Diamond",
					"type": "percentage",
					"weight": 0.4,
					"url": "https://artificialanalysis.ai/evaluations/gpqa-diamond",
					"description": "Graduate-level physics, chemistry, and biology questions"
				},
				{
					"id": "arc_agi_2",
					"name": "ARC-AGI-2",
					"type": "percentage",
					"weight": 0.3,
					"url": "https://arcprize.org/leaderboard#leaderboard-table",
					"description": "ARC-AGI-2 is a contamination-resistant benchmark that tests abstract reasoning and generalization in AI systems."
				},
				{
					"id": "livebench",
					"name": "LiveBench Global",
					"type": "percentage",
					"weight": 0.05,
					"url": "https://livebench.ai",
					"description": "Continuously updated benchmark with 21 diverse tasks resistant to contamination (fallback)"
				},
				{
					"id": "livebench_reasoning",
					"name": "LiveBench Reasoning",
					"type": "percentage",
					"weight": 0.1,
					"url": "https://livebench.ai",
					"description": "Monthly-updated reasoning tasks: theory of mind, zebra puzzles, spatial reasoning"
				},
				{
					"id": "humanity_last_exam",
					"name": "HLE",
					"type": "percentage",
					"weight": 0.05,
					"url": "https://scale.com/hle",
					"description": "Humanity's Last Exam: PhD-level questions across 100+ domains"
				},
				{
					"id": "lmarena_hard_elo",
					"name": "LMArena Hard Prompts",
					"type": "elo",
					"weight": 0.1,
					"url": "https://lmarena.ai/leaderboard/text/hard-prompts",
					"description": "Human preference on challenging reasoning tasks",
					"elo_range": {
						"min": 1100,
						"max": 1550
					}
				}
			]
		},
		{
			"id": "agents",
			"name": "Agents & Tools",
			"emoji": "",
			"weight": 0.18,
			"description": "Function calling, computer use, and tool integration capabilities for autonomous operation.",
			"benchmarks": [
				{
					"id": "bfcl",
					"name": "Berkeley Function Calling (BFCL)",
					"type": "percentage",
					"weight": 0.25,
					"url": "https://gorilla.cs.berkeley.edu/leaderboard.html",
					"description": "Function calling accuracy across multiple languages"
				},
				{
					"id": "tau_bench",
					"name": "TAU-Bench",
					"type": "percentage",
					"weight": 0.2,
					"url": "https://taubench.com/#leaderboard",
					"description": "-bench evaluates AI agents in dual-control environments where both agent and user actively collaborate using tools to solve shared problems."
				},
				{
					"id": "osworld",
					"name": "OSWorld",
					"type": "percentage",
					"weight": 0.2,
					"url": "https://os-world.github.io",
					"description": "Computer control and GUI interaction tasks"
				},
				{
					"id": "webdev_arena_elo",
					"name": "WebDev Arena",
					"type": "elo",
					"weight": 0.15,
					"url": "https://web.lmarena.ai/leaderboard",
					"description": "Web development task completion",
					"elo_range": {
						"min": 1100,
						"max": 1600
					}
				},
				{
					"id": "livebench_agentic_coding",
					"name": "LiveBench Agentic Coding",
					"type": "percentage",
					"weight": 0.1,
					"url": "https://livebench.ai",
					"description": "Agentic coding with Mini-SWE-Agent for autonomous code completion"
				},
				{
					"id": "livebench_data_analysis",
					"name": "LiveBench Data Analysis",
					"type": "percentage",
					"weight": 0.1,
					"url": "https://livebench.ai",
					"description": "Table operations, joins, and type annotation tasks"
				}
			]
		},
		{
			"id": "conversation",
			"name": "Conversation",
			"emoji": "",
			"weight": 0.12,
			"description": "Creative writing, instruction following, and conversational quality.",
			"benchmarks": [
				{
					"id": "lmarena_creative_elo",
					"name": "LMArena Creative Writing",
					"type": "elo",
					"weight": 0.4375,
					"url": "https://lmarena.ai/leaderboard/text/creative-writing",
					"description": "Human preference on creative writing tasks",
					"elo_range": {
						"min": 1100,
						"max": 1600
					}
				},
				{
					"id": "lmarena_if_elo",
					"name": "LMArena Instruction Following",
					"type": "elo",
					"weight": 0.4375,
					"url": "https://lmarena.ai/leaderboard/text/instruction-following",
					"description": "Human preference on instruction following tasks",
					"elo_range": {
						"min": 1100,
						"max": 1600
					}
				},
				{
					"id": "livebench_if",
					"name": "LiveBench IF",
					"type": "percentage",
					"weight": 0.125,
					"url": "https://livebench.ai",
					"description": "Monthly-updated instruction following evaluation resistant to contamination"
				}
			]
		},
		{
			"id": "math",
			"name": "Math",
			"emoji": "",
			"weight": 0.1,
			"description": "Mathematical problem solving from elementary to competition level.",
			"benchmarks": [
				{
					"id": "math_500",
					"name": "MATH-500",
					"type": "percentage",
					"weight": 0.42,
					"url": "https://github.com/hendrycks/math",
					"description": "Competition mathematics problems"
				},
				{
					"id": "aime",
					"name": "AIME 2025",
					"type": "percentage",
					"weight": 0.2,
					"url": "https://artofproblemsolving.com/wiki/index.php/AIME",
					"description": "American Invitational Mathematics Examination problems"
				},
				{
					"id": "lmarena_math_elo",
					"name": "LMArena Math",
					"type": "elo",
					"weight": 0.2,
					"url": "https://lmarena.ai/leaderboard/text/math",
					"description": "Human preference on math tasks",
					"elo_range": {
						"min": 1100,
						"max": 1600
					}
				},
				{
					"id": "frontiermath",
					"name": "FrontierMath",
					"type": "percentage",
					"weight": 0.08,
					"url": "https://epoch.ai/frontiermath",
					"description": "Research-level mathematics problems designed by experts"
				},
				{
					"id": "livebench_math",
					"name": "LiveBench Math",
					"type": "percentage",
					"weight": 0.1,
					"url": "https://livebench.ai",
					"description": "Monthly-updated math from recent competitions (IMO, USAMO, AMPS)"
				}
			]
		},
		{
			"id": "multimodal",
			"name": "Multimodal",
			"emoji": "",
			"weight": 0.07,
			"description": "Vision and text understanding, including charts, diagrams, and images.",
			"benchmarks": [
				{
					"id": "mathvista",
					"name": "MathVista",
					"type": "percentage",
					"weight": 0.2,
					"url": "https://mathvista.github.io",
					"description": "Math reasoning with visual context"
				},
				{
					"id": "mmmu",
					"name": "MMMU",
					"type": "percentage",
					"weight": 0.2,
					"url": "https://mmmu-benchmark.github.io",
					"description": "Massive multi-discipline multimodal understanding"
				},
				{
					"id": "mmmu_pro",
					"name": "MMMU-Pro",
					"type": "percentage",
					"weight": 0.2,
					"url": "https://mmmu-benchmark.github.io",
					"description": "Enhanced MMMU with more robust evaluation"
				},
				{
					"id": "lmarena_vision_elo",
					"name": "LMArena Vision",
					"type": "elo",
					"weight": 0.4,
					"url": "https://lmarena.ai/leaderboard/vision",
					"description": "Human preference on vision tasks",
					"elo_range": {
						"min": 1100,
						"max": 1450
					}
				}
			]
		},
		{
			"id": "knowledge",
			"name": "Knowledge",
			"emoji": "",
			"weight": 0.03,
			"description": "Knowledge assessment across subjects and languages.",
			"benchmarks": [
				{
					"id": "mmlu_pro",
					"name": "MMLU-Pro",
					"type": "percentage",
					"weight": 0.3,
					"url": "https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro",
					"description": "Enhanced MMLU with 10 answer options and reasoning focus"
				},
				{
					"id": "mmmlu",
					"name": "MMMLU",
					"type": "percentage",
					"weight": 0.3,
					"url": "https://huggingface.co/datasets/openai/mmmlu",
					"description": "Multilingual MMLU across 14 languages"
				},
				{
					"id": "simpleqa",
					"name": "SimpleQA Verified",
					"type": "percentage",
					"weight": 0.2,
					"url": "https://openai.com/index/introducing-simpleqa/",
					"description": "1,000 factoid questions testing factual accuracy and hallucination detection"
				},
				{
					"id": "lmarena_en_elo",
					"name": "LMArena English",
					"type": "elo",
					"weight": 0.05,
					"url": "https://lmarena.ai/leaderboard",
					"description": "Human preference on English tasks",
					"elo_range": {
						"min": 1100,
						"max": 1600
					}
				},
				{
					"id": "lmarena_zh_elo",
					"name": "LMArena Chinese",
					"type": "elo",
					"weight": 0.05,
					"url": "https://lmarena.ai/leaderboard",
					"description": "Human preference on Chinese tasks",
					"elo_range": {
						"min": 1100,
						"max": 1600
					}
				},
				{
					"id": "livebench_language",
					"name": "LiveBench Language",
					"type": "percentage",
					"weight": 0.1,
					"url": "https://livebench.ai",
					"description": "Monthly-updated language understanding tasks across multiple domains"
				}
			]
		}
	]
}
