{
	"meta": {
		"version": "2025.12.26",
		"last_update": "2025-12-26T18:00:00Z",
		"schema_version": "1.0"
	},
	"models": [
		{
			"id": "claude-opus-4-5-20251101",
			"name": "Claude Opus 4.5",
			"provider": "Anthropic",
			"type": "proprietary",
			"release_date": "2025-11-24",
			"pricing": {
				"input_per_1m": 5,
				"output_per_1m": 25,
				"average_per_1m": 15
			},
			"performance": {
				"output_speed_tps": 65,
				"latency_ttft_ms": 3750,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Anthropic's most intelligent model. Excels at complex reasoning and advanced coding with extended autonomous operation. New pricing makes it more competitive.",
			"benchmark_scores": {
				"aider_polyglot": 95.5,
				"aime": 78.5,
				"arc_agi_2": 7.8,
				"bfcl": 82.5,
				"frontiermath": null,
				"gpqa_diamond": 87,
				"humanity_last_exam": null,
				"ifeval": null,
				"live_code_bench": 72.3,
				"livebench": null,
				"lmarena_coding_elo": 1515,
				"lmarena_creative_elo": 1455,
				"lmarena_en_elo": 1476,
				"lmarena_hard_elo": 1493,
				"lmarena_if_elo": 1471,
				"lmarena_math_elo": 1380,
				"lmarena_vision_elo": 1189,
				"lmarena_zh_elo": 1355,
				"math_500": 88.5,
				"mathvista": 72.8,
				"mmlu_pro": 82,
				"mmmlu": 85.2,
				"mmmu": 70.5,
				"mmmu_pro": null,
				"osworld": 61.4,
				"swe_bench": 80.9,
				"tau_bench": 64.2,
				"terminal_bench": 58.5,
				"webdev_arena_elo": 1480
			}
		},
		{
			"id": "claude-opus-4-5-20251101-thinking-32k",
			"name": "Claude Opus 4.5 Thinking",
			"provider": "Anthropic",
			"type": "proprietary",
			"release_date": "2025-11-24",
			"pricing": {
				"input_per_1m": 5,
				"output_per_1m": 25,
				"average_per_1m": 15
			},
			"performance": {
				"output_speed_tps": 35,
				"latency_ttft_ms": 8500,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Extended thinking variant of Opus 4.5. Excels at complex reasoning tasks with visible chain-of-thought. Won LMArena Triple Crown (Expert, WebDev, Math). Higher latency due to deliberation.",
			"benchmark_scores": {
				"aider_polyglot": null,
				"aime": 93,
				"arc_agi_2": 37.6,
				"bfcl": 80.9,
				"frontiermath": null,
				"gpqa_diamond": 87,
				"humanity_last_exam": null,
				"ifeval": null,
				"live_code_bench": 75.5,
				"livebench": null,
				"lmarena_coding_elo": 1540,
				"lmarena_creative_elo": 1454,
				"lmarena_en_elo": 1486,
				"lmarena_hard_elo": 1502,
				"lmarena_if_elo": 1475,
				"lmarena_math_elo": 1468,
				"lmarena_vision_elo": 1210,
				"lmarena_zh_elo": 1492,
				"math_500": 94.5,
				"mathvista": 78.5,
				"mmlu_pro": null,
				"mmmlu": 84.6,
				"mmmu": 83,
				"mmmu_pro": null,
				"osworld": 66.3,
				"swe_bench": 80.9,
				"tau_bench": 93.6,
				"terminal_bench": 59.3,
				"webdev_arena_elo": 1520
			}
		},
		{
			"id": "claude-sonnet-4-5-20250929",
			"name": "Claude Sonnet 4.5",
			"provider": "Anthropic",
			"type": "proprietary",
			"release_date": "2025-09-29",
			"pricing": {
				"input_per_1m": 3,
				"output_per_1m": 15,
				"average_per_1m": 9
			},
			"performance": {
				"output_speed_tps": 77,
				"latency_ttft_ms": 1200,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Best-in-class for software engineering. State-of-the-art on SWE-bench with excellent balance of speed and capability.",
			"benchmark_scores": {
				"aider_polyglot": 92.9,
				"aime": 72.3,
				"arc_agi_2": 3.8,
				"bfcl": 80.8,
				"frontiermath": 2,
				"gpqa_diamond": 78.5,
				"humanity_last_exam": 30,
				"ifeval": 88.5,
				"live_code_bench": 74.5,
				"livebench": 82,
				"lmarena_coding_elo": 1506,
				"lmarena_creative_elo": 1380,
				"lmarena_en_elo": 1375,
				"lmarena_hard_elo": 1474,
				"lmarena_if_elo": 1452,
				"lmarena_math_elo": 1355,
				"lmarena_vision_elo": 1295,
				"lmarena_zh_elo": 1325,
				"math_500": 82.3,
				"mathvista": 68.5,
				"mmlu_pro": 78,
				"mmmlu": 82.8,
				"mmmu": 66.2,
				"mmmu_pro": null,
				"osworld": 44,
				"swe_bench": 82,
				"tau_bench": 58.5,
				"terminal_bench": 62.1,
				"webdev_arena_elo": 1387
			}
		},
		{
			"id": "claude-sonnet-4-5-20250929-thinking-32k",
			"name": "Claude Sonnet 4.5 Thinking",
			"provider": "Anthropic",
			"type": "proprietary",
			"release_date": "2025-09-29",
			"pricing": {
				"input_per_1m": 3,
				"output_per_1m": 15,
				"average_per_1m": 9
			},
			"performance": {
				"output_speed_tps": 45,
				"latency_ttft_ms": 5500,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Extended thinking variant of Sonnet 4.5. Excels at complex reasoning tasks with visible chain-of-thought. Higher latency due to deliberation but improved math and coding benchmarks.",
			"benchmark_scores": {
				"aider_polyglot": 93.3,
				"aime": 88.5,
				"arc_agi_2": 13.6,
				"bfcl": 83.5,
				"frontiermath": 5,
				"gpqa_diamond": 83.4,
				"humanity_last_exam": 35,
				"ifeval": 91,
				"live_code_bench": 77.2,
				"livebench": 85,
				"lmarena_coding_elo": 1525,
				"lmarena_creative_elo": 1395,
				"lmarena_en_elo": 1431,
				"lmarena_hard_elo": 1485,
				"lmarena_if_elo": 1457,
				"lmarena_math_elo": 1405,
				"lmarena_vision_elo": 1335,
				"lmarena_zh_elo": 1375,
				"math_500": 90.5,
				"mathvista": 74.5,
				"mmlu_pro": 80,
				"mmmlu": 84.5,
				"mmmu": 70.2,
				"mmmu_pro": null,
				"osworld": 55.8,
				"swe_bench": 82,
				"tau_bench": 62.5,
				"terminal_bench": 63.5,
				"webdev_arena_elo": 1393
			}
		},
		{
			"id": "claude-opus-4-1-20250805",
			"name": "Claude Opus 4.1",
			"provider": "Anthropic",
			"type": "proprietary",
			"release_date": "2025-08-05",
			"pricing": {
				"input_per_1m": 15,
				"output_per_1m": 75,
				"average_per_1m": 45
			},
			"performance": {
				"output_speed_tps": 52,
				"latency_ttft_ms": 4200,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Previous flagship with excellent sustained autonomous operation up to 30 hours. Strong at complex multi-step reasoning but higher cost.",
			"benchmark_scores": {
				"aider_polyglot": null,
				"aime": 70.5,
				"arc_agi_2": 8.1,
				"bfcl": 78.5,
				"frontiermath": null,
				"gpqa_diamond": 74.9,
				"humanity_last_exam": null,
				"ifeval": null,
				"live_code_bench": 68.2,
				"livebench": null,
				"lmarena_coding_elo": 1355,
				"lmarena_creative_elo": 1365,
				"lmarena_en_elo": 1360,
				"lmarena_hard_elo": 1385,
				"lmarena_if_elo": 1449,
				"lmarena_math_elo": 1345,
				"lmarena_vision_elo": 1285,
				"lmarena_zh_elo": 1310,
				"math_500": 80.2,
				"mathvista": 68.2,
				"mmlu_pro": null,
				"mmmlu": 81.5,
				"mmmu": 65.5,
				"mmmu_pro": null,
				"osworld": 58.2,
				"swe_bench": 74.5,
				"tau_bench": 62.8,
				"terminal_bench": 54.2,
				"webdev_arena_elo": 1335
			}
		},
		{
			"id": "gpt-4o-2024-05-13",
			"name": "GPT-4o",
			"provider": "OpenAI",
			"type": "proprietary",
			"release_date": "2024-05-13",
			"pricing": {
				"input_per_1m": 2.5,
				"output_per_1m": 10,
				"average_per_1m": 6.25
			},
			"performance": {
				"output_speed_tps": 110,
				"latency_ttft_ms": 320,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Excellent multimodal capabilities with very fast inference. Good value for general-purpose tasks but superseded by newer models on benchmarks.",
			"benchmark_scores": {
				"aider_polyglot": 16,
				"aime": 45.2,
				"arc_agi_2": 5.5,
				"bfcl": 76.2,
				"frontiermath": null,
				"gpqa_diamond": 53.6,
				"humanity_last_exam": 26.6,
				"ifeval": null,
				"live_code_bench": 48.2,
				"livebench": null,
				"lmarena_coding_elo": 1285,
				"lmarena_creative_elo": 1295,
				"lmarena_en_elo": 1295,
				"lmarena_hard_elo": 1285,
				"lmarena_if_elo": 1340,
				"lmarena_math_elo": 1275,
				"lmarena_vision_elo": 1285,
				"lmarena_zh_elo": 1245,
				"math_500": 76.6,
				"mathvista": 63.8,
				"mmlu_pro": 72.6,
				"mmmlu": 78.2,
				"mmmu": 58.5,
				"mmmu_pro": null,
				"osworld": 32.5,
				"swe_bench": 30.8,
				"tau_bench": 45.5,
				"terminal_bench": 28.5,
				"webdev_arena_elo": 1253
			}
		},
		{
			"id": "gpt-5.1",
			"name": "GPT 5.1",
			"provider": "OpenAI",
			"type": "proprietary",
			"release_date": "2025-11-12",
			"pricing": {
				"input_per_1m": 1.25,
				"output_per_1m": 10,
				"average_per_1m": 5.625
			},
			"performance": {
				"output_speed_tps": 95,
				"latency_ttft_ms": 400,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "OpenAI's latest flagship with adaptive reasoning. 30% faster than GPT-4o with improved instruction following. First model to reach 70 on Artificial Analysis Index.",
			"benchmark_scores": {
				"aider_polyglot": null,
				"aime": 85,
				"arc_agi_2": 17.6,
				"bfcl": 82.5,
				"frontiermath": null,
				"gpqa_diamond": 86.6,
				"humanity_last_exam": null,
				"ifeval": null,
				"live_code_bench": 68.5,
				"livebench": null,
				"lmarena_coding_elo": 1345,
				"lmarena_creative_elo": 1365,
				"lmarena_en_elo": 1385,
				"lmarena_hard_elo": 1385,
				"lmarena_if_elo": 1420,
				"lmarena_math_elo": 1365,
				"lmarena_vision_elo": 1239,
				"lmarena_zh_elo": 1325,
				"math_500": 85.5,
				"mathvista": 68.5,
				"mmlu_pro": null,
				"mmmlu": 85,
				"mmmu": 65.5,
				"mmmu_pro": null,
				"osworld": null,
				"swe_bench": 76.3,
				"tau_bench": null,
				"terminal_bench": null,
				"webdev_arena_elo": 1393
			}
		},
		{
			"id": "gpt-5.1-high",
			"name": "GPT 5.1 High",
			"provider": "OpenAI",
			"type": "proprietary",
			"release_date": "2025-11-12",
			"pricing": {
				"input_per_1m": 15,
				"output_per_1m": 120,
				"average_per_1m": 67.5
			},
			"performance": {
				"output_speed_tps": 40,
				"latency_ttft_ms": 1600,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Extended thinking variant with high reasoning effort. Scores 69 on Artificial Analysis Index. Higher latency and cost but excels on complex reasoning tasks.",
			"benchmark_scores": {
				"aider_polyglot": 88,
				"aime": 95,
				"arc_agi_2": 20,
				"bfcl": 84.5,
				"frontiermath": 2,
				"gpqa_diamond": 88.5,
				"humanity_last_exam": 25.32,
				"ifeval": 85,
				"live_code_bench": 78,
				"livebench": 80,
				"lmarena_coding_elo": 1490,
				"lmarena_creative_elo": 1395,
				"lmarena_en_elo": 1464,
				"lmarena_hard_elo": 1445,
				"lmarena_if_elo": 1446,
				"lmarena_math_elo": 1425,
				"lmarena_vision_elo": 1248,
				"lmarena_zh_elo": 1494,
				"math_500": 92,
				"mathvista": 72.5,
				"mmlu_pro": 86.9,
				"mmmlu": 87.5,
				"mmmu": 70,
				"mmmu_pro": null,
				"osworld": null,
				"swe_bench": 76.3,
				"tau_bench": null,
				"terminal_bench": 47.6,
				"webdev_arena_elo": 1393
			}
		},
		{
			"id": "gpt-5.2",
			"name": "GPT 5.2",
			"provider": "OpenAI",
			"type": "proprietary",
			"release_date": "2025-12-11",
			"pricing": {
				"input_per_1m": 1.75,
				"output_per_1m": 14,
				"average_per_1m": 7.875
			},
			"performance": {
				"output_speed_tps": 85,
				"latency_ttft_ms": 450,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "OpenAI's latest flagship with 256K context. First to score 90%+ on ARC-AGI-1 and 52.9% on ARC-AGI-2. Strong improvements over GPT 5.1 in all categories.",
			"benchmark_scores": {
				"aider_polyglot": 92.4,
				"aime": 92,
				"arc_agi_2": 48.5,
				"bfcl": 85.5,
				"frontiermath": 4,
				"gpqa_diamond": 90.5,
				"humanity_last_exam": 28,
				"ifeval": 89,
				"live_code_bench": 72.5,
				"livebench": 84,
				"lmarena_coding_elo": 1355,
				"lmarena_creative_elo": 1345,
				"lmarena_en_elo": 1360,
				"lmarena_hard_elo": 1370,
				"lmarena_if_elo": 1375,
				"lmarena_math_elo": 1325,
				"lmarena_vision_elo": 1290,
				"lmarena_zh_elo": 1300,
				"math_500": 88.5,
				"mathvista": 72.5,
				"mmlu_pro": 88.5,
				"mmmlu": 87.5,
				"mmmu": 70.5,
				"mmmu_pro": null,
				"osworld": null,
				"swe_bench": 80.0,
				"tau_bench": 88.5,
				"terminal_bench": null,
				"webdev_arena_elo": 1398
			}
		},
		{
			"id": "gpt-5.2-thinking",
			"name": "GPT 5.2 Thinking",
			"provider": "OpenAI",
			"type": "proprietary",
			"release_date": "2025-12-11",
			"pricing": {
				"input_per_1m": 1.75,
				"output_per_1m": 14,
				"average_per_1m": 7.875
			},
			"performance": {
				"output_speed_tps": 45,
				"latency_ttft_ms": 1200,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Extended thinking variant with 100% on AIME 2025. Top reasoning performance with 52.9% ARC-AGI-2 Verified. Achieves 80% on SWE-Bench and 55.6% on SWE-Bench Pro.",
			"benchmark_scores": {
				"aider_polyglot": null,
				"aime": 100,
				"arc_agi_2": 52.9,
				"bfcl": 88.5,
				"frontiermath": null,
				"gpqa_diamond": 92.4,
				"humanity_last_exam": null,
				"ifeval": null,
				"live_code_bench": 78.5,
				"livebench": null,
				"lmarena_coding_elo": 1365,
				"lmarena_creative_elo": 1355,
				"lmarena_en_elo": 1370,
				"lmarena_hard_elo": 1380,
				"lmarena_if_elo": 1385,
				"lmarena_math_elo": 1335,
				"lmarena_vision_elo": 1300,
				"lmarena_zh_elo": 1310,
				"math_500": 94.5,
				"mathvista": 78.5,
				"mmlu_pro": null,
				"mmmlu": 89.5,
				"mmmu": 76.5,
				"mmmu_pro": null,
				"osworld": 58.5,
				"swe_bench": 80,
				"tau_bench": 98.7,
				"terminal_bench": 52.5,
				"webdev_arena_elo": 1440
			}
		},
		{
			"id": "gpt-5.2-pro",
			"name": "GPT 5.2 Pro",
			"provider": "OpenAI",
			"type": "proprietary",
			"release_date": "2025-12-11",
			"pricing": {
				"input_per_1m": 15,
				"output_per_1m": 120,
				"average_per_1m": 67.5
			},
			"performance": {
				"output_speed_tps": 28,
				"latency_ttft_ms": 2800,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Premium tier with highest accuracy. First model to score 93.2% GPQA Diamond and 54.2% ARC-AGI-2 Verified. Peak performance for high-stakes scenarios. 70.9% match on GDPval knowledge tasks.",
			"benchmark_scores": {
				"aider_polyglot": null,
				"aime": 100,
				"arc_agi_2": 54.2,
				"bfcl": 90.5,
				"frontiermath": null,
				"gpqa_diamond": 93.2,
				"humanity_last_exam": null,
				"ifeval": null,
				"live_code_bench": 82,
				"livebench": null,
				"lmarena_coding_elo": 1375,
				"lmarena_creative_elo": 1365,
				"lmarena_en_elo": 1380,
				"lmarena_hard_elo": 1390,
				"lmarena_if_elo": 1395,
				"lmarena_math_elo": 1345,
				"lmarena_vision_elo": 1310,
				"lmarena_zh_elo": 1320,
				"math_500": 96.5,
				"mathvista": 82.5,
				"mmlu_pro": null,
				"mmmlu": 91.5,
				"mmmu": 80.5,
				"mmmu_pro": null,
				"osworld": 62.5,
				"swe_bench": 82.5,
				"tau_bench": 99.2,
				"terminal_bench": 55.5,
				"webdev_arena_elo": 1484
			}
		},
		{
			"id": "gemini-3-pro",
			"name": "Gemini 3 Pro",
			"provider": "Google",
			"type": "proprietary",
			"release_date": "2025-11-18",
			"pricing": {
				"input_per_1m": 2,
				"output_per_1m": 12,
				"average_per_1m": 7
			},
			"performance": {
				"output_speed_tps": 128,
				"latency_ttft_ms": 680,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "First model to break 1500 Elo on LMArena. Exceptional at algorithmic/competitive programming with breakthrough reasoning performance.",
			"benchmark_scores": {
				"aider_polyglot": 82.2,
				"aime": 95,
				"arc_agi_2": 31.1,
				"bfcl": 82.8,
				"frontiermath": 19,
				"gpqa_diamond": 91.9,
				"humanity_last_exam": 37.52,
				"ifeval": null,
				"live_code_bench": 78.5,
				"livebench": null,
				"lmarena_coding_elo": 1520,
				"lmarena_creative_elo": 1493,
				"lmarena_en_elo": 1493,
				"lmarena_hard_elo": 1505,
				"lmarena_if_elo": 1474,
				"lmarena_math_elo": 1480,
				"lmarena_vision_elo": 1309,
				"lmarena_zh_elo": 1520,
				"math_500": 92.8,
				"mathvista": 78.5,
				"mmlu_pro": 89,
				"mmmlu": 91.8,
				"mmmu": 75.2,
				"mmmu_pro": null,
				"osworld": 48.5,
				"swe_bench": 76.2,
				"tau_bench": 85.4,
				"terminal_bench": 54.2,
				"webdev_arena_elo": 1478
			}
		},
		{
			"id": "gemini-3-flash",
			"name": "Gemini 3 Flash",
			"provider": "Google",
			"type": "proprietary",
			"release_date": "2025-12-17",
			"pricing": {
				"input_per_1m": 0.5,
				"output_per_1m": 3,
				"average_per_1m": 1.75
			},
			"performance": {
				"output_speed_tps": 218,
				"latency_ttft_ms": 200,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Fastest intelligent model at 218 TPS. Outperforms even Gemini 3 Pro on SWE-Bench (78%) at quarter the cost. Minimal thinking mode. 3x faster than 2.5 Pro with 30% fewer tokens.",
			"benchmark_scores": {
				"aider_polyglot": 61.9,
				"aime": null,
				"arc_agi_2": 33.6,
				"bfcl": null,
				"frontiermath": null,
				"gpqa_diamond": 90.4,
				"humanity_last_exam": null,
				"ifeval": null,
				"live_code_bench": null,
				"livebench": null,
				"lmarena_coding_elo": 1497,
				"lmarena_creative_elo": 1438,
				"lmarena_en_elo": 1464,
				"lmarena_hard_elo": 1481,
				"lmarena_if_elo": 1443,
				"lmarena_math_elo": 1474,
				"lmarena_vision_elo": 1268,
				"lmarena_zh_elo": 1506,
				"math_500": null,
				"mathvista": null,
				"mmlu_pro": 84,
				"mmmlu": null,
				"mmmu": 81.2,
				"mmmu_pro": null,
				"osworld": null,
				"swe_bench": 78,
				"tau_bench": 90.2,
				"terminal_bench": null,
				"webdev_arena_elo": 1381
			}
		},
		{
			"id": "gemini-3-flash-thinking",
			"name": "Gemini 3 Flash Thinking",
			"provider": "Google",
			"type": "proprietary",
			"release_date": "2025-12-17",
			"pricing": {
				"input_per_1m": 0.5,
				"output_per_1m": 3,
				"average_per_1m": 1.75
			},
			"performance": {
				"output_speed_tps": 180,
				"latency_ttft_ms": 400,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Thinking mode of Gemini 3 Flash. Uses thinking_level parameter for controlled reasoning. Balances speed with enhanced reasoning capability for complex tasks. #2 on LMArena Overall.",
			"benchmark_scores": {
				"aider_polyglot": 61.9,
				"aime": null,
				"arc_agi_2": 36,
				"bfcl": null,
				"frontiermath": null,
				"gpqa_diamond": 91.5,
				"humanity_last_exam": null,
				"ifeval": null,
				"live_code_bench": null,
				"livebench": null,
				"lmarena_coding_elo": 1505,
				"lmarena_creative_elo": 1463,
				"lmarena_en_elo": 1476,
				"lmarena_hard_elo": 1489,
				"lmarena_if_elo": 1456,
				"lmarena_math_elo": 1480,
				"lmarena_vision_elo": 1284,
				"lmarena_zh_elo": 1523,
				"math_500": null,
				"mathvista": null,
				"mmlu_pro": 84,
				"mmmlu": null,
				"mmmu": 82,
				"mmmu_pro": null,
				"osworld": null,
				"swe_bench": 78,
				"tau_bench": 92,
				"terminal_bench": null,
				"webdev_arena_elo": 1465
			}
		},
		{
			"id": "gemini-2.5-pro",
			"name": "Gemini 2.5 Pro",
			"provider": "Google",
			"type": "proprietary",
			"release_date": "2025-03-25",
			"pricing": {
				"input_per_1m": 1.25,
				"output_per_1m": 5,
				"average_per_1m": 3.13
			},
			"performance": {
				"output_speed_tps": 165,
				"latency_ttft_ms": 520,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Excellent value with 1M token context window. Strong reasoning at lower cost but superseded by Gemini 3 on cutting-edge tasks.",
			"benchmark_scores": {
				"aider_polyglot": 76.5,
				"aime": 78.2,
				"arc_agi_2": 4.9,
				"bfcl": 79.5,
				"frontiermath": 2,
				"gpqa_diamond": 86.4,
				"humanity_last_exam": 26.9,
				"ifeval": 82,
				"live_code_bench": 65.5,
				"livebench": 78,
				"lmarena_coding_elo": 1345,
				"lmarena_creative_elo": 1452,
				"lmarena_en_elo": 1454,
				"lmarena_hard_elo": 1450,
				"lmarena_if_elo": 1443,
				"lmarena_math_elo": 1375,
				"lmarena_vision_elo": 1249,
				"lmarena_zh_elo": 1497,
				"math_500": 85.5,
				"mathvista": 72.5,
				"mmlu_pro": 86.7,
				"mmmlu": 84.2,
				"mmmu": 68.8,
				"mmmu_pro": null,
				"osworld": 42.5,
				"swe_bench": 63.8,
				"tau_bench": 52.8,
				"terminal_bench": 45.2,
				"webdev_arena_elo": 1315
			}
		},
		{
			"id": "deepseek-v3.1",
			"name": "DeepSeek V3.1",
			"provider": "DeepSeek",
			"type": "open-source",
			"release_date": "2025-08-15",
			"pricing": {
				"input_per_1m": 0.27,
				"output_per_1m": 1.1,
				"average_per_1m": 0.69
			},
			"performance": {
				"output_speed_tps": 120,
				"latency_ttft_ms": 450,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Outstanding value as open-source model. Strong coding performance at fraction of proprietary model costs. Excellent for tool use.",
			"benchmark_scores": {
				"aider_polyglot": 68.4,
				"aime": 68.5,
				"arc_agi_2": 8.5,
				"bfcl": 80.5,
				"frontiermath": 2,
				"gpqa_diamond": 75.2,
				"humanity_last_exam": 15.1,
				"ifeval": 86.1,
				"live_code_bench": 58.5,
				"livebench": 74.8,
				"lmarena_coding_elo": 1382,
				"lmarena_creative_elo": 1325,
				"lmarena_en_elo": 1335,
				"lmarena_hard_elo": 1355,
				"lmarena_if_elo": 1365,
				"lmarena_math_elo": 1315,
				"lmarena_vision_elo": 1265,
				"lmarena_zh_elo": 1385,
				"math_500": 78.5,
				"mathvista": 65.5,
				"mmlu_pro": 83.1,
				"mmmlu": 82.5,
				"mmmu": 62.5,
				"mmmu_pro": null,
				"osworld": 38.5,
				"swe_bench": 66,
				"tau_bench": 55.2,
				"terminal_bench": 31.3,
				"webdev_arena_elo": 1285
			}
		},
		{
			"id": "deepseek-r1",
			"name": "DeepSeek R1",
			"provider": "DeepSeek",
			"type": "open-source",
			"release_date": "2025-05-28",
			"pricing": {
				"input_per_1m": 0.55,
				"output_per_1m": 2.19,
				"average_per_1m": 1.37
			},
			"performance": {
				"output_speed_tps": 85,
				"latency_ttft_ms": 1200,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Reasoning-focused model with excellent math performance. Competitive with o3 on GPQA at fraction of cost.",
			"benchmark_scores": {
				"aider_polyglot": 71.6,
				"aime": 87.5,
				"arc_agi_2": 1.3,
				"bfcl": 75.8,
				"frontiermath": 2,
				"gpqa_diamond": 81,
				"humanity_last_exam": 18,
				"ifeval": 84,
				"live_code_bench": 73.3,
				"livebench": 73.3,
				"lmarena_coding_elo": 1305,
				"lmarena_creative_elo": 1345,
				"lmarena_en_elo": 1355,
				"lmarena_hard_elo": 1395,
				"lmarena_if_elo": 1355,
				"lmarena_math_elo": 1385,
				"lmarena_vision_elo": 1275,
				"lmarena_zh_elo": 1395,
				"math_500": 88.5,
				"mathvista": 68.2,
				"mmlu_pro": 84,
				"mmmlu": 84,
				"mmmu": 64.5,
				"mmmu_pro": null,
				"osworld": 35.2,
				"swe_bench": 57.6,
				"tau_bench": 48.5,
				"terminal_bench": 42.5,
				"webdev_arena_elo": 1265
			}
		},
		{
			"id": "grok-4.1",
			"name": "Grok 4.1",
			"provider": "xAI",
			"type": "proprietary",
			"release_date": "2025-11-17",
			"pricing": {
				"input_per_1m": 3,
				"output_per_1m": 15,
				"average_per_1m": 9
			},
			"performance": {
				"output_speed_tps": 95,
				"latency_ttft_ms": 850,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "xAI's flagship successor to Grok 4. #2 on LMArena Overall (1465 Elo). 3x reduction in hallucinations vs Grok 4. Excels at emotional intelligence and conversation.",
			"benchmark_scores": {
				"aider_polyglot": 79.6,
				"aime": 96,
				"arc_agi_2": 15.9,
				"bfcl": 83.5,
				"frontiermath": 5,
				"gpqa_diamond": 89.5,
				"humanity_last_exam": 44.4,
				"ifeval": null,
				"live_code_bench": 70.5,
				"livebench": null,
				"lmarena_coding_elo": 1375,
				"lmarena_creative_elo": 1424,
				"lmarena_en_elo": 1465,
				"lmarena_hard_elo": 1477,
				"lmarena_if_elo": 1435,
				"lmarena_math_elo": 1455,
				"lmarena_vision_elo": null,
				"lmarena_zh_elo": 1513,
				"math_500": 97.2,
				"mathvista": 76.5,
				"mmlu_pro": 85,
				"mmmlu": 87.5,
				"mmmu": 73,
				"mmmu_pro": null,
				"osworld": 48,
				"swe_bench": 76.5,
				"tau_bench": 58.5,
				"terminal_bench": 54,
				"webdev_arena_elo": 1335
			}
		},
		{
			"id": "grok-4.1-thinking",
			"name": "Grok 4.1 Thinking",
			"provider": "xAI",
			"type": "proprietary",
			"release_date": "2025-11-17",
			"pricing": {
				"input_per_1m": 3,
				"output_per_1m": 15,
				"average_per_1m": 9
			},
			"performance": {
				"output_speed_tps": 45,
				"latency_ttft_ms": 2200,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Extended thinking variant of Grok 4.1. #1 on LMArena Overall (1483 Elo). Deep reasoning with chain-of-thought. Top-tier emotional intelligence (1586 EQ-Bench Elo).",
			"benchmark_scores": {
				"aider_polyglot": 82,
				"aime": 97.5,
				"arc_agi_2": 22.5,
				"bfcl": 85.5,
				"frontiermath": 10,
				"gpqa_diamond": 91,
				"humanity_last_exam": 50.7,
				"ifeval": null,
				"live_code_bench": 74.5,
				"livebench": null,
				"lmarena_coding_elo": 1508,
				"lmarena_creative_elo": 1445,
				"lmarena_en_elo": 1483,
				"lmarena_hard_elo": 1490,
				"lmarena_if_elo": 1442,
				"lmarena_math_elo": 1475,
				"lmarena_vision_elo": null,
				"lmarena_zh_elo": 1509,
				"math_500": 98,
				"mathvista": 79.5,
				"mmlu_pro": 88,
				"mmmlu": 88.5,
				"mmmu": 76,
				"mmmu_pro": null,
				"osworld": 52.5,
				"swe_bench": 75,
				"tau_bench": 62.5,
				"terminal_bench": 56.5,
				"webdev_arena_elo": 1358
			}
		},
		{
			"id": "llama-4-maverick-17b-128e-instruct",
			"name": "Llama 4 Maverick",
			"provider": "Meta",
			"type": "open-source",
			"release_date": "2025-04-05",
			"pricing": {
				"input_per_1m": 0,
				"output_per_1m": 0,
				"average_per_1m": 0
			},
			"performance": {
				"output_speed_tps": 155,
				"latency_ttft_ms": 380,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Open-weights model with strong coding. Excellent LiveCodeBench performance. Free for self-hosting, competitive with closed models.",
			"benchmark_scores": {
				"aider_polyglot": 70,
				"aime": 62.5,
				"arc_agi_2": 7.8,
				"bfcl": 77.6,
				"frontiermath": null,
				"gpqa_diamond": 68.5,
				"humanity_last_exam": 20,
				"ifeval": null,
				"live_code_bench": 43.4,
				"livebench": null,
				"lmarena_coding_elo": 1295,
				"lmarena_creative_elo": 1305,
				"lmarena_en_elo": 1305,
				"lmarena_hard_elo": 1315,
				"lmarena_if_elo": 1335,
				"lmarena_math_elo": 1285,
				"lmarena_vision_elo": 1245,
				"lmarena_zh_elo": 1255,
				"math_500": 61.2,
				"mathvista": 58.5,
				"mmlu_pro": 85.5,
				"mmmlu": 78.5,
				"mmmu": 55.2,
				"mmmu_pro": null,
				"osworld": 32.5,
				"swe_bench": 58.5,
				"tau_bench": 48.5,
				"terminal_bench": 38.5,
				"webdev_arena_elo": 1255
			}
		},
		{
			"id": "llama-4-scout-17b-16e-instruct",
			"name": "Llama 4 Scout",
			"provider": "Meta",
			"type": "open-source",
			"release_date": "2025-04-05",
			"pricing": {
				"input_per_1m": 0,
				"output_per_1m": 0,
				"average_per_1m": 0
			},
			"performance": {
				"output_speed_tps": 2600,
				"latency_ttft_ms": 120,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Blazing fast with 10M token context. Exceptional speed at 2600 tokens/s. Ideal for high-throughput applications.",
			"benchmark_scores": {
				"aider_polyglot": null,
				"aime": 45.5,
				"arc_agi_2": 5.2,
				"bfcl": 72.5,
				"frontiermath": null,
				"gpqa_diamond": 52.5,
				"humanity_last_exam": null,
				"ifeval": null,
				"live_code_bench": 35.5,
				"livebench": null,
				"lmarena_coding_elo": 1225,
				"lmarena_creative_elo": 1255,
				"lmarena_en_elo": 1255,
				"lmarena_hard_elo": 1245,
				"lmarena_if_elo": 1285,
				"lmarena_math_elo": 1225,
				"lmarena_vision_elo": 1195,
				"lmarena_zh_elo": 1205,
				"math_500": 52.5,
				"mathvista": 48.5,
				"mmlu_pro": null,
				"mmmlu": 72.5,
				"mmmu": 45.5,
				"mmmu_pro": null,
				"osworld": 25.5,
				"swe_bench": 42.5,
				"tau_bench": 38.5,
				"terminal_bench": 28.5,
				"webdev_arena_elo": 1205
			}
		},
		{
			"id": "qwen3-235b-a22b-instruct-2507",
			"name": "Qwen3 235B",
			"provider": "Alibaba",
			"type": "open-source",
			"release_date": "2025-04-29",
			"pricing": {
				"input_per_1m": 0,
				"output_per_1m": 0,
				"average_per_1m": 0
			},
			"performance": {
				"output_speed_tps": 75,
				"latency_ttft_ms": 850,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Flagship open-source MoE model. Outperforms DeepSeek-R1 on 17/23 benchmarks. State-of-the-art reasoning among open models.",
			"benchmark_scores": {
				"aider_polyglot": 61.8,
				"aime": 82.5,
				"arc_agi_2": 9.5,
				"bfcl": 82.5,
				"frontiermath": null,
				"gpqa_diamond": 78.5,
				"humanity_last_exam": null,
				"ifeval": null,
				"live_code_bench": 68.5,
				"livebench": null,
				"lmarena_coding_elo": 1335,
				"lmarena_creative_elo": 1355,
				"lmarena_en_elo": 1365,
				"lmarena_hard_elo": 1385,
				"lmarena_if_elo": 1380,
				"lmarena_math_elo": 1365,
				"lmarena_vision_elo": 1295,
				"lmarena_zh_elo": 1425,
				"math_500": 85.5,
				"mathvista": 68.5,
				"mmlu_pro": 78,
				"mmmlu": 85.5,
				"mmmu": 65.5,
				"mmmu_pro": null,
				"osworld": 38.5,
				"swe_bench": 62.5,
				"tau_bench": 52.5,
				"terminal_bench": 45.5,
				"webdev_arena_elo": 1295
			}
		},
		{
			"id": "qwen3-32b",
			"name": "Qwen3 32B",
			"provider": "Alibaba",
			"type": "open-source",
			"release_date": "2025-04-29",
			"pricing": {
				"input_per_1m": 0,
				"output_per_1m": 0,
				"average_per_1m": 0
			},
			"performance": {
				"output_speed_tps": 145,
				"latency_ttft_ms": 320,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Largest dense Qwen3 model. Outperforms QwQ-32B while more efficient. Great for local deployment.",
			"benchmark_scores": {
				"aider_polyglot": null,
				"aime": 68.5,
				"arc_agi_2": 6.8,
				"bfcl": 75.5,
				"frontiermath": null,
				"gpqa_diamond": 65.5,
				"humanity_last_exam": null,
				"ifeval": null,
				"live_code_bench": 55.5,
				"livebench": null,
				"lmarena_coding_elo": 1275,
				"lmarena_creative_elo": 1295,
				"lmarena_en_elo": 1305,
				"lmarena_hard_elo": 1315,
				"lmarena_if_elo": 1345,
				"lmarena_math_elo": 1295,
				"lmarena_vision_elo": 1245,
				"lmarena_zh_elo": 1365,
				"math_500": 72.5,
				"mathvista": 58.5,
				"mmlu_pro": null,
				"mmmlu": 79.5,
				"mmmu": 55.5,
				"mmmu_pro": null,
				"osworld": 32.5,
				"swe_bench": 52.5,
				"tau_bench": 45.5,
				"terminal_bench": 35.5,
				"webdev_arena_elo": 1245
			}
		},
		{
			"id": "qwen3-max-preview",
			"name": "Qwen3 Max Preview",
			"provider": "Alibaba",
			"type": "proprietary",
			"release_date": "2025-12-01",
			"pricing": {
				"input_per_1m": 1.2,
				"output_per_1m": 6,
				"average_per_1m": 3.6
			},
			"performance": {
				"output_speed_tps": 85,
				"latency_ttft_ms": 750,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Alibaba's flagship proprietary model in preview. Top 3 on LMArena. Strong coding (SWE-Bench 69.6% verified) and math performance. Thinking variant achieves 100% on AIME 2025.",
			"benchmark_scores": {
				"aider_polyglot": null,
				"aime": 60.7,
				"arc_agi_2": null,
				"bfcl": 80.5,
				"frontiermath": null,
				"gpqa_diamond": 77.8,
				"humanity_last_exam": null,
				"ifeval": null,
				"live_code_bench": 66.9,
				"livebench": null,
				"lmarena_coding_elo": 1365,
				"lmarena_creative_elo": 1375,
				"lmarena_en_elo": 1440,
				"lmarena_hard_elo": 1425,
				"lmarena_if_elo": 1395,
				"lmarena_math_elo": 1385,
				"lmarena_vision_elo": 1305,
				"lmarena_zh_elo": 1445,
				"math_500": 82.5,
				"mathvista": 68.5,
				"mmlu_pro": null,
				"mmmlu": 83.5,
				"mmmu": 65.5,
				"mmmu_pro": null,
				"osworld": null,
				"swe_bench": 69.6,
				"tau_bench": null,
				"terminal_bench": null,
				"webdev_arena_elo": 1305
			}
		},
		{
			"id": "o3-2025-04-16",
			"name": "OpenAI o3",
			"provider": "OpenAI",
			"type": "proprietary",
			"release_date": "2025-04-16",
			"pricing": {
				"input_per_1m": 10,
				"output_per_1m": 40,
				"average_per_1m": 25
			},
			"performance": {
				"output_speed_tps": 35,
				"latency_ttft_ms": 2500,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Premier reasoning model with extended thinking. Excels at complex multi-step problems. Higher latency due to deliberation.",
			"benchmark_scores": {
				"aider_polyglot": 81.3,
				"aime": 86.5,
				"arc_agi_2": 6.5,
				"bfcl": 78.5,
				"frontiermath": 15,
				"gpqa_diamond": 83.3,
				"humanity_last_exam": 24,
				"ifeval": 87,
				"live_code_bench": 72.5,
				"livebench": 79,
				"lmarena_coding_elo": 1355,
				"lmarena_creative_elo": 1375,
				"lmarena_en_elo": 1455,
				"lmarena_hard_elo": 1485,
				"lmarena_if_elo": 1415,
				"lmarena_math_elo": 1455,
				"lmarena_vision_elo": 1218,
				"lmarena_zh_elo": 1385,
				"math_500": 95.8,
				"mathvista": 78.5,
				"mmlu_pro": 86.4,
				"mmmlu": 85.5,
				"mmmu": 74.5,
				"mmmu_pro": null,
				"osworld": 48.5,
				"swe_bench": 71.7,
				"tau_bench": 55.5,
				"terminal_bench": 58.5,
				"webdev_arena_elo": 1315
			}
		},
		{
			"id": "gemini-2.5-flash",
			"name": "Gemini 2.5 Flash",
			"provider": "Google",
			"type": "proprietary",
			"release_date": "2025-04-17",
			"pricing": {
				"input_per_1m": 0.15,
				"output_per_1m": 0.6,
				"average_per_1m": 0.38
			},
			"performance": {
				"output_speed_tps": 372,
				"latency_ttft_ms": 180,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Fastest major model at 372 tokens/s. Incredible value for speed-critical applications. Good reasoning at ultra-low cost.",
			"benchmark_scores": {
				"aider_polyglot": null,
				"aime": 65.5,
				"arc_agi_2": 6.5,
				"bfcl": 74.5,
				"frontiermath": null,
				"gpqa_diamond": 72.5,
				"humanity_last_exam": null,
				"ifeval": null,
				"live_code_bench": 52.5,
				"livebench": null,
				"lmarena_coding_elo": 1255,
				"lmarena_creative_elo": 1315,
				"lmarena_en_elo": 1335,
				"lmarena_hard_elo": 1325,
				"lmarena_if_elo": 1375,
				"lmarena_math_elo": 1325,
				"lmarena_vision_elo": 1305,
				"lmarena_zh_elo": 1305,
				"math_500": 78.5,
				"mathvista": 65.5,
				"mmlu_pro": null,
				"mmmlu": 80.5,
				"mmmu": 62.5,
				"mmmu_pro": null,
				"osworld": 35.5,
				"swe_bench": 48.5,
				"tau_bench": 45.5,
				"terminal_bench": 32.5,
				"webdev_arena_elo": 1313
			}
		},
		{
			"id": "minimax-m2",
			"name": "MiniMax M2",
			"provider": "MiniMax",
			"type": "open-source",
			"release_date": "2025-10-27",
			"pricing": {
				"input_per_1m": 0.3,
				"output_per_1m": 1.2,
				"average_per_1m": 0.75
			},
			"performance": {
				"output_speed_tps": 100,
				"latency_ttft_ms": 450,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "MIT-licensed 230B MoE model excelling at coding tasks. Tops Multi-SWE-Bench and WebDev Arena. Fast inference at 100 TPS with excellent value for agentic workflows.",
			"benchmark_scores": {
				"aider_polyglot": null,
				"aime": null,
				"arc_agi_2": null,
				"bfcl": null,
				"frontiermath": null,
				"gpqa_diamond": 78,
				"humanity_last_exam": null,
				"ifeval": null,
				"live_code_bench": null,
				"livebench": null,
				"lmarena_coding_elo": 1346,
				"lmarena_creative_elo": 1346,
				"lmarena_en_elo": 1346,
				"lmarena_hard_elo": 1346,
				"lmarena_if_elo": null,
				"lmarena_math_elo": 1346,
				"lmarena_vision_elo": null,
				"lmarena_zh_elo": null,
				"math_500": null,
				"mathvista": null,
				"mmlu_pro": null,
				"mmmlu": null,
				"mmmu": null,
				"mmmu_pro": null,
				"osworld": null,
				"swe_bench": 69.4,
				"tau_bench": null,
				"terminal_bench": null,
				"webdev_arena_elo": 1317
			}
		},
		{
			"id": "kimi-k2-0905-preview",
			"name": "Kimi K2",
			"provider": "Moonshot AI",
			"type": "open-source",
			"release_date": "2025-09-05",
			"pricing": {
				"input_per_1m": 0.6,
				"output_per_1m": 2.5,
				"average_per_1m": 1.55
			},
			"performance": {
				"output_speed_tps": 85,
				"latency_ttft_ms": 650,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Open-source MoE model (1T total, 32B active) with strong agentic performance. Competitive with Claude Sonnet 4 on SWE-bench at fraction of cost. 256K context window.",
			"benchmark_scores": {
				"aider_polyglot": null,
				"aime": 49.5,
				"arc_agi_2": null,
				"bfcl": 59.6,
				"frontiermath": null,
				"gpqa_diamond": 71.3,
				"humanity_last_exam": null,
				"ifeval": null,
				"live_code_bench": 53.7,
				"livebench": null,
				"lmarena_coding_elo": 1466,
				"lmarena_creative_elo": 1381,
				"lmarena_en_elo": null,
				"lmarena_hard_elo": 1433,
				"lmarena_if_elo": 1380,
				"lmarena_math_elo": 1417,
				"lmarena_vision_elo": null,
				"lmarena_zh_elo": null,
				"math_500": null,
				"mathvista": null,
				"mmlu_pro": null,
				"mmmlu": null,
				"mmmu": null,
				"mmmu_pro": null,
				"osworld": null,
				"swe_bench": 69.2,
				"tau_bench": null,
				"terminal_bench": 35.7,
				"webdev_arena_elo": null
			}
		},
		{
			"id": "kimi-k2-thinking-turbo",
			"name": "Kimi K2 Thinking",
			"provider": "Moonshot AI",
			"type": "open-source",
			"release_date": "2025-11-06",
			"pricing": {
				"input_per_1m": 0.6,
				"output_per_1m": 2.5,
				"average_per_1m": 1.55
			},
			"performance": {
				"output_speed_tps": 45,
				"latency_ttft_ms": 1800,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Thinking agent with multi-step reasoning and tool use. Sets open-source records on SWE-bench (71.3%) and Humanity's Last Exam (44.9% w/tools). Can execute 200-300 sequential tool calls.",
			"benchmark_scores": {
				"aider_polyglot": null,
				"aime": 94.5,
				"arc_agi_2": null,
				"bfcl": 90,
				"frontiermath": null,
				"gpqa_diamond": 84.5,
				"humanity_last_exam": null,
				"ifeval": null,
				"live_code_bench": 83,
				"livebench": null,
				"lmarena_coding_elo": 1479,
				"lmarena_creative_elo": 1402,
				"lmarena_en_elo": 1444,
				"lmarena_hard_elo": 1448,
				"lmarena_if_elo": 1416,
				"lmarena_math_elo": 1428,
				"lmarena_vision_elo": null,
				"lmarena_zh_elo": 1462,
				"math_500": 97.4,
				"mathvista": null,
				"mmlu_pro": null,
				"mmmlu": null,
				"mmmu": null,
				"mmmu_pro": null,
				"osworld": null,
				"swe_bench": 71.3,
				"tau_bench": 93,
				"terminal_bench": 47.1,
				"webdev_arena_elo": 1346
			}
		},
		{
			"id": "longcat-flash-chat",
			"name": "Longcat Flash Chat",
			"provider": "Meituan",
			"type": "open-source",
			"release_date": "2025-12-01",
			"pricing": {
				"input_per_1m": 0.15,
				"output_per_1m": 0.75,
				"average_per_1m": 0.45
			},
			"performance": {
				"output_speed_tps": 100,
				"latency_ttft_ms": 350,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Meituan's MoE model (560B total, 27B active) optimized for agentic and conversational tasks. 128K context with strong tool use and Chinese language performance.",
			"benchmark_scores": {
				"aider_polyglot": null,
				"aime": null,
				"arc_agi_2": null,
				"bfcl": 82,
				"frontiermath": null,
				"gpqa_diamond": 73.23,
				"humanity_last_exam": null,
				"ifeval": null,
				"live_code_bench": 48.02,
				"livebench": null,
				"lmarena_coding_elo": 1474,
				"lmarena_creative_elo": 1331,
				"lmarena_en_elo": 1425,
				"lmarena_hard_elo": null,
				"lmarena_if_elo": 1390,
				"lmarena_math_elo": 1429,
				"lmarena_vision_elo": null,
				"lmarena_zh_elo": 1414,
				"math_500": 96.4,
				"mathvista": null,
				"mmlu_pro": null,
				"mmmlu": null,
				"mmmu": null,
				"mmmu_pro": null,
				"osworld": null,
				"swe_bench": 60.4,
				"tau_bench": 67.7,
				"terminal_bench": 39.51,
				"webdev_arena_elo": null
			}
		},
		{
			"id": "mistral-large-3",
			"name": "Mistral Large 3",
			"provider": "Mistral",
			"type": "open-source",
			"release_date": "2025-12-02",
			"pricing": {
				"input_per_1m": 0.5,
				"output_per_1m": 1.5,
				"average_per_1m": 1
			},
			"performance": {
				"output_speed_tps": 90,
				"latency_ttft_ms": 450,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Mistral's flagship MoE model (675B total, 41B active). Top open-source coding model on LMArena. Excels at structured reasoning and math with competitive pricing.",
			"benchmark_scores": {
				"aider_polyglot": null,
				"aime": 53.3,
				"arc_agi_2": null,
				"bfcl": null,
				"frontiermath": null,
				"gpqa_diamond": 43.9,
				"humanity_last_exam": null,
				"ifeval": null,
				"live_code_bench": null,
				"livebench": null,
				"lmarena_coding_elo": 1335,
				"lmarena_creative_elo": 1335,
				"lmarena_en_elo": 1418,
				"lmarena_hard_elo": 1345,
				"lmarena_if_elo": null,
				"lmarena_math_elo": 1345,
				"lmarena_vision_elo": null,
				"lmarena_zh_elo": null,
				"math_500": 93.6,
				"mathvista": null,
				"mmlu_pro": null,
				"mmmlu": null,
				"mmmu": null,
				"mmmu_pro": null,
				"osworld": null,
				"swe_bench": 55,
				"tau_bench": null,
				"terminal_bench": 23.8,
				"webdev_arena_elo": null
			}
		}
	],
	"categories": [
		{
			"id": "coding",
			"name": "Coding",
			"emoji": "üíª",
			"weight": 0.25,
			"description": "Measures ability to write, edit, and debug code across multiple programming languages. Includes real-world GitHub issue resolution and competitive programming.",
			"benchmarks": [
				{
					"id": "swe_bench",
					"name": "SWE-Bench Verified",
					"type": "percentage",
					"weight": 0.45,
					"url": "https://swebench.com",
					"description": "Real-world GitHub issue resolution from popular Python repositories"
				},
				{
					"id": "terminal_bench",
					"name": "Terminal-Bench",
					"type": "percentage",
					"weight": 0.05,
					"url": "https://github.com/terminal-bench/terminal-bench",
					"description": "Terminal and command-line task completion"
				},
				{
					"id": "lmarena_coding_elo",
					"name": "LMArena Coding",
					"type": "elo",
					"weight": 0.25,
					"url": "https://lmarena.ai/leaderboard/text/coding",
					"description": "Human preference votes on coding tasks",
					"elo_range": {
						"min": 1100,
						"max": 1600
					}
				},
				{
					"id": "live_code_bench",
					"name": "LiveCodeBench",
					"type": "percentage",
					"weight": 0.15,
					"url": "https://livecodebench.github.io",
					"description": "Competitive programming problems from recent contests"
				},
				{
					"id": "aider_polyglot",
					"name": "AIDER Polyglot",
					"type": "percentage",
					"weight": 0.1,
					"url": "https://aider.chat/docs/leaderboards/",
					"description": "Multilingual coding benchmark testing code generation and editing"
				}
			]
		},
		{
			"id": "reasoning",
			"name": "Reasoning",
			"emoji": "üß†",
			"weight": 0.25,
			"description": "Complex problem-solving and PhD-level questions in science, mathematics, and logical reasoning.",
			"benchmarks": [
				{
					"id": "gpqa_diamond",
					"name": "GPQA Diamond",
					"type": "percentage",
					"weight": 0.4,
					"url": "https://artificialanalysis.ai/evaluations/gpqa-diamond",
					"description": "Graduate-level physics, chemistry, and biology questions"
				},
				{
					"id": "arc_agi_2",
					"name": "ARC-AGI-2",
					"type": "percentage",
					"weight": 0.3,
					"url": "https://arcprize.org/leaderboard#leaderboard-table",
					"description": "ARC-AGI-2 is a contamination-resistant benchmark that tests abstract reasoning and generalization in AI systems."
				},
				{
					"id": "livebench",
					"name": "LiveBench",
					"type": "percentage",
					"weight": 0.1,
					"url": "https://livebench.ai",
					"description": "Continuously updated benchmark with 21 diverse tasks resistant to contamination"
				},
				{
					"id": "humanity_last_exam",
					"name": "HLE",
					"type": "percentage",
					"weight": 0.05,
					"url": "https://scale.com/hle",
					"description": "Humanity's Last Exam: PhD-level questions across 100+ domains"
				},
				{
					"id": "lmarena_hard_elo",
					"name": "LMArena Hard Prompts",
					"type": "elo",
					"weight": 0.15,
					"url": "https://lmarena.ai/leaderboard/text/hard-prompts",
					"description": "Human preference on challenging reasoning tasks",
					"elo_range": {
						"min": 1100,
						"max": 1550
					}
				}
			]
		},
		{
			"id": "agents",
			"name": "Agents & Tools",
			"emoji": "ü§ñ",
			"weight": 0.18,
			"description": "Function calling, computer use, and tool integration capabilities for autonomous operation.",
			"benchmarks": [
				{
					"id": "bfcl",
					"name": "Berkeley Function Calling (BFCL)",
					"type": "percentage",
					"weight": 0.3,
					"url": "https://gorilla.cs.berkeley.edu/leaderboard.html",
					"description": "Function calling accuracy across multiple languages"
				},
				{
					"id": "tau_bench",
					"name": "TAU-Bench",
					"type": "percentage",
					"weight": 0.25,
					"url": "https://taubench.com/#leaderboard",
					"description": "œÑ¬≤-bench evaluates AI agents in dual-control environments where both agent and user actively collaborate using tools to solve shared problems."
				},
				{
					"id": "osworld",
					"name": "OSWorld",
					"type": "percentage",
					"weight": 0.25,
					"url": "https://os-world.github.io",
					"description": "Computer control and GUI interaction tasks"
				},
				{
					"id": "webdev_arena_elo",
					"name": "WebDev Arena",
					"type": "elo",
					"weight": 0.2,
					"url": "https://web.lmarena.ai/leaderboard",
					"description": "Web development task completion",
					"elo_range": {
						"min": 1100,
						"max": 1600
					}
				}
			]
		},
		{
			"id": "conversation",
			"name": "Conversation",
			"emoji": "üí¨",
			"weight": 0.12,
			"description": "Creative writing, instruction following, and conversational quality.",
			"benchmarks": [
				{
					"id": "lmarena_creative_elo",
					"name": "LMArena Creative Writing",
					"type": "elo",
					"weight": 0.35,
					"url": "https://lmarena.ai/leaderboard/text/creative-writing",
					"description": "Human preference on creative writing tasks",
					"elo_range": {
						"min": 1100,
						"max": 1600
					}
				},
				{
					"id": "lmarena_if_elo",
					"name": "LMArena Instruction Following",
					"type": "elo",
					"weight": 0.35,
					"url": "https://lmarena.ai/leaderboard/text/instruction-following",
					"description": "Human preference on instruction following tasks",
					"elo_range": {
						"min": 1100,
						"max": 1600
					}
				},
				{
					"id": "ifeval",
					"name": "IFEval",
					"type": "percentage",
					"weight": 0.3,
					"url": "https://github.com/google-research/google-research/tree/master/instruction_following_eval",
					"description": "Programmatically verifiable instruction following evaluation"
				}
			]
		},
		{
			"id": "math",
			"name": "Math",
			"emoji": "üî¢",
			"weight": 0.1,
			"description": "Mathematical problem solving from elementary to competition level.",
			"benchmarks": [
				{
					"id": "math_500",
					"name": "MATH-500",
					"type": "percentage",
					"weight": 0.5,
					"url": "https://github.com/hendrycks/math",
					"description": "Competition mathematics problems"
				},
				{
					"id": "aime",
					"name": "AIME 2025",
					"type": "percentage",
					"weight": 0.2,
					"url": "https://artofproblemsolving.com/wiki/index.php/AIME",
					"description": "American Invitational Mathematics Examination problems"
				},
				{
					"id": "lmarena_math_elo",
					"name": "LMArena Math",
					"type": "elo",
					"weight": 0.2,
					"url": "https://lmarena.ai/leaderboard/text/math",
					"description": "Human preference on math tasks",
					"elo_range": {
						"min": 1100,
						"max": 1600
					}
				},
				{
					"id": "frontiermath",
					"name": "FrontierMath",
					"type": "percentage",
					"weight": 0.1,
					"url": "https://epoch.ai/frontiermath",
					"description": "Research-level mathematics problems designed by experts"
				}
			]
		},
		{
			"id": "multimodal",
			"name": "Multimodal",
			"emoji": "üëÅÔ∏è",
			"weight": 0.07,
			"description": "Vision and text understanding, including charts, diagrams, and images.",
			"benchmarks": [
				{
					"id": "mathvista",
					"name": "MathVista",
					"type": "percentage",
					"weight": 0.2,
					"url": "https://mathvista.github.io",
					"description": "Math reasoning with visual context"
				},
				{
					"id": "mmmu",
					"name": "MMMU",
					"type": "percentage",
					"weight": 0.2,
					"url": "https://mmmu-benchmark.github.io",
					"description": "Massive multi-discipline multimodal understanding"
				},
				{
					"id": "mmmu_pro",
					"name": "MMMU-Pro",
					"type": "percentage",
					"weight": 0.2,
					"url": "https://mmmu-benchmark.github.io",
					"description": "Enhanced MMMU with more robust evaluation"
				},
				{
					"id": "lmarena_vision_elo",
					"name": "LMArena Vision",
					"type": "elo",
					"weight": 0.4,
					"url": "https://lmarena.ai/leaderboard/vision",
					"description": "Human preference on vision tasks",
					"elo_range": {
						"min": 1100,
						"max": 1450
					}
				}
			]
		},
		{
			"id": "knowledge",
			"name": "Knowledge",
			"emoji": "üß†",
			"weight": 0.03,
			"description": "Knowledge assessment across subjects and languages.",
			"benchmarks": [
				{
					"id": "mmlu_pro",
					"name": "MMLU-Pro",
					"type": "percentage",
					"weight": 0.4,
					"url": "https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro",
					"description": "Enhanced MMLU with 10 answer options and reasoning focus"
				},
				{
					"id": "mmmlu",
					"name": "MMMLU",
					"type": "percentage",
					"weight": 0.4,
					"url": "https://huggingface.co/datasets/openai/mmmlu",
					"description": "Multilingual MMLU across 14 languages"
				},
				{
					"id": "lmarena_en_elo",
					"name": "LMArena English",
					"type": "elo",
					"weight": 0.1,
					"url": "https://lmarena.ai/leaderboard",
					"description": "Human preference on English tasks",
					"elo_range": {
						"min": 1100,
						"max": 1600
					}
				},
				{
					"id": "lmarena_zh_elo",
					"name": "LMArena Chinese",
					"type": "elo",
					"weight": 0.1,
					"url": "https://lmarena.ai/leaderboard",
					"description": "Human preference on Chinese tasks",
					"elo_range": {
						"min": 1100,
						"max": 1600
					}
				}
			]
		}
	]
}
