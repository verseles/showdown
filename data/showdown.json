{
	"meta": {
		"version": "2026.02.24",
		"last_update": "2026-02-24T15:00:00Z",
		"schema_version": "1.0"
	},
	"models": [
		{
			"id": "minimax-m2-1",
			"name": "MiniMax M2.1",
			"aka": ["minimax-m2.1", "minimax-m21", "m2.1"],
			"provider": "MiniMax",
			"type": "open-source",
			"release_date": "2025-12-23",
			"pricing": {
				"input_per_1m": 0.3,
				"output_per_1m": 1.2,
				"average_per_1m": 0.75
			},
			"performance": {
				"output_speed_tps": 148,
				"latency_ttft_ms": 400,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Significant update to M2 with improved coding and agentic capabilities. Scores 81.0% on LiveCodeBench and 74.0% on SWE-Bench Verified.",
			"benchmark_scores": {
				"swe_bench": 74.0,
				"terminal_bench": 47.9,
				"live_code_bench": 81.0,
				"gpqa_diamond": 83.0,
				"aime": 83.0,
				"mmlu_pro": 88.0,
				"humanity_last_exam": 22.2,
				"lmarena_coding_elo": 1414,
				"lmarena_creative_elo": null,
				"lmarena_en_elo": null,
				"lmarena_hard_elo": null,
				"lmarena_if_elo": null,
				"lmarena_math_elo": null,
				"lmarena_vision_elo": null,
				"lmarena_zh_elo": null,
				"math_500": null,
				"mathvista": null,
				"mmmlu": null,
				"simpleqa": null,
				"mmmu": null,
				"mmmu_pro": null,
				"osworld": null,
				"tau_bench": 87.0,
				"webdev_arena_elo": 1445,
				"livebench_reasoning": null,
				"livebench_coding": null,
				"livebench_agentic_coding": null,
				"livebench_math": null,
				"livebench_data_analysis": null,
				"livebench_language": null,
				"livebench_if": null,
				"bfcl": null,
				"livebench": null,
				"frontiermath": null
			}
		},
		{
			"id": "minimax-m2-5",
			"name": "MiniMax M2.5",
			"aka": ["minimax-m2.5", "minimax-m25", "m2.5"],
			"provider": "MiniMax",
			"type": "open-source",
			"release_date": "2026-02-12",
			"pricing": {
				"input_per_1m": 0.15,
				"output_per_1m": 1.2,
				"average_per_1m": 0.68
			},
			"performance": {
				"output_speed_tps": 50,
				"latency_ttft_ms": 2170,
				"source": "https://artificialanalysis.ai/models/minimax-m2-5"
			},
			"editor_notes": "MiniMax's latest 229B MoE model focused on agentic coding. Officially reports 80.2% on SWE-Bench Verified, with additional AIME and GPQA scores captured from MiniMax public release material.",
			"benchmark_scores": {
				"swe_bench": 80.2,
				"terminal_bench": null,
				"live_code_bench": null,
				"gpqa_diamond": 85.2,
				"aime": 86.3,
				"mmlu_pro": null,
				"humanity_last_exam": null,
				"lmarena_coding_elo": null,
				"lmarena_creative_elo": null,
				"lmarena_en_elo": null,
				"lmarena_hard_elo": null,
				"lmarena_if_elo": null,
				"lmarena_math_elo": null,
				"lmarena_vision_elo": null,
				"lmarena_zh_elo": null,
				"math_500": null,
				"mathvista": null,
				"mmmlu": null,
				"simpleqa": null,
				"mmmu": null,
				"mmmu_pro": null,
				"osworld": null,
				"tau_bench": null,
				"webdev_arena_elo": null,
				"livebench_reasoning": null,
				"livebench_coding": null,
				"livebench_agentic_coding": null,
				"livebench_math": null,
				"livebench_data_analysis": null,
				"livebench_language": null,
				"livebench_if": null,
				"bfcl": null,
				"livebench": null,
				"frontiermath": null
			}
		},
		{
			"id": "claude-opus-4-6-20260205",
			"name": "Claude Opus 4.6",
			"aka": [
				"claude-opus-4-6",
				"claude-opus-4.6",
				"claude-4.6-opus",
				"opus-4.6",
				"claude-opus-4-6-base"
			],
			"inferior_of": "claude-opus-4-6-20260205-thinking-32k",
			"provider": "Anthropic",
			"type": "proprietary",
			"release_date": "2026-02-05",
			"pricing": {
				"input_per_1m": 5,
				"output_per_1m": 25,
				"average_per_1m": 15
			},
			"performance": {
				"output_speed_tps": 67.8,
				"latency_ttft_ms": 1650,
				"source": "https://artificialanalysis.ai/models/claude-opus-4-6/providers"
			},
			"editor_notes": "Latest Claude Opus generation with adaptive thinking controls and improved long-context reasoning. Base profile is kept conservative where only family-level benchmark claims are available.",
			"benchmark_scores": {
				"aime": null,
				"arc_agi_2": 64.6,
				"bfcl": null,
				"frontiermath": null,
				"gpqa_diamond": null,
				"humanity_last_exam": 40.0,
				"live_code_bench": null,
				"livebench": null,
				"lmarena_coding_elo": 1530,
				"lmarena_creative_elo": null,
				"lmarena_en_elo": 1490,
				"lmarena_hard_elo": null,
				"lmarena_if_elo": null,
				"lmarena_math_elo": null,
				"lmarena_vision_elo": null,
				"lmarena_zh_elo": null,
				"math_500": null,
				"mathvista": null,
				"mmlu_pro": 89.5,
				"mmmlu": 91.1,
				"simpleqa": null,
				"mmmu": null,
				"mmmu_pro": 73.9,
				"osworld": null,
				"swe_bench": 75.6,
				"tau_bench": null,
				"terminal_bench": 69.9,
				"webdev_arena_elo": null,
				"livebench_reasoning": null,
				"livebench_coding": null,
				"livebench_agentic_coding": null,
				"livebench_math": null,
				"livebench_data_analysis": null,
				"livebench_language": null,
				"livebench_if": null
			}
		},
		{
			"id": "claude-opus-4-6-20260205-thinking-32k",
			"name": "Claude Opus 4.6 Thinking",
			"aka": [
				"claude-opus-4-6-thinking",
				"claude-opus-4.6-thinking",
				"opus-4.6-thinking",
				"claude-opus-4-6-adaptive",
				"claude-opus-4.6-adaptive",
				"opus-4.6-adaptive"
			],
			"superior_of": "claude-opus-4-6-20260205",
			"provider": "Anthropic",
			"type": "proprietary",
			"release_date": "2026-02-05",
			"pricing": {
				"input_per_1m": 5,
				"output_per_1m": 25,
				"average_per_1m": 15
			},
			"performance": {
				"output_speed_tps": 67.8,
				"latency_ttft_ms": 1650,
				"source": "https://artificialanalysis.ai/models/claude-opus-4-6/providers"
			},
			"editor_notes": "High-effort adaptive thinking profile for Claude Opus 4.6. Family-level benchmark claims are attributed here while base variant remains null to allow conservative inferior_of imputation; SWE-Bench is populated from Anthropic's launch disclosure.",
			"benchmark_scores": {
				"aime": null,
				"arc_agi_2": 68.8,
				"bfcl": null,
				"frontiermath": 40.0,
				"gpqa_diamond": 91.3,
				"humanity_last_exam": 40.0,
				"live_code_bench": null,
				"livebench": null,
				"lmarena_coding_elo": 1545,
				"lmarena_creative_elo": null,
				"lmarena_en_elo": 1503,
				"lmarena_hard_elo": null,
				"lmarena_if_elo": null,
				"lmarena_math_elo": null,
				"lmarena_vision_elo": null,
				"lmarena_zh_elo": null,
				"math_500": null,
				"mathvista": null,
				"mmlu_pro": 89.7,
				"mmmlu": 91.1,
				"simpleqa": null,
				"mmmu": null,
				"mmmu_pro": 77.3,
				"osworld": 72.7,
				"swe_bench": 80.8,
				"tau_bench": null,
				"terminal_bench": 65.4,
				"webdev_arena_elo": null,
				"livebench_reasoning": null,
				"livebench_coding": null,
				"livebench_agentic_coding": null,
				"livebench_math": null,
				"livebench_data_analysis": null,
				"livebench_language": null,
				"livebench_if": null
			}
		},
		{
			"id": "claude-opus-4-5-20251101",
			"name": "Claude Opus 4.5",
			"aka": ["claude-opus-4-5", "claude-opus-4.5", "claude-4.5-opus", "opus-4.5"],
			"inferior_of": "claude-opus-4-5-20251101-thinking-32k",
			"provider": "Anthropic",
			"type": "proprietary",
			"release_date": "2025-11-24",
			"pricing": {
				"input_per_1m": 5,
				"output_per_1m": 25,
				"average_per_1m": 15
			},
			"performance": {
				"output_speed_tps": 65,
				"latency_ttft_ms": 3750,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Anthropic's most intelligent model. Excels at complex reasoning and advanced coding with extended autonomous operation. New pricing makes it more competitive.",
			"benchmark_scores": {
				"aime": 87,
				"arc_agi_2": 37.6,
				"bfcl": 82.5,
				"frontiermath": 20.7,
				"gpqa_diamond": 87,
				"humanity_last_exam": 25.2,
				"live_code_bench": 72.3,
				"livebench": 65.01,
				"lmarena_coding_elo": 1519,
				"lmarena_creative_elo": 1465,
				"lmarena_en_elo": 1478,
				"lmarena_hard_elo": 1497,
				"lmarena_if_elo": 1486,
				"lmarena_math_elo": 1472,
				"lmarena_vision_elo": 1188,
				"lmarena_zh_elo": 1486,
				"math_500": 88.5,
				"mathvista": 72.8,
				"mmlu_pro": 82,
				"mmmlu": 90.8,
				"simpleqa": 30,
				"mmmu": 80.7,
				"mmmu_pro": 80.7,
				"osworld": 61.4,
				"swe_bench": 80.9,
				"tau_bench": 64.2,
				"terminal_bench": 59.3,
				"webdev_arena_elo": 1479,
				"livebench_reasoning": 53.21,
				"livebench_coding": 78.51,
				"livebench_agentic_coding": 63.33,
				"livebench_math": 86.09,
				"livebench_data_analysis": 67.19,
				"livebench_language": 78.66,
				"livebench_if": 28.11
			},
			"imputed_metadata": {
				"simpleqa": {
					"original_value": null,
					"imputed_value": 30,
					"method": "estimated",
					"imputed_date": "2025-12-27",
					"note": "Estimated as ~70% of thinking variant (41.8)"
				}
			}
		},
		{
			"id": "claude-opus-4-5-20251101-thinking-32k",
			"name": "Claude Opus 4.5 Thinking",
			"aka": ["claude-opus-4.5-thinking", "claude-opus-4-5-thinking", "opus-4.5-thinking"],
			"superior_of": "claude-opus-4-5-20251101",
			"provider": "Anthropic",
			"type": "proprietary",
			"release_date": "2025-11-24",
			"pricing": {
				"input_per_1m": 5,
				"output_per_1m": 25,
				"average_per_1m": 15
			},
			"performance": {
				"output_speed_tps": 35,
				"latency_ttft_ms": 8500,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Extended thinking variant of Opus 4.5. Excels at complex reasoning tasks with visible chain-of-thought. Won LMArena Triple Crown (Expert, WebDev, Math). Higher latency due to deliberation.",
			"benchmark_scores": {
				"aime": 86.1,
				"arc_agi_2": 37.6,
				"bfcl": 80.9,
				"frontiermath": 20.7,
				"gpqa_diamond": 86,
				"humanity_last_exam": 25.2,
				"live_code_bench": 75.5,
				"livebench": 75.61,
				"lmarena_coding_elo": 1538,
				"lmarena_creative_elo": 1458,
				"lmarena_en_elo": 1485,
				"lmarena_hard_elo": 1503,
				"lmarena_if_elo": 1479,
				"lmarena_math_elo": 1468,
				"lmarena_vision_elo": 1210,
				"lmarena_zh_elo": 1481,
				"math_500": 94.5,
				"mathvista": 78.5,
				"mmlu_pro": 87.3,
				"mmmlu": 90.8,
				"simpleqa": 41.8,
				"mmmu": 83,
				"mmmu_pro": 80.7,
				"osworld": 66.3,
				"swe_bench": 80.9,
				"tau_bench": 93.6,
				"terminal_bench": 59.3,
				"webdev_arena_elo": 1510,
				"livebench_reasoning": 80.09,
				"livebench_coding": 79.65,
				"livebench_agentic_coding": 63.33,
				"livebench_math": 90.39,
				"livebench_data_analysis": 71.98,
				"livebench_language": 81.26,
				"livebench_if": 62.55
			}
		},
		{
			"id": "claude-sonnet-4-6-20260217",
			"name": "Claude Sonnet 4.6",
			"aka": [
				"claude-sonnet-4-6",
				"claude-sonnet-4.6",
				"claude-4.6-sonnet",
				"sonnet-4.6",
				"claude-sonnet-4-6-base"
			],
			"inferior_of": "claude-sonnet-4-6-20260217-thinking-32k",
			"provider": "Anthropic",
			"type": "proprietary",
			"release_date": "2026-02-17",
			"pricing": {
				"input_per_1m": 3,
				"output_per_1m": 15,
				"average_per_1m": 9
			},
			"performance": {
				"output_speed_tps": 77,
				"latency_ttft_ms": 1200,
				"source": "https://artificialanalysis.ai/models/claude-4-5-sonnet"
			},
			"editor_notes": "Base profile for Claude Sonnet 4.6. Family-level benchmark disclosures are attributed to the thinking variant, keeping base benchmarks null for conservative inferior_of imputation.",
			"benchmark_scores": {
				"aime": null,
				"arc_agi_2": null,
				"bfcl": null,
				"frontiermath": null,
				"gpqa_diamond": null,
				"humanity_last_exam": null,
				"live_code_bench": null,
				"livebench": null,
				"lmarena_coding_elo": null,
				"lmarena_creative_elo": null,
				"lmarena_en_elo": null,
				"lmarena_hard_elo": null,
				"lmarena_if_elo": null,
				"lmarena_math_elo": null,
				"lmarena_vision_elo": null,
				"lmarena_zh_elo": null,
				"math_500": null,
				"mathvista": null,
				"mmlu_pro": null,
				"mmmlu": 89.3,
				"simpleqa": null,
				"mmmu": null,
				"mmmu_pro": null,
				"osworld": null,
				"swe_bench": null,
				"tau_bench": null,
				"terminal_bench": null,
				"webdev_arena_elo": null,
				"livebench_reasoning": null,
				"livebench_coding": null,
				"livebench_agentic_coding": null,
				"livebench_math": null,
				"livebench_data_analysis": null,
				"livebench_language": null,
				"livebench_if": null
			}
		},
		{
			"id": "claude-sonnet-4-6-20260217-thinking-32k",
			"name": "Claude Sonnet 4.6 Thinking",
			"aka": [
				"claude-sonnet-4-6-thinking",
				"claude-sonnet-4.6-thinking",
				"sonnet-4.6-thinking",
				"claude-sonnet-4-6-adaptive",
				"claude-sonnet-4.6-adaptive",
				"sonnet-4.6-adaptive"
			],
			"superior_of": "claude-sonnet-4-6-20260217",
			"provider": "Anthropic",
			"type": "proprietary",
			"release_date": "2026-02-17",
			"pricing": {
				"input_per_1m": 3,
				"output_per_1m": 15,
				"average_per_1m": 9
			},
			"performance": {
				"output_speed_tps": 45,
				"latency_ttft_ms": 5500,
				"source": "https://artificialanalysis.ai/models/claude-4-5-sonnet"
			},
			"editor_notes": "High-effort adaptive thinking profile for Claude Sonnet 4.6 with benchmark values populated from Anthropic's official system card (single-source fallback where independent mirrors are unavailable).",
			"benchmark_scores": {
				"aime": 95.6,
				"arc_agi_2": 58.3,
				"bfcl": null,
				"frontiermath": null,
				"gpqa_diamond": 89.9,
				"humanity_last_exam": 33.2,
				"live_code_bench": null,
				"livebench": null,
				"lmarena_coding_elo": null,
				"lmarena_creative_elo": null,
				"lmarena_en_elo": null,
				"lmarena_hard_elo": null,
				"lmarena_if_elo": null,
				"lmarena_math_elo": null,
				"lmarena_vision_elo": null,
				"lmarena_zh_elo": null,
				"math_500": null,
				"mathvista": null,
				"mmlu_pro": null,
				"mmmlu": 89.3,
				"simpleqa": null,
				"mmmu": null,
				"mmmu_pro": 74.5,
				"osworld": 72.5,
				"swe_bench": 79.6,
				"tau_bench": null,
				"terminal_bench": 59.1,
				"webdev_arena_elo": null,
				"livebench_reasoning": null,
				"livebench_coding": null,
				"livebench_agentic_coding": null,
				"livebench_math": null,
				"livebench_data_analysis": null,
				"livebench_language": null,
				"livebench_if": null
			}
		},
		{
			"id": "claude-sonnet-4-5-20250929",
			"name": "Claude Sonnet 4.5",
			"aka": ["claude-sonnet-4-5", "claude-sonnet-4.5", "sonnet-4.5"],
			"inferior_of": "claude-sonnet-4-5-20250929-thinking-32k",
			"provider": "Anthropic",
			"type": "proprietary",
			"release_date": "2025-09-29",
			"pricing": {
				"input_per_1m": 3,
				"output_per_1m": 15,
				"average_per_1m": 9
			},
			"performance": {
				"output_speed_tps": 77,
				"latency_ttft_ms": 1200,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Best-in-class for software engineering. State-of-the-art on SWE-bench with excellent balance of speed and capability.",
			"benchmark_scores": {
				"aime": 87,
				"arc_agi_2": 13.6,
				"bfcl": 80.8,
				"frontiermath": 9.3,
				"gpqa_diamond": 83.4,
				"humanity_last_exam": 13.7,
				"live_code_bench": 74.5,
				"livebench": 70.31,
				"lmarena_coding_elo": 1508,
				"lmarena_creative_elo": 1445,
				"lmarena_en_elo": 1467,
				"lmarena_hard_elo": 1478,
				"lmarena_if_elo": 1454,
				"lmarena_math_elo": 1423,
				"lmarena_vision_elo": 1295,
				"lmarena_zh_elo": 1468,
				"math_500": 82.3,
				"mathvista": 68.5,
				"mmlu_pro": 78,
				"mmmlu": 89.1,
				"simpleqa": 29.3,
				"mmmu": 66.2,
				"mmmu_pro": 68,
				"osworld": 61.4,
				"swe_bench": 77.2,
				"tau_bench": 58.5,
				"terminal_bench": 60.3,
				"webdev_arena_elo": 1387,
				"livebench_reasoning": 42.29,
				"livebench_coding": 76.07,
				"livebench_agentic_coding": 48.33,
				"livebench_math": 80.83,
				"livebench_data_analysis": 67.34,
				"livebench_language": 76,
				"livebench_if": 23.52
			},
			"imputed_metadata": {
				"livebench": {
					"original_value": null,
					"imputed_value": 70.31,
					"method": "manual",
					"imputed_date": "2026-01-18",
					"note": "Filled from Sonnet 4.5 Thinking as per request"
				}
			}
		},
		{
			"id": "claude-sonnet-4-5-20250929-thinking-32k",
			"name": "Claude Sonnet 4.5 Thinking",
			"aka": ["claude-sonnet-4.5-thinking", "claude-sonnet-4-5-thinking", "sonnet-4.5-thinking"],
			"superior_of": "claude-sonnet-4-5-20250929",
			"provider": "Anthropic",
			"type": "proprietary",
			"release_date": "2025-09-29",
			"pricing": {
				"input_per_1m": 3,
				"output_per_1m": 15,
				"average_per_1m": 9
			},
			"performance": {
				"output_speed_tps": 45,
				"latency_ttft_ms": 5500,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Extended thinking variant of Sonnet 4.5. Excels at complex reasoning tasks with visible chain-of-thought. Higher latency due to deliberation but improved math and coding benchmarks.",
			"benchmark_scores": {
				"aime": 77.8,
				"arc_agi_2": 13.6,
				"bfcl": 83.5,
				"frontiermath": 15.2,
				"gpqa_diamond": 81.7,
				"humanity_last_exam": 35,
				"live_code_bench": 77.2,
				"livebench": 70.31,
				"lmarena_coding_elo": 1520,
				"lmarena_creative_elo": 1444,
				"lmarena_en_elo": 1469,
				"lmarena_hard_elo": 1483,
				"lmarena_if_elo": 1463,
				"lmarena_math_elo": 1459,
				"lmarena_vision_elo": 1204,
				"lmarena_zh_elo": 1460,
				"math_500": 90.5,
				"mathvista": 74.5,
				"mmlu_pro": 87.4,
				"mmmlu": 84.5,
				"simpleqa": 23.6,
				"mmmu": 70.2,
				"mmmu_pro": 68,
				"osworld": 55.8,
				"swe_bench": 77.2,
				"tau_bench": 62.5,
				"terminal_bench": 63.5,
				"webdev_arena_elo": 1393,
				"livebench_reasoning": 77.59,
				"livebench_coding": 80.36,
				"livebench_agentic_coding": 53.33,
				"livebench_math": 79.31,
				"livebench_data_analysis": 71.76,
				"livebench_language": 76.45,
				"livebench_if": 53.35
			},
			"imputed_metadata": {
				"mmmu_pro": {
					"original_value": null,
					"imputed_value": 68,
					"method": "manual",
					"imputed_date": "2026-01-18",
					"note": "Filled from Sonnet 4.5 as per request"
				}
			}
		},
		{
			"id": "claude-opus-4-1-20250805",
			"name": "Claude Opus 4.1",
			"aka": ["claude-opus-4-1", "claude-opus-4.1", "claude-4.1-opus", "opus-4.1"],
			"provider": "Anthropic",
			"type": "proprietary",
			"release_date": "2025-08-05",
			"pricing": {
				"input_per_1m": 15,
				"output_per_1m": 75,
				"average_per_1m": 45
			},
			"performance": {
				"output_speed_tps": 52,
				"latency_ttft_ms": 4200,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Previous flagship with excellent sustained autonomous operation up to 30 hours. Strong at complex multi-step reasoning but higher cost.",
			"benchmark_scores": {
				"aime": 40,
				"arc_agi_2": 8.1,
				"bfcl": 78.5,
				"frontiermath": 5.9,
				"gpqa_diamond": 73.2,
				"humanity_last_exam": null,
				"live_code_bench": 68.2,
				"livebench": null,
				"lmarena_coding_elo": 1503,
				"lmarena_creative_elo": 1441,
				"lmarena_en_elo": 1457,
				"lmarena_hard_elo": 1476,
				"lmarena_if_elo": 1452,
				"lmarena_math_elo": 1434,
				"lmarena_vision_elo": 1285,
				"lmarena_zh_elo": 1451,
				"math_500": 80.2,
				"mathvista": 68.2,
				"mmlu_pro": null,
				"mmmlu": 81.5,
				"simpleqa": 34.8,
				"mmmu": 65.5,
				"mmmu_pro": null,
				"osworld": 58.2,
				"swe_bench": 74.5,
				"tau_bench": 62.8,
				"terminal_bench": 54.2,
				"webdev_arena_elo": 1386,
				"livebench_reasoning": 40.89,
				"livebench_coding": 76.07,
				"livebench_agentic_coding": 53.33,
				"livebench_math": 80.44,
				"livebench_data_analysis": 66.95,
				"livebench_language": 76.75,
				"livebench_if": 25.92
			}
		},
		{
			"id": "gpt-4o-2024-05-13",
			"name": "GPT-4o",
			"aka": ["gpt-4o", "gpt4o", "gpt-4o-base"],
			"provider": "OpenAI",
			"type": "proprietary",
			"release_date": "2024-05-13",
			"pricing": {
				"input_per_1m": 2.5,
				"output_per_1m": 10,
				"average_per_1m": 6.25
			},
			"performance": {
				"output_speed_tps": 110,
				"latency_ttft_ms": 320,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Excellent multimodal capabilities with very fast inference. Good value for general-purpose tasks but superseded by newer models on benchmarks.",
			"benchmark_scores": {
				"aime": 6.3,
				"arc_agi_2": 5.5,
				"bfcl": 76.2,
				"frontiermath": 0.3,
				"gpqa_diamond": 48.9,
				"humanity_last_exam": 26.6,
				"live_code_bench": 48.2,
				"livebench": 52.6,
				"lmarena_coding_elo": 1368,
				"lmarena_creative_elo": 1336,
				"lmarena_en_elo": 1358,
				"lmarena_hard_elo": 1336,
				"lmarena_if_elo": 1320,
				"lmarena_math_elo": 1308,
				"lmarena_vision_elo": 1162,
				"lmarena_zh_elo": 1331,
				"math_500": 76.6,
				"mathvista": 63.8,
				"mmlu_pro": 72.6,
				"mmmlu": 78.2,
				"simpleqa": 8,
				"mmmu": 58.5,
				"mmmu_pro": null,
				"osworld": 32.5,
				"swe_bench": 33.2,
				"tau_bench": 45.5,
				"terminal_bench": 28.5,
				"webdev_arena_elo": 1253,
				"livebench_reasoning": null,
				"livebench_coding": null,
				"livebench_agentic_coding": null,
				"livebench_math": null,
				"livebench_data_analysis": null,
				"livebench_language": null,
				"livebench_if": null
			},
			"imputed_metadata": {
				"simpleqa": {
					"original_value": null,
					"imputed_value": 8,
					"method": "estimated",
					"imputed_date": "2025-12-27",
					"note": "Estimated based on older model performance"
				}
			}
		},
		{
			"id": "gpt-4.5-preview-2025-02-27",
			"name": "GPT-4.5",
			"aka": ["gpt-4.5", "gpt-4.5-preview"],
			"superior_of": "gpt-4o-2024-05-13",
			"provider": "OpenAI",
			"type": "proprietary",
			"release_date": "2025-02-27",
			"pricing": {
				"input_per_1m": 3,
				"output_per_1m": 12,
				"average_per_1m": 7.5
			},
			"performance": {
				"output_speed_tps": 85,
				"latency_ttft_ms": 450,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Significant upgrade over 4o with improved factual accuracy and reasoning. Leading in factual accuracy on SimpleQA during release.",
			"benchmark_scores": {
				"aime": 75,
				"arc_agi_2": 7.5,
				"bfcl": 78.5,
				"gpqa_diamond": 65,
				"lmarena_en_elo": 1448,
				"lmarena_coding_elo": 1458,
				"simpleqa": 62.5,
				"mmmu": 74.4,
				"mmmlu": 85.1,
				"lmarena_hard_elo": 1439,
				"lmarena_math_elo": 1414,
				"lmarena_vision_elo": 1226,
				"lmarena_creative_elo": 1437,
				"lmarena_if_elo": 1435,
				"swe_bench": 38.0
			}
		},
		{
			"id": "gpt-5.1",
			"name": "GPT 5.1",
			"aka": ["gpt-5.1-base", "gpt-5.1-chat"],
			"superior_of": "gpt-4.5-preview-2025-02-27",
			"provider": "OpenAI",
			"type": "proprietary",
			"release_date": "2025-11-12",
			"pricing": {
				"input_per_1m": 1.5,
				"output_per_1m": 6,
				"average_per_1m": 3.75
			},
			"performance": {
				"output_speed_tps": 120,
				"latency_ttft_ms": 250,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "OpenAI's late 2025 standard model. Strong performance across coding and math with significantly improved speed over GPT-5.",
			"benchmark_scores": {
				"swe_bench": 76.3,
				"aime": 94,
				"lmarena_en_elo": 1442,
				"lmarena_coding_elo": 1473,
				"livebench": 72.39,
				"lmarena_hard_elo": 1454,
				"lmarena_math_elo": 1420,
				"lmarena_vision_elo": 1241
			}
		},
		{
			"id": "gpt-5.1-high",
			"name": "GPT 5.1 High",
			"aka": ["gpt-5.1-thinking", "gpt-5-1-high", "gpt5.1-high", "gpt-5.1-high-effort"],
			"superior_of": "gpt-5.1",
			"provider": "OpenAI",
			"type": "proprietary",
			"release_date": "2025-11-12",
			"pricing": {
				"input_per_1m": 15,
				"output_per_1m": 120,
				"average_per_1m": 67.5
			},
			"performance": {
				"output_speed_tps": 40,
				"latency_ttft_ms": 1600,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "AKA GPT-5.1 Thinking (high effort). Extended thinking variant with high reasoning effort. Scores 69 on Artificial Analysis Index. Higher latency and cost but excels on complex reasoning tasks.",
			"benchmark_scores": {
				"aime": 94,
				"arc_agi_2": 17.6,
				"bfcl": 84.5,
				"frontiermath": 31,
				"gpqa_diamond": 88.1,
				"humanity_last_exam": 26.5,
				"live_code_bench": 78,
				"livebench": 72.39,
				"lmarena_coding_elo": 1493,
				"lmarena_creative_elo": 1438,
				"lmarena_en_elo": 1468,
				"lmarena_hard_elo": 1476,
				"lmarena_if_elo": 1450,
				"lmarena_math_elo": 1465,
				"lmarena_vision_elo": 1249,
				"lmarena_zh_elo": 1496,
				"math_500": 92,
				"mathvista": 72.5,
				"mmlu_pro": 86.9,
				"mmmlu": 91,
				"simpleqa": 34.9,
				"mmmu": 85.4,
				"mmmu_pro": 76,
				"osworld": null,
				"swe_bench": 76.3,
				"tau_bench": null,
				"terminal_bench": 47.6,
				"webdev_arena_elo": 1392,
				"livebench_reasoning": 78.79,
				"livebench_coding": 72.49,
				"livebench_agentic_coding": 53.33,
				"livebench_math": 86.9,
				"livebench_data_analysis": 72.07,
				"livebench_language": 79.26,
				"livebench_if": 63.9
			}
		},
		{
			"id": "gpt-5.2",
			"name": "GPT 5.2",
			"aka": ["gpt-5.2-instant", "gpt-5.2-small"],
			"superior_of": "gpt-5.1-high",
			"provider": "OpenAI",
			"type": "proprietary",
			"release_date": "2025-12-11",
			"pricing": {
				"input_per_1m": 1.75,
				"output_per_1m": 14,
				"average_per_1m": 7.875
			},
			"performance": {
				"output_speed_tps": 187,
				"latency_ttft_ms": 150,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "OpenAI's latest standard model. Blazing fast inference at 187 TPS. Excellent balance of speed and intelligence.",
			"benchmark_scores": {
				"swe_bench": 80,
				"arc_agi_2": 43.3,
				"lmarena_en_elo": 1438,
				"lmarena_coding_elo": 1498,
				"lmarena_hard_elo": 1466,
				"lmarena_math_elo": 1444
			}
		},
		{
			"id": "gpt-5.2-high",
			"name": "GPT 5.2 High",
			"aka": ["gpt-5.2-thinking", "gpt-5.2-high-effort", "gpt-5-2-thinking", "gpt-5-2-high"],
			"superior_of": "gpt-5.2",
			"provider": "OpenAI",
			"type": "proprietary",
			"release_date": "2025-12-11",
			"pricing": {
				"input_per_1m": 1.75,
				"output_per_1m": 14,
				"average_per_1m": 7.875
			},
			"performance": {
				"output_speed_tps": 45,
				"latency_ttft_ms": 1200,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "AKA GPT-5.2 Thinking (high effort). Extended thinking variant with 100% on AIME 2025. Top reasoning performance with 43.3% ARC-AGI-2 Verified. Achieves 80% on SWE-Bench and 55.6% on SWE-Bench Pro.",
			"benchmark_scores": {
				"aime": 100,
				"arc_agi_2": 43.3,
				"bfcl": 88.5,
				"frontiermath": 40.3,
				"gpqa_diamond": 92.4,
				"humanity_last_exam": 34.5,
				"live_code_bench": 78.5,
				"livebench": 74.07,
				"lmarena_coding_elo": 1492,
				"lmarena_creative_elo": 1388,
				"lmarena_en_elo": 1446,
				"lmarena_hard_elo": 1462,
				"lmarena_if_elo": 1437,
				"lmarena_math_elo": 1468,
				"lmarena_vision_elo": 1280,
				"lmarena_zh_elo": 1475,
				"math_500": 94.5,
				"mathvista": 78.5,
				"mmlu_pro": 86.2,
				"mmmlu": 89.5,
				"simpleqa": 38.2,
				"mmmu": 76.5,
				"mmmu_pro": 86.5,
				"osworld": 58.5,
				"swe_bench": 80,
				"tau_bench": 98.7,
				"terminal_bench": 52.5,
				"webdev_arena_elo": 1476,
				"livebench_reasoning": 83.21,
				"livebench_coding": 76.07,
				"livebench_agentic_coding": 51.67,
				"livebench_math": 93.17,
				"livebench_data_analysis": 72.79,
				"livebench_language": 79.81,
				"livebench_if": 61.77
			}
		},
		{
			"id": "gpt-5.2-pro",
			"name": "GPT 5.2 Pro",
			"aka": ["gpt-5.2-xhigh", "gpt-5.2-correlates", "gpt-5-2-pro", "gpt-5-2-xhigh"],
			"superior_of": "gpt-5.2-high",
			"provider": "OpenAI",
			"type": "proprietary",
			"release_date": "2025-12-11",
			"pricing": {
				"input_per_1m": 21,
				"output_per_1m": 168,
				"average_per_1m": 94.5
			},
			"performance": {
				"output_speed_tps": 28,
				"latency_ttft_ms": 2800,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "AKA GPT-5.2 xhigh (correlates). Premium tier with highest accuracy. Utilizing the Poetiq system, it achieves 75% on ARC-AGI-2 (public-eval), significantly surpassing human average level (60%). First model to score 93.2% GPQA Diamond.",
			"benchmark_scores": {
				"aime": 96.1,
				"arc_agi_2": 54.2,
				"bfcl": 90.5,
				"frontiermath": 40.7,
				"gpqa_diamond": 93.2,
				"humanity_last_exam": 36.6,
				"live_code_bench": 82,
				"livebench": 72.67,
				"lmarena_coding_elo": null,
				"lmarena_creative_elo": null,
				"lmarena_en_elo": null,
				"lmarena_hard_elo": null,
				"lmarena_if_elo": null,
				"lmarena_math_elo": null,
				"lmarena_vision_elo": null,
				"lmarena_zh_elo": null,
				"math_500": 96.5,
				"mathvista": 82.5,
				"mmlu_pro": null,
				"mmmlu": 91.5,
				"simpleqa": 38.9,
				"mmmu": 80.5,
				"mmmu_pro": 86.5,
				"osworld": 62.5,
				"swe_bench": 82.5,
				"tau_bench": 99.2,
				"terminal_bench": 55.5,
				"webdev_arena_elo": 1484,
				"livebench_reasoning": 81.69,
				"livebench_coding": 72.11,
				"livebench_agentic_coding": 51.67,
				"livebench_math": 94.22,
				"livebench_data_analysis": 72.42,
				"livebench_language": 80.69,
				"livebench_if": 63.96
			}
		},
		{
			"id": "gemini-3-1-pro-preview",
			"name": "Gemini 3.1 Pro Preview",
			"aka": ["gemini-3.1-pro", "gemini3.1-pro"],
			"provider": "Google",
			"type": "proprietary",
			"release_date": "2026-02-19",
			"pricing": {
				"input_per_1m": 2,
				"output_per_1m": 12,
				"average_per_1m": 7
			},
			"performance": {
				"output_speed_tps": 130,
				"latency_ttft_ms": 600,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Preview of the updated Gemini 3.1 Pro model. Very recent release, benchmark data is currently being collected.",
			"benchmark_scores": {
				"aime": null,
				"arc_agi_2": null,
				"bfcl": null,
				"frontiermath": null,
				"gpqa_diamond": null,
				"humanity_last_exam": null,
				"live_code_bench": null,
				"livebench": null,
				"lmarena_coding_elo": null,
				"lmarena_creative_elo": null,
				"lmarena_en_elo": null,
				"lmarena_hard_elo": null,
				"lmarena_if_elo": null,
				"lmarena_math_elo": null,
				"lmarena_vision_elo": null,
				"lmarena_zh_elo": null,
				"math_500": null,
				"mathvista": null,
				"mmlu_pro": null,
				"mmmlu": null,
				"simpleqa": null,
				"mmmu": null,
				"mmmu_pro": null,
				"osworld": null,
				"swe_bench": null,
				"tau_bench": null,
				"terminal_bench": null,
				"webdev_arena_elo": null,
				"livebench_reasoning": null,
				"livebench_coding": null,
				"livebench_agentic_coding": null,
				"livebench_math": null,
				"livebench_data_analysis": null,
				"livebench_language": null,
				"livebench_if": null
			},
			"inferior_of": "gemini-3-1-pro-preview-thinking"
		},
		{
			"id": "gemini-3-1-pro-preview-thinking",
			"name": "Gemini 3.1 Pro Preview Thinking",
			"aka": [
				"gemini-3.1-pro-thinking",
				"gemini3.1-pro-thinking",
				"gemini-3.1-pro-preview-thinking"
			],
			"provider": "Google",
			"type": "proprietary",
			"release_date": "2026-02-19",
			"pricing": {
				"input_per_1m": 2,
				"output_per_1m": 12,
				"average_per_1m": 7
			},
			"performance": {
				"output_speed_tps": 130,
				"latency_ttft_ms": 600,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Preview of the updated Gemini 3.1 Pro model. Very recent release, benchmark data is currently being collected.",
			"benchmark_scores": {
				"aime": null,
				"arc_agi_2": 77.1,
				"bfcl": null,
				"frontiermath": null,
				"gpqa_diamond": 94.3,
				"humanity_last_exam": 44.4,
				"live_code_bench": null,
				"livebench": null,
				"lmarena_coding_elo": 1531,
				"lmarena_creative_elo": null,
				"lmarena_en_elo": 1505,
				"lmarena_hard_elo": null,
				"lmarena_if_elo": null,
				"lmarena_math_elo": null,
				"lmarena_vision_elo": 1310,
				"lmarena_zh_elo": null,
				"math_500": null,
				"mathvista": null,
				"mmlu_pro": 91.0,
				"mmmlu": null,
				"simpleqa": null,
				"mmmu": null,
				"mmmu_pro": null,
				"osworld": null,
				"swe_bench": 80.6,
				"tau_bench": null,
				"terminal_bench": 68.5,
				"webdev_arena_elo": null,
				"livebench_reasoning": null,
				"livebench_coding": null,
				"livebench_agentic_coding": null,
				"livebench_math": null,
				"livebench_data_analysis": null,
				"livebench_language": null,
				"livebench_if": null
			},
			"superior_of": "gemini-3-1-pro-preview"
		},
		{
			"id": "gemini-3-pro",
			"name": "Gemini 3 Pro",
			"aka": ["gemini-3.0-pro", "gemini3-pro"],
			"provider": "Google",
			"type": "proprietary",
			"release_date": "2025-11-18",
			"pricing": {
				"input_per_1m": 2,
				"output_per_1m": 12,
				"average_per_1m": 7
			},
			"performance": {
				"output_speed_tps": 128,
				"latency_ttft_ms": 680,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "First model to break 1500 Elo on LMArena. Exceptional at algorithmic/competitive programming with breakthrough reasoning performance.",
			"benchmark_scores": {
				"aime": 95,
				"arc_agi_2": 31.1,
				"bfcl": 82.8,
				"frontiermath": 37.6,
				"gpqa_diamond": 91.9,
				"humanity_last_exam": 37.5,
				"live_code_bench": 78.5,
				"livebench": 73.46,
				"lmarena_coding_elo": 1518,
				"lmarena_creative_elo": 1490,
				"lmarena_en_elo": 1490,
				"lmarena_hard_elo": 1503,
				"lmarena_if_elo": 1475,
				"lmarena_math_elo": 1481,
				"lmarena_vision_elo": 1308,
				"lmarena_zh_elo": 1523,
				"math_500": 92.8,
				"mathvista": 78.5,
				"mmlu_pro": 90.1,
				"mmmlu": 91.8,
				"simpleqa": 72.1,
				"mmmu": 75.2,
				"mmmu_pro": 81,
				"osworld": 48.5,
				"swe_bench": 76.2,
				"tau_bench": 85.4,
				"terminal_bench": 54.2,
				"webdev_arena_elo": 1468,
				"livebench_reasoning": 77.42,
				"livebench_coding": 74.6,
				"livebench_agentic_coding": 55,
				"livebench_math": 81.84,
				"livebench_data_analysis": 74.91,
				"livebench_language": 84.62,
				"livebench_if": 65.85
			}
		},
		{
			"id": "gemini-3-flash",
			"name": "Gemini 3 Flash",
			"aka": ["gemini-3.0-flash", "gemini3-flash", "gemini-3-flash-non-thinking"],
			"provider": "Google",
			"type": "proprietary",
			"release_date": "2025-12-17",
			"pricing": {
				"input_per_1m": 0.5,
				"output_per_1m": 3,
				"average_per_1m": 1.75
			},
			"performance": {
				"output_speed_tps": 218,
				"latency_ttft_ms": 200,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Fastest intelligent model at 218 TPS. Minimal thinking mode. 3x faster than 2.5 Pro with 30% fewer tokens.",
			"benchmark_scores": {
				"aime": null,
				"arc_agi_2": 33.6,
				"bfcl": 92.8,
				"frontiermath": 35.6,
				"gpqa_diamond": null,
				"humanity_last_exam": 33.7,
				"live_code_bench": null,
				"livebench": 73.74,
				"lmarena_coding_elo": 1508,
				"lmarena_creative_elo": 1460,
				"lmarena_en_elo": 1476,
				"lmarena_hard_elo": 1491,
				"lmarena_if_elo": 1448,
				"lmarena_math_elo": 1474,
				"lmarena_vision_elo": 1280,
				"lmarena_zh_elo": 1517,
				"math_500": null,
				"mathvista": null,
				"mmlu_pro": 88.6,
				"mmmlu": 91.8,
				"simpleqa": 67.4,
				"mmmu": 81.2,
				"mmmu_pro": 81.2,
				"osworld": null,
				"swe_bench": null,
				"tau_bench": 90.2,
				"terminal_bench": null,
				"webdev_arena_elo": 1455,
				"livebench_reasoning": 74.55,
				"livebench_coding": 73.9,
				"livebench_agentic_coding": 40,
				"livebench_math": 93.57,
				"livebench_data_analysis": 74.74,
				"livebench_language": 84.56,
				"livebench_if": 74.86
			}
		},
		{
			"id": "gemini-3-flash-thinking",
			"name": "Gemini 3 Flash Thinking",
			"aka": ["gemini-3-flash-thinking-mode", "gemini3-flash-thinking", "gemini-3-flash-preview"],
			"superior_of": "gemini-3-flash",
			"provider": "Google",
			"type": "proprietary",
			"release_date": "2025-12-17",
			"pricing": {
				"input_per_1m": 0.5,
				"output_per_1m": 3,
				"average_per_1m": 1.75
			},
			"performance": {
				"output_speed_tps": 180,
				"latency_ttft_ms": 400,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Thinking mode of Gemini 3 Flash. Outperforms even Gemini 3 Pro on SWE-Bench (78%) at quarter the cost. Uses thinking_level parameter for controlled reasoning. #2 on LMArena Overall.",
			"benchmark_scores": {
				"aime": 95.2,
				"arc_agi_2": 33.6,
				"bfcl": 92.8,
				"frontiermath": 35.6,
				"gpqa_diamond": 90.4,
				"humanity_last_exam": 43.5,
				"live_code_bench": 90.8,
				"livebench": 72.4,
				"lmarena_coding_elo": 1502,
				"lmarena_creative_elo": 1466,
				"lmarena_en_elo": 1471,
				"lmarena_hard_elo": 1487,
				"lmarena_if_elo": 1452,
				"lmarena_math_elo": 1482,
				"lmarena_vision_elo": 1280,
				"lmarena_zh_elo": 1530,
				"math_500": null,
				"mathvista": null,
				"mmlu_pro": 88.6,
				"mmmlu": 91.8,
				"simpleqa": 67.4,
				"mmmu": 82,
				"mmmu_pro": 81.2,
				"osworld": null,
				"swe_bench": 78,
				"tau_bench": 92,
				"terminal_bench": 47.6,
				"webdev_arena_elo": 1455,
				"livebench_reasoning": 74.55,
				"livebench_coding": 73.9,
				"livebench_agentic_coding": 40,
				"livebench_math": 93.57,
				"livebench_data_analysis": 74.74,
				"livebench_language": 84.56,
				"livebench_if": 74.86
			},
			"imputed_metadata": {
				"simpleqa": {
					"original_value": null,
					"imputed_value": 67.4,
					"method": "estimated",
					"imputed_date": "2025-12-27",
					"note": "Using Gemini 3 Flash base value"
				},
				"mmmlu": {
					"original_value": null,
					"imputed_value": 91.8,
					"method": "manual",
					"imputed_date": "2026-01-18",
					"note": "Using Gemini 3 Flash base value per AGENTS.md fallback"
				},
				"mmmu_pro": {
					"original_value": null,
					"imputed_value": 81.2,
					"method": "manual",
					"imputed_date": "2026-01-18",
					"note": "Using Gemini 3 Flash base value per AGENTS.md fallback"
				}
			}
		},
		{
			"id": "gemini-2.5-pro",
			"name": "Gemini 2.5 Pro",
			"aka": ["gemini-2.5-pro", "gemini-25-pro", "gemini-2-5-pro"],
			"provider": "Google",
			"type": "proprietary",
			"release_date": "2025-03-25",
			"pricing": {
				"input_per_1m": 1.25,
				"output_per_1m": 5,
				"average_per_1m": 3.13
			},
			"performance": {
				"output_speed_tps": 165,
				"latency_ttft_ms": 520,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Excellent value with 1M token context window. Strong reasoning at lower cost but superseded by Gemini 3 on cutting-edge tasks.",
			"benchmark_scores": {
				"aime": 88,
				"arc_agi_2": 4.9,
				"bfcl": 79.5,
				"frontiermath": 14.1,
				"gpqa_diamond": 86.4,
				"humanity_last_exam": 21.6,
				"live_code_bench": 73.6,
				"livebench": 61.17,
				"lmarena_coding_elo": 1467,
				"lmarena_creative_elo": 1451,
				"lmarena_en_elo": 1452,
				"lmarena_hard_elo": 1460,
				"lmarena_if_elo": 1444,
				"lmarena_math_elo": 1452,
				"lmarena_vision_elo": 1248,
				"lmarena_zh_elo": 1495,
				"math_500": 85.5,
				"mathvista": 72.5,
				"mmlu_pro": 86.7,
				"mmmlu": 89.5,
				"simpleqa": 54.5,
				"mmmu": 81.7,
				"mmmu_pro": 68,
				"osworld": 42.5,
				"swe_bench": 63.8,
				"tau_bench": 54.9,
				"terminal_bench": 32.6,
				"webdev_arena_elo": 1209,
				"livebench_reasoning": 70.81,
				"livebench_coding": 75.69,
				"livebench_agentic_coding": 33.33,
				"livebench_math": 83.09,
				"livebench_data_analysis": 71.5,
				"livebench_language": 75.5,
				"livebench_if": 33.07
			}
		},
		{
			"id": "deepseek-v3.2",
			"name": "DeepSeek V3.2",
			"aka": ["deepseek-v3-2", "deepseek-v32", "deepseek-v3.2-base"],
			"provider": "DeepSeek",
			"type": "open-source",
			"release_date": "2025-12-01",
			"pricing": {
				"input_per_1m": 0.27,
				"output_per_1m": 1.1,
				"average_per_1m": 0.69
			},
			"performance": {
				"output_speed_tps": 120,
				"latency_ttft_ms": 450,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Outstanding value as open-source model. V3.2 improves reasoning and tool use. Strong coding performance at fraction of proprietary model costs.",
			"benchmark_scores": {
				"aime": 49.8,
				"arc_agi_2": 8.5,
				"bfcl": 80.5,
				"frontiermath": 22.1,
				"gpqa_diamond": 74.9,
				"humanity_last_exam": 30,
				"live_code_bench": 56.4,
				"livebench": 54.94,
				"lmarena_coding_elo": 1469,
				"lmarena_creative_elo": 1392,
				"lmarena_en_elo": 1436,
				"lmarena_hard_elo": 1443,
				"lmarena_if_elo": 1401,
				"lmarena_math_elo": 1427,
				"lmarena_vision_elo": 1444,
				"lmarena_zh_elo": 1463,
				"math_500": 82.8,
				"mathvista": 65.5,
				"mmlu_pro": 83.7,
				"mmmlu": 82.5,
				"simpleqa": 27.5,
				"mmmu": 62.5,
				"mmmu_pro": 83.7,
				"osworld": 38.5,
				"swe_bench": 66,
				"tau_bench": 55.2,
				"terminal_bench": 31.3,
				"webdev_arena_elo": 1285,
				"livebench_reasoning": 44.25,
				"livebench_coding": 75.69,
				"livebench_agentic_coding": 46.67,
				"livebench_math": 63.95,
				"livebench_data_analysis": 66.71,
				"livebench_language": 64.24,
				"livebench_if": 23.06
			}
		},
		{
			"id": "deepseek-v3.2-thinking",
			"name": "DeepSeek V3.2 Thinking",
			"aka": ["deepseek-v3-2-thinking", "deepseek-v32-thinking", "deepseek-v3.2-reasoning"],
			"superior_of": "deepseek-v3.2",
			"provider": "DeepSeek",
			"type": "open-source",
			"release_date": "2025-12-01",
			"pricing": {
				"input_per_1m": 0.27,
				"output_per_1m": 1.1,
				"average_per_1m": 0.69
			},
			"performance": {
				"output_speed_tps": 60,
				"latency_ttft_ms": 1000,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Thinking variant of DeepSeek V3.2 with enhanced reasoning capabilities and gold-medal performance in math olympiads.",
			"benchmark_scores": {
				"aime": 88.4,
				"arc_agi_2": 8.5,
				"bfcl": 80.5,
				"frontiermath": 22.1,
				"gpqa_diamond": 80.1,
				"humanity_last_exam": 15.9,
				"live_code_bench": 74.8,
				"livebench": 65.17,
				"lmarena_coding_elo": 1471,
				"lmarena_creative_elo": 1405,
				"lmarena_en_elo": 1443,
				"lmarena_hard_elo": 1443,
				"lmarena_if_elo": 1416,
				"lmarena_math_elo": 1412,
				"lmarena_vision_elo": 1440,
				"lmarena_zh_elo": 1470,
				"math_500": 78.5,
				"mathvista": 65.5,
				"mmlu_pro": 84.8,
				"mmmlu": 83.5,
				"simpleqa": 27.5,
				"mmmu": 62.5,
				"mmmu_pro": 83.7,
				"osworld": 38.5,
				"swe_bench": 66,
				"tau_bench": 55.2,
				"terminal_bench": 31,
				"webdev_arena_elo": 1290,
				"livebench_reasoning": 77.17,
				"livebench_coding": 64.62,
				"livebench_agentic_coding": 40,
				"livebench_math": 85.03,
				"livebench_data_analysis": 70.8,
				"livebench_language": 70.41,
				"livebench_if": 48.19
			}
		},
		{
			"id": "deepseek-r1",
			"name": "DeepSeek R1",
			"aka": ["deepseek-r1-2025-05-28", "deepseek-r1-0528", "deepseek-r1-reasoning"],
			"provider": "DeepSeek",
			"type": "open-source",
			"release_date": "2025-05-28",
			"pricing": {
				"input_per_1m": 0.55,
				"output_per_1m": 2.19,
				"average_per_1m": 1.37
			},
			"performance": {
				"output_speed_tps": 85,
				"latency_ttft_ms": 1200,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Reasoning-focused model with excellent math performance. Competitive with o3 on GPQA at fraction of cost.",
			"benchmark_scores": {
				"aime": 87.5,
				"arc_agi_2": 1.3,
				"bfcl": 75.8,
				"frontiermath": 1.7,
				"gpqa_diamond": 81.0,
				"humanity_last_exam": 17.7,
				"live_code_bench": 73.1,
				"livebench": 71.38,
				"lmarena_coding_elo": 1444,
				"lmarena_creative_elo": 1389,
				"lmarena_en_elo": 1430,
				"lmarena_hard_elo": 1433,
				"lmarena_if_elo": 1389,
				"lmarena_math_elo": 1400,
				"lmarena_vision_elo": 1413,
				"lmarena_zh_elo": 1419,
				"math_500": 97.3,
				"mathvista": 68.2,
				"mmlu_pro": 85,
				"mmmlu": 84,
				"simpleqa": 27.8,
				"mmmu": 64.5,
				"mmmu_pro": 85,
				"osworld": 35.2,
				"swe_bench": 57.6,
				"tau_bench": 58.7,
				"terminal_bench": 42.5,
				"webdev_arena_elo": 1265,
				"livebench_reasoning": 83.17,
				"livebench_coding": 66.74,
				"livebench_agentic_coding": null,
				"livebench_math": 79.54,
				"livebench_data_analysis": 69.78,
				"livebench_language": 48.53,
				"livebench_if": 80.5
			}
		},
		{
			"id": "grok-4-20",
			"name": "Grok 4.20",
			"aka": ["grok-4.20"],
			"provider": "xAI",
			"type": "proprietary",
			"release_date": "2026-02-19",
			"pricing": {
				"input_per_1m": 3,
				"output_per_1m": 15,
				"average_per_1m": 9
			},
			"performance": {
				"output_speed_tps": 100,
				"latency_ttft_ms": 800,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Newest Grok model release. Limited data available at launch.",
			"benchmark_scores": {
				"aime": null,
				"arc_agi_2": null,
				"bfcl": null,
				"frontiermath": null,
				"gpqa_diamond": null,
				"humanity_last_exam": null,
				"live_code_bench": null,
				"livebench": null,
				"lmarena_coding_elo": null,
				"lmarena_creative_elo": null,
				"lmarena_en_elo": null,
				"lmarena_hard_elo": null,
				"lmarena_if_elo": null,
				"lmarena_math_elo": null,
				"lmarena_vision_elo": null,
				"lmarena_zh_elo": null,
				"math_500": null,
				"mathvista": null,
				"mmlu_pro": null,
				"mmmlu": null,
				"simpleqa": null,
				"mmmu": null,
				"mmmu_pro": null,
				"osworld": null,
				"swe_bench": null,
				"tau_bench": null,
				"terminal_bench": null,
				"webdev_arena_elo": null,
				"livebench_reasoning": null,
				"livebench_coding": null,
				"livebench_agentic_coding": null,
				"livebench_math": null,
				"livebench_data_analysis": null,
				"livebench_language": null,
				"livebench_if": null
			},
			"inferior_of": "grok-4-20-thinking"
		},
		{
			"id": "grok-4-20-thinking",
			"name": "Grok 4.20 Thinking",
			"aka": ["grok-4.20-thinking", "grok-4.20-heavy"],
			"provider": "xAI",
			"type": "proprietary",
			"release_date": "2026-02-19",
			"pricing": {
				"input_per_1m": 3,
				"output_per_1m": 15,
				"average_per_1m": 9
			},
			"performance": {
				"output_speed_tps": 100,
				"latency_ttft_ms": 800,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Newest Grok model release. Limited data available at launch.",
			"benchmark_scores": {
				"aime": 100,
				"arc_agi_2": null,
				"bfcl": null,
				"frontiermath": null,
				"gpqa_diamond": 87.5,
				"humanity_last_exam": null,
				"live_code_bench": null,
				"livebench": null,
				"lmarena_coding_elo": null,
				"lmarena_creative_elo": null,
				"lmarena_en_elo": null,
				"lmarena_hard_elo": null,
				"lmarena_if_elo": null,
				"lmarena_math_elo": null,
				"lmarena_vision_elo": null,
				"lmarena_zh_elo": null,
				"math_500": null,
				"mathvista": null,
				"mmlu_pro": null,
				"mmmlu": null,
				"simpleqa": null,
				"mmmu": null,
				"mmmu_pro": null,
				"osworld": null,
				"swe_bench": null,
				"tau_bench": null,
				"terminal_bench": null,
				"webdev_arena_elo": null,
				"livebench_reasoning": null,
				"livebench_coding": null,
				"livebench_agentic_coding": null,
				"livebench_math": null,
				"livebench_data_analysis": null,
				"livebench_language": null,
				"livebench_if": null
			},
			"superior_of": "grok-4-20"
		},
		{
			"id": "grok-4.1",
			"name": "Grok 4.1",
			"aka": ["grok-4-1", "grok4.1"],
			"provider": "xAI",
			"type": "proprietary",
			"release_date": "2025-11-17",
			"pricing": {
				"input_per_1m": 3,
				"output_per_1m": 15,
				"average_per_1m": 9
			},
			"performance": {
				"output_speed_tps": 95,
				"latency_ttft_ms": 850,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "xAI's flagship successor to Grok 4. #2 on LMArena Overall (1465 Elo). 3x reduction in hallucinations vs Grok 4. Excels at emotional intelligence and conversation.",
			"benchmark_scores": {
				"aime": 84,
				"arc_agi_2": 15.9,
				"bfcl": 83.5,
				"frontiermath": 19.7,
				"gpqa_diamond": 87,
				"humanity_last_exam": 44.4,
				"live_code_bench": 70.5,
				"livebench": null,
				"lmarena_coding_elo": 1495,
				"lmarena_creative_elo": 1428,
				"lmarena_en_elo": 1477,
				"lmarena_hard_elo": 1476,
				"lmarena_if_elo": 1434,
				"lmarena_math_elo": 1439,
				"lmarena_vision_elo": 1465,
				"lmarena_zh_elo": 1508,
				"math_500": 97.2,
				"mathvista": 76.5,
				"mmlu_pro": 88,
				"mmmlu": 87.5,
				"simpleqa": 47.9,
				"mmmu": 73,
				"mmmu_pro": 72.7,
				"osworld": 48,
				"swe_bench": null,
				"tau_bench": 58.5,
				"terminal_bench": 54,
				"webdev_arena_elo": 1335,
				"livebench_reasoning": null,
				"livebench_coding": null,
				"livebench_agentic_coding": null,
				"livebench_math": null,
				"livebench_data_analysis": null,
				"livebench_language": null,
				"livebench_if": null
			}
		},
		{
			"id": "grok-4.1-thinking",
			"name": "Grok 4.1 Thinking",
			"aka": ["grok-4.1-think", "grok-4-1-thinking", "grok4.1-thinking"],
			"superior_of": "grok-4.1",
			"provider": "xAI",
			"type": "proprietary",
			"release_date": "2025-11-17",
			"pricing": {
				"input_per_1m": 3,
				"output_per_1m": 15,
				"average_per_1m": 9
			},
			"performance": {
				"output_speed_tps": 45,
				"latency_ttft_ms": 2200,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Extended thinking variant of Grok 4.1. #1 on LMArena Overall (1483 Elo). Deep reasoning with chain-of-thought. Top-tier emotional intelligence (1586 EQ-Bench Elo).",
			"benchmark_scores": {
				"aime": 84,
				"arc_agi_2": 26,
				"bfcl": 85.5,
				"frontiermath": 19.7,
				"gpqa_diamond": 87,
				"humanity_last_exam": 50.7,
				"live_code_bench": 74.5,
				"livebench": null,
				"lmarena_coding_elo": 1483,
				"lmarena_creative_elo": 1442,
				"lmarena_en_elo": 1482,
				"lmarena_hard_elo": 1485,
				"lmarena_if_elo": 1436,
				"lmarena_math_elo": 1450,
				"lmarena_vision_elo": null,
				"lmarena_zh_elo": 1498,
				"math_500": 98,
				"mathvista": 79.5,
				"mmlu_pro": 89,
				"mmmlu": 88.5,
				"simpleqa": 47.9,
				"mmmu": 76,
				"mmmu_pro": null,
				"osworld": 52.5,
				"swe_bench": null,
				"tau_bench": 62.5,
				"terminal_bench": 56.5,
				"webdev_arena_elo": 1202,
				"livebench_reasoning": null,
				"livebench_coding": null,
				"livebench_agentic_coding": null,
				"livebench_math": null,
				"livebench_data_analysis": null,
				"livebench_language": null,
				"livebench_if": null
			},
			"imputed_metadata": {
				"simpleqa": {
					"original_value": null,
					"imputed_value": 47.9,
					"method": "estimated",
					"imputed_date": "2025-12-27",
					"note": "Using Grok 4.1 base value"
				}
			}
		},
		{
			"id": "llama-4-maverick-17b-128e-instruct",
			"name": "Llama 4 Maverick",
			"aka": ["llama-4-maverick", "llama4-maverick", "llama-4-maverick-instruct"],
			"provider": "Meta",
			"type": "open-source",
			"release_date": "2025-04-05",
			"pricing": {
				"input_per_1m": 0,
				"output_per_1m": 0,
				"average_per_1m": 0
			},
			"performance": {
				"output_speed_tps": 155,
				"latency_ttft_ms": 380,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Open-weights model with strong coding. Excellent LiveCodeBench performance. Free for self-hosting, competitive with closed models.",
			"benchmark_scores": {
				"aime": 20.6,
				"arc_agi_2": 7.8,
				"bfcl": 77.6,
				"frontiermath": null,
				"gpqa_diamond": 67,
				"humanity_last_exam": 20,
				"live_code_bench": 43.4,
				"livebench": null,
				"lmarena_coding_elo": 1372,
				"lmarena_creative_elo": 1307,
				"lmarena_en_elo": 1342,
				"lmarena_hard_elo": 1338,
				"lmarena_if_elo": 1310,
				"lmarena_math_elo": 1325,
				"lmarena_vision_elo": 1146,
				"lmarena_zh_elo": 1310,
				"math_500": 61.2,
				"mathvista": 58.5,
				"mmlu_pro": 62.9,
				"mmmlu": 78.5,
				"simpleqa": 15,
				"mmmu": 55.2,
				"mmmu_pro": null,
				"osworld": 32.5,
				"swe_bench": 58.5,
				"tau_bench": 48.5,
				"terminal_bench": 38.5,
				"webdev_arena_elo": 1255,
				"livebench_reasoning": null,
				"livebench_coding": null,
				"livebench_agentic_coding": null,
				"livebench_math": null,
				"livebench_data_analysis": null,
				"livebench_language": null,
				"livebench_if": null
			},
			"imputed_metadata": {
				"simpleqa": {
					"original_value": null,
					"imputed_value": 15,
					"method": "estimated",
					"imputed_date": "2025-12-27",
					"note": "Estimated for open-source model"
				}
			}
		},
		{
			"id": "llama-4-scout-17b-16e-instruct",
			"name": "Llama 4 Scout",
			"aka": ["llama-4-scout", "llama4-scout", "llama-4-scout-instruct"],
			"provider": "Meta",
			"type": "open-source",
			"release_date": "2025-04-05",
			"pricing": {
				"input_per_1m": 0,
				"output_per_1m": 0,
				"average_per_1m": 0
			},
			"performance": {
				"output_speed_tps": 2600,
				"latency_ttft_ms": 120,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Blazing fast with 10M token context. Exceptional speed at 2600 tokens/s. Ideal for high-throughput applications.",
			"benchmark_scores": {
				"aime": 7.8,
				"arc_agi_2": 5.2,
				"bfcl": 72.5,
				"frontiermath": null,
				"gpqa_diamond": 51.8,
				"humanity_last_exam": null,
				"live_code_bench": 35.5,
				"livebench": null,
				"lmarena_coding_elo": 1361,
				"lmarena_creative_elo": 1290,
				"lmarena_en_elo": 1343,
				"lmarena_hard_elo": 1329,
				"lmarena_if_elo": 1296,
				"lmarena_math_elo": 1316,
				"lmarena_vision_elo": 1125,
				"lmarena_zh_elo": 1305,
				"math_500": 52.5,
				"mathvista": 48.5,
				"mmlu_pro": 58.2,
				"mmmlu": 72.5,
				"simpleqa": 10,
				"mmmu": 45.5,
				"mmmu_pro": null,
				"osworld": 25.5,
				"swe_bench": 42.5,
				"tau_bench": 38.5,
				"terminal_bench": 28.5,
				"webdev_arena_elo": 1205,
				"livebench_reasoning": null,
				"livebench_coding": null,
				"livebench_agentic_coding": null,
				"livebench_math": null,
				"livebench_data_analysis": null,
				"livebench_language": null,
				"livebench_if": null
			},
			"imputed_metadata": {
				"simpleqa": {
					"original_value": null,
					"imputed_value": 10,
					"method": "estimated",
					"imputed_date": "2025-12-27",
					"note": "Estimated for smaller open-source model"
				}
			}
		},
		{
			"id": "qwen3-235b-a22b-instruct-2507",
			"name": "Qwen3 235B",
			"aka": ["qwen3-235b-a22b-instruct", "qwen3-235b", "qwen3-235b-2507"],
			"provider": "Alibaba",
			"type": "open-source",
			"release_date": "2025-04-29",
			"pricing": {
				"input_per_1m": 0,
				"output_per_1m": 0,
				"average_per_1m": 0
			},
			"performance": {
				"output_speed_tps": 75,
				"latency_ttft_ms": 850,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Flagship open-source MoE model. Outperforms DeepSeek-R1 on 17/23 benchmarks. State-of-the-art reasoning among open models.",
			"benchmark_scores": {
				"aime": 86.7,
				"arc_agi_2": 9.5,
				"bfcl": 82.5,
				"frontiermath": 8.5,
				"gpqa_diamond": 80.1,
				"humanity_last_exam": null,
				"live_code_bench": 68.5,
				"livebench": 77.1,
				"lmarena_coding_elo": 1471,
				"lmarena_creative_elo": 1379,
				"lmarena_en_elo": 1433,
				"lmarena_hard_elo": 1447,
				"lmarena_if_elo": 1415,
				"lmarena_math_elo": 1426,
				"lmarena_vision_elo": 1209,
				"lmarena_zh_elo": 1465,
				"math_500": 85.5,
				"mathvista": 68.5,
				"mmlu_pro": 78,
				"mmmlu": 85.5,
				"simpleqa": 50.1,
				"mmmu": 65.5,
				"mmmu_pro": null,
				"osworld": 38.5,
				"swe_bench": 62.5,
				"tau_bench": 52.5,
				"terminal_bench": 45.5,
				"webdev_arena_elo": 1295,
				"livebench_reasoning": null,
				"livebench_coding": null,
				"livebench_agentic_coding": null,
				"livebench_math": null,
				"livebench_data_analysis": null,
				"livebench_language": null,
				"livebench_if": null
			}
		},
		{
			"id": "qwen3-32b",
			"name": "Qwen3 32B",
			"aka": ["qwen-3-32b", "qwen3-32b-instruct", "qwen-32b"],
			"provider": "Alibaba",
			"type": "open-source",
			"release_date": "2025-04-29",
			"pricing": {
				"input_per_1m": 0,
				"output_per_1m": 0,
				"average_per_1m": 0
			},
			"performance": {
				"output_speed_tps": 145,
				"latency_ttft_ms": 320,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Largest dense Qwen3 model. Outperforms QwQ-32B while more efficient. Great for local deployment.",
			"benchmark_scores": {
				"aime": 68.5,
				"arc_agi_2": 6.8,
				"bfcl": 75.5,
				"frontiermath": null,
				"gpqa_diamond": 65.5,
				"humanity_last_exam": null,
				"live_code_bench": 55.5,
				"livebench": 74.9,
				"lmarena_coding_elo": 1407,
				"lmarena_creative_elo": 1304,
				"lmarena_en_elo": 1366,
				"lmarena_hard_elo": 1403,
				"lmarena_if_elo": 1345,
				"lmarena_math_elo": 1403,
				"lmarena_vision_elo": 1327,
				"lmarena_zh_elo": 1365,
				"math_500": 72.5,
				"mathvista": 58.5,
				"mmlu_pro": null,
				"mmmlu": 79.5,
				"simpleqa": 25,
				"mmmu": 55.5,
				"mmmu_pro": 68.1,
				"osworld": 32.5,
				"swe_bench": 52.5,
				"tau_bench": 45.5,
				"terminal_bench": 35.5,
				"webdev_arena_elo": 1245,
				"livebench_reasoning": null,
				"livebench_coding": null,
				"livebench_agentic_coding": null,
				"livebench_math": null,
				"livebench_data_analysis": null,
				"livebench_language": null,
				"livebench_if": null
			},
			"imputed_metadata": {
				"simpleqa": {
					"original_value": null,
					"imputed_date": "2025-12-27",
					"imputed_value": 25,
					"method": "estimated",
					"note": "Estimated as ~50% of Qwen3 235B (50.1)"
				}
			}
		},
		{
			"id": "qwen3-max-preview",
			"name": "Qwen3 Max Preview",
			"aka": ["qwen3-max", "qwen-max-preview", "qwen3-max-preview-2025"],
			"provider": "Alibaba",
			"type": "proprietary",
			"release_date": "2025-12-01",
			"pricing": {
				"input_per_1m": 1.2,
				"output_per_1m": 6,
				"average_per_1m": 3.6
			},
			"performance": {
				"output_speed_tps": 85,
				"latency_ttft_ms": 750,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Alibaba's flagship proprietary model in preview. Top 3 on LMArena. Strong coding (SWE-Bench 69.6% verified) and math performance. Thinking variant achieves 100% on AIME 2025.",
			"benchmark_scores": {
				"aime": 73.3,
				"arc_agi_2": null,
				"bfcl": 80.5,
				"frontiermath": null,
				"gpqa_diamond": 72.6,
				"humanity_last_exam": null,
				"live_code_bench": 66.9,
				"livebench": null,
				"lmarena_coding_elo": 1482,
				"lmarena_creative_elo": 1397,
				"lmarena_en_elo": 1441,
				"lmarena_hard_elo": 1457,
				"lmarena_if_elo": 1425,
				"lmarena_math_elo": 1441,
				"lmarena_vision_elo": null,
				"lmarena_zh_elo": 1490,
				"math_500": 82.5,
				"mathvista": 68.5,
				"mmlu_pro": null,
				"mmmlu": 83.5,
				"simpleqa": 67.5,
				"mmmu": 65.5,
				"mmmu_pro": null,
				"osworld": null,
				"swe_bench": 69.6,
				"tau_bench": 74.8,
				"terminal_bench": 36.3,
				"webdev_arena_elo": 1305,
				"livebench_reasoning": null,
				"livebench_coding": null,
				"livebench_agentic_coding": null,
				"livebench_math": null,
				"livebench_data_analysis": null,
				"livebench_language": null,
				"livebench_if": null
			}
		},
		{
			"id": "o3-2025-04-16",
			"name": "OpenAI o3",
			"aka": ["o3", "openai-o3", "o3-2025-04-16"],
			"provider": "OpenAI",
			"type": "proprietary",
			"release_date": "2025-04-16",
			"pricing": {
				"input_per_1m": 10,
				"output_per_1m": 40,
				"average_per_1m": 25
			},
			"performance": {
				"output_speed_tps": 35,
				"latency_ttft_ms": 2500,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Premier reasoning model with extended thinking. Excels at complex multi-step problems. Higher latency due to deliberation.",
			"benchmark_scores": {
				"aime": 83.9,
				"arc_agi_2": 6.5,
				"bfcl": 78.5,
				"frontiermath": 18.7,
				"gpqa_diamond": 81.8,
				"humanity_last_exam": 24,
				"live_code_bench": 75.8,
				"livebench": 79,
				"lmarena_coding_elo": 1458,
				"lmarena_creative_elo": 1384,
				"lmarena_en_elo": 1440,
				"lmarena_hard_elo": 1439,
				"lmarena_if_elo": 1399,
				"lmarena_math_elo": 1453,
				"lmarena_vision_elo": 1217,
				"lmarena_zh_elo": 1459,
				"math_500": 95.8,
				"mathvista": 78.5,
				"mmlu_pro": 86.4,
				"mmmlu": 85.5,
				"simpleqa": 53,
				"mmmu": 74.5,
				"mmmu_pro": 76.4,
				"osworld": 48.5,
				"swe_bench": 71.7,
				"tau_bench": 55.5,
				"terminal_bench": 58.5,
				"webdev_arena_elo": 1315,
				"livebench_reasoning": null,
				"livebench_coding": null,
				"livebench_agentic_coding": null,
				"livebench_math": null,
				"livebench_data_analysis": null,
				"livebench_language": null,
				"livebench_if": null
			}
		},
		{
			"id": "o4-mini-2025-04-16",
			"name": "o4-mini",
			"aka": ["o4-mini", "o4-mini-high"],
			"provider": "OpenAI",
			"type": "proprietary",
			"release_date": "2025-04-16",
			"pricing": {
				"input_per_1m": 4.0,
				"output_per_1m": 16.0,
				"average_per_1m": 10.0
			},
			"performance": {
				"output_speed_tps": 100,
				"latency_ttft_ms": 1000,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Fast, cost-efficient reasoning model. Scores 80.2% on LiveCodeBench and 68.1% on SWE-Bench Verified. Features high reasoning effort.",
			"benchmark_scores": {
				"swe_bench": 68.1,
				"terminal_bench": null,
				"lmarena_coding_elo": 1431,
				"live_code_bench": 80.2,
				"gpqa_diamond": 81.4,
				"aime": 78.0,
				"arc_agi_2": null,
				"lmarena_hard_elo": 1405,
				"bfcl": null,
				"tau_bench": 60.5,
				"osworld": null,
				"webdev_arena_elo": null,
				"lmarena_creative_elo": null,
				"lmarena_if_elo": null,
				"math_500": 98.2,
				"lmarena_math_elo": 1419,
				"frontiermath": null,
				"mathvista": 84.3,
				"mmmu": 81.6,
				"mmmu_pro": null,
				"lmarena_vision_elo": null,
				"mmlu_pro": null,
				"mmmlu": 90.3,
				"simpleqa": 19.3,
				"lmarena_en_elo": 1405,
				"lmarena_zh_elo": null,
				"humanity_last_exam": 17.7
			}
		},
		{
			"id": "o3-mini-20250129",
			"name": "OpenAI o3-mini",
			"aka": ["o3-mini", "o3-mini-high"],
			"provider": "OpenAI",
			"type": "proprietary",
			"release_date": "2025-01-29",
			"pricing": {
				"input_per_1m": 1.1,
				"output_per_1m": 4.4,
				"average_per_1m": 2.75
			},
			"performance": {
				"output_speed_tps": 115,
				"latency_ttft_ms": 5200,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Cost-efficient reasoning model optimized for STEM and coding. #1 in math/reasoning for its price tier. Features 'high' reasoning effort mode.",
			"benchmark_scores": {
				"aime": 87.3,
				"arc_agi_2": 6,
				"bfcl": 98.7,
				"frontiermath": 9.2,
				"gpqa_diamond": 79.7,
				"humanity_last_exam": 14,
				"live_code_bench": 75.8,
				"livebench": 84.6,
				"lmarena_coding_elo": 1434,
				"lmarena_creative_elo": null,
				"lmarena_en_elo": 1381,
				"lmarena_hard_elo": 1400,
				"lmarena_if_elo": null,
				"lmarena_math_elo": 1409,
				"lmarena_vision_elo": null,
				"lmarena_zh_elo": null,
				"math_500": 97.9,
				"mathvista": null,
				"mmlu_pro": null,
				"mmmlu": 80.7,
				"simpleqa": 13.8,
				"mmmu": null,
				"mmmu_pro": null,
				"osworld": null,
				"swe_bench": 49.3,
				"tau_bench": 45.0,
				"terminal_bench": null,
				"webdev_arena_elo": 1146,
				"livebench_reasoning": 89.6,
				"livebench_coding": 82.7,
				"livebench_agentic_coding": null,
				"livebench_math": 76.5,
				"livebench_data_analysis": 70.64,
				"livebench_language": 50.7,
				"livebench_if": 84.4
			}
		},
		{
			"id": "gemini-2.5-flash",
			"name": "Gemini 2.5 Flash",
			"aka": ["gemini-2.5-flash", "gemini-25-flash", "gemini-2-5-flash"],
			"provider": "Google",
			"type": "proprietary",
			"release_date": "2025-05-20",
			"pricing": {
				"input_per_1m": 0.15,
				"output_per_1m": 0.6,
				"average_per_1m": 0.38
			},
			"performance": {
				"output_speed_tps": 372,
				"latency_ttft_ms": 180,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Fastest major model at 372 tokens/s. Incredible value for speed-critical applications. Good reasoning at ultra-low cost.",
			"benchmark_scores": {
				"aime": 65.5,
				"arc_agi_2": 6.5,
				"bfcl": 74.5,
				"frontiermath": null,
				"gpqa_diamond": 72.5,
				"humanity_last_exam": 11,
				"live_code_bench": 61.9,
				"livebench": null,
				"lmarena_coding_elo": 1424,
				"lmarena_creative_elo": 1397,
				"lmarena_en_elo": 1413,
				"lmarena_hard_elo": 1418,
				"lmarena_if_elo": 1401,
				"lmarena_math_elo": 1414,
				"lmarena_vision_elo": 1213,
				"lmarena_zh_elo": 1437,
				"math_500": 78.5,
				"mathvista": 65.5,
				"mmlu_pro": null,
				"mmmlu": 80.5,
				"simpleqa": 45,
				"mmmu": 62.5,
				"mmmu_pro": null,
				"osworld": 35.5,
				"swe_bench": 48.5,
				"tau_bench": 45.5,
				"terminal_bench": 32.5,
				"webdev_arena_elo": 1313,
				"livebench_reasoning": null,
				"livebench_coding": null,
				"livebench_agentic_coding": null,
				"livebench_math": null,
				"livebench_data_analysis": null,
				"livebench_language": null,
				"livebench_if": null
			},
			"imputed_metadata": {
				"simpleqa": {
					"original_value": null,
					"imputed_date": "2025-12-27",
					"imputed_value": 45,
					"method": "estimated",
					"note": "Estimated as ~80% of Gemini 2.5 Pro (56.0)"
				}
			}
		},
		{
			"id": "minimax-m2",
			"name": "MiniMax M2",
			"aka": ["minimax-m2.0", "minimax-m20", "m2"],
			"provider": "MiniMax",
			"type": "open-source",
			"release_date": "2025-10-27",
			"pricing": {
				"input_per_1m": 0.3,
				"output_per_1m": 1.2,
				"average_per_1m": 0.75
			},
			"performance": {
				"output_speed_tps": 100,
				"latency_ttft_ms": 450,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "MIT-licensed 230B MoE model excelling at coding tasks. Tops Multi-SWE-Bench and WebDev Arena. Fast inference at 100 TPS with excellent value for agentic workflows.",
			"benchmark_scores": {
				"aime": 92.7,
				"arc_agi_2": null,
				"bfcl": null,
				"frontiermath": null,
				"gpqa_diamond": 78,
				"humanity_last_exam": 12.5,
				"live_code_bench": 83,
				"livebench": null,
				"lmarena_coding_elo": 1385,
				"lmarena_creative_elo": 1289,
				"lmarena_en_elo": 1373,
				"lmarena_hard_elo": 1367,
				"lmarena_if_elo": 1333,
				"lmarena_math_elo": 1367,
				"lmarena_vision_elo": 1372,
				"lmarena_zh_elo": 1390,
				"math_500": null,
				"mathvista": null,
				"mmlu_pro": 82.0,
				"mmmlu": null,
				"simpleqa": 18,
				"mmmu": null,
				"mmmu_pro": null,
				"osworld": null,
				"swe_bench": 69.4,
				"tau_bench": 87.0,
				"terminal_bench": 30.0,
				"webdev_arena_elo": 1313,
				"livebench_reasoning": null,
				"livebench_coding": null,
				"livebench_agentic_coding": null,
				"livebench_math": null,
				"livebench_data_analysis": null,
				"livebench_language": null,
				"livebench_if": null
			},
			"imputed_metadata": {
				"simpleqa": {
					"original_value": null,
					"imputed_date": "2025-12-27",
					"imputed_value": 18,
					"method": "estimated",
					"note": "Estimated for open-source MoE model"
				}
			}
		},
		{
			"id": "kimi-k2-0905-preview",
			"name": "Kimi K2",
			"aka": ["kimi-k2", "kimi-k2-preview", "k2-0905"],
			"provider": "Moonshot AI",
			"type": "open-source",
			"release_date": "2025-09-05",
			"pricing": {
				"input_per_1m": 0.6,
				"output_per_1m": 2.5,
				"average_per_1m": 1.55
			},
			"performance": {
				"output_speed_tps": 85,
				"latency_ttft_ms": 650,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Open-source MoE model (1T total, 32B active) with strong agentic performance. Competitive with Claude Sonnet 4 on SWE-bench at fraction of cost. 256K context window.",
			"benchmark_scores": {
				"aime": 49.5,
				"arc_agi_2": 2.5,
				"bfcl": 59.6,
				"frontiermath": null,
				"gpqa_diamond": 71.3,
				"humanity_last_exam": null,
				"live_code_bench": 53.7,
				"livebench": 50.97,
				"lmarena_coding_elo": 1468,
				"lmarena_creative_elo": 1381,
				"lmarena_en_elo": 1424,
				"lmarena_hard_elo": 1434,
				"lmarena_if_elo": 1382,
				"lmarena_math_elo": 1416,
				"lmarena_vision_elo": null,
				"lmarena_zh_elo": 1444,
				"math_500": 97.4,
				"mathvista": null,
				"mmlu_pro": null,
				"mmmlu": null,
				"simpleqa": 20,
				"mmmu": null,
				"mmmu_pro": null,
				"osworld": null,
				"swe_bench": 71.2,
				"tau_bench": null,
				"terminal_bench": 35.7,
				"webdev_arena_elo": null,
				"livebench_reasoning": null,
				"livebench_coding": null,
				"livebench_agentic_coding": null,
				"livebench_math": null,
				"livebench_data_analysis": null,
				"livebench_language": null,
				"livebench_if": null
			},
			"imputed_metadata": {
				"simpleqa": {
					"original_value": null,
					"imputed_date": "2025-12-27",
					"imputed_value": 20,
					"method": "estimated",
					"note": "Estimated as ~65% of Kimi K2 Thinking (31.6)"
				}
			}
		},
		{
			"id": "kimi-k2-thinking-turbo",
			"name": "Kimi K2 Thinking",
			"aka": ["kimi-k2-thinking", "k2-thinking", "kimi-k2-think", "kimi-k2-thinking-turbo"],
			"superior_of": "kimi-k2-0905-preview",
			"provider": "Moonshot AI",
			"type": "open-source",
			"release_date": "2025-11-06",
			"pricing": {
				"input_per_1m": 0.6,
				"output_per_1m": 2.5,
				"average_per_1m": 1.55
			},
			"performance": {
				"output_speed_tps": 45,
				"latency_ttft_ms": 1800,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Thinking agent with multi-step reasoning and tool use. Sets open-source records on SWE-bench (71.3%) and Humanity's Last Exam (44.9% w/tools). Can execute 200-300 sequential tool calls.",
			"benchmark_scores": {
				"aime": 83.1,
				"arc_agi_2": 2.5,
				"bfcl": 90,
				"frontiermath": 21.4,
				"gpqa_diamond": 84.2,
				"humanity_last_exam": 23.9,
				"live_code_bench": 83,
				"livebench": 64.2,
				"lmarena_coding_elo": 1483,
				"lmarena_creative_elo": 1399,
				"lmarena_en_elo": 1447,
				"lmarena_hard_elo": 1450,
				"lmarena_if_elo": 1419,
				"lmarena_math_elo": 1440,
				"lmarena_vision_elo": null,
				"lmarena_zh_elo": 1456,
				"math_500": 97.4,
				"mathvista": null,
				"mmlu_pro": 89.8,
				"mmmlu": 87.7,
				"simpleqa": 31.6,
				"mmmu": null,
				"mmmu_pro": null,
				"osworld": null,
				"swe_bench": 71.3,
				"tau_bench": 93,
				"terminal_bench": 47.1,
				"webdev_arena_elo": 1337,
				"livebench_reasoning": null,
				"livebench_coding": null,
				"livebench_agentic_coding": null,
				"livebench_math": null,
				"livebench_data_analysis": null,
				"livebench_language": null,
				"livebench_if": null
			}
		},
		{
			"id": "kimi-k2.5-instant",
			"name": "Kimi K2.5 Instant",
			"aka": ["kimi-k2.5"],
			"provider": "Moonshot AI",
			"type": "open-source",
			"release_date": "2026-02-10",
			"pricing": {
				"input_per_1m": 0.6,
				"output_per_1m": 2.5,
				"average_per_1m": 1.55
			},
			"performance": {
				"output_speed_tps": 85,
				"latency_ttft_ms": 650,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "High-speed version of Kimi K2.5, offering strong performance across general tasks with low latency.",
			"benchmark_scores": {
				"swe_bench": null,
				"terminal_bench": null,
				"lmarena_coding_elo": 1517,
				"live_code_bench": null,
				"gpqa_diamond": null,
				"arc_agi_2": null,
				"livebench": null,
				"humanity_last_exam": null,
				"lmarena_hard_elo": 1470,
				"bfcl": null,
				"tau_bench": null,
				"osworld": null,
				"webdev_arena_elo": null,
				"lmarena_creative_elo": null,
				"lmarena_if_elo": null,
				"math_500": null,
				"aime": null,
				"lmarena_math_elo": 1447,
				"frontiermath": null,
				"mathvista": null,
				"mmmu": null,
				"mmmu_pro": null,
				"lmarena_vision_elo": null,
				"mmlu_pro": null,
				"mmmlu": null,
				"lmarena_en_elo": 1454,
				"lmarena_zh_elo": null,
				"simpleqa": null,
				"livebench_reasoning": null,
				"livebench_coding": null,
				"livebench_agentic_coding": null,
				"livebench_math": null,
				"livebench_data_analysis": null,
				"livebench_language": null,
				"livebench_if": null
			}
		},
		{
			"id": "kimi-k2.5-thinking",
			"name": "Kimi K2.5 Thinking",
			"aka": ["kimi-k2.5-thinking", "kimi-k25-thinking", "k2.5-thinking"],
			"superior_of": "kimi-k2.5-instant",
			"provider": "Moonshot AI",
			"type": "open-source",
			"release_date": "2026-02-10",
			"pricing": {
				"input_per_1m": 0.6,
				"output_per_1m": 2.5,
				"average_per_1m": 1.55
			},
			"performance": {
				"output_speed_tps": 45,
				"latency_ttft_ms": 1800,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Latest thinking model from Moonshot AI with enhanced reasoning and multi-step tool use. Optimized for complex coding and math tasks.",
			"benchmark_scores": {
				"swe_bench": 76.8,
				"terminal_bench": 50.8,
				"lmarena_coding_elo": 1513,
				"live_code_bench": 85.0,
				"gpqa_diamond": 87.6,
				"arc_agi_2": null,
				"livebench": 69.07,
				"humanity_last_exam": 30.1,
				"lmarena_hard_elo": 1470,
				"bfcl": null,
				"tau_bench": null,
				"osworld": null,
				"webdev_arena_elo": null,
				"lmarena_creative_elo": null,
				"lmarena_if_elo": null,
				"math_500": null,
				"aime": 96.1,
				"lmarena_math_elo": 1473,
				"frontiermath": null,
				"mathvista": null,
				"mmmu": null,
				"mmmu_pro": 78.5,
				"lmarena_vision_elo": null,
				"mmlu_pro": 87.1,
				"mmmlu": null,
				"lmarena_en_elo": 1459,
				"lmarena_zh_elo": null,
				"simpleqa": null,
				"livebench_reasoning": 75.96,
				"livebench_coding": 77.86,
				"livebench_agentic_coding": 48.33,
				"livebench_math": 84.87,
				"livebench_data_analysis": 61.36,
				"livebench_language": 77.67,
				"livebench_if": 57.41
			}
		},
		{
			"id": "longcat-flash-chat",
			"name": "Longcat Flash Chat",
			"aka": ["longcat-flash", "longcat-chat", "longcat-flashchat"],
			"provider": "Meituan",
			"type": "open-source",
			"release_date": "2025-12-01",
			"pricing": {
				"input_per_1m": 0.15,
				"output_per_1m": 0.75,
				"average_per_1m": 0.45
			},
			"performance": {
				"output_speed_tps": 100,
				"latency_ttft_ms": 350,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Meituan's MoE model (560B total, 27B active) optimized for agentic and conversational tasks. 128K context with strong tool use and Chinese language performance.",
			"benchmark_scores": {
				"aime": 68.3,
				"arc_agi_2": null,
				"bfcl": 82,
				"frontiermath": null,
				"gpqa_diamond": 73.23,
				"humanity_last_exam": null,
				"live_code_bench": 48.02,
				"livebench": null,
				"lmarena_coding_elo": 1475,
				"lmarena_creative_elo": 1388,
				"lmarena_en_elo": 1425,
				"lmarena_hard_elo": 1425,
				"lmarena_if_elo": 1388,
				"lmarena_math_elo": 1423,
				"lmarena_vision_elo": null,
				"lmarena_zh_elo": 1412,
				"math_500": 96.4,
				"mathvista": null,
				"mmlu_pro": null,
				"mmmlu": null,
				"simpleqa": 15,
				"mmmu": null,
				"mmmu_pro": null,
				"osworld": null,
				"swe_bench": 60.4,
				"tau_bench": 67.7,
				"terminal_bench": 39.51,
				"webdev_arena_elo": 1332,
				"livebench_reasoning": null,
				"livebench_coding": null,
				"livebench_agentic_coding": null,
				"livebench_math": null,
				"livebench_data_analysis": null,
				"livebench_language": null,
				"livebench_if": null
			},
			"imputed_metadata": {
				"simpleqa": {
					"original_value": null,
					"imputed_date": "2025-12-27",
					"imputed_value": 15,
					"method": "estimated",
					"note": "Estimated for open-source MoE model"
				}
			}
		},
		{
			"id": "mistral-large-3",
			"name": "Mistral Large 3",
			"aka": ["mistral-large-3.0", "mistral-large3", "mistral-large-v3"],
			"provider": "Mistral",
			"type": "open-source",
			"release_date": "2025-12-02",
			"pricing": {
				"input_per_1m": 0.5,
				"output_per_1m": 1.5,
				"average_per_1m": 1
			},
			"performance": {
				"output_speed_tps": 90,
				"latency_ttft_ms": 450,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Mistral's flagship MoE model (675B total, 41B active). Top open-source coding model on LMArena. Excels at structured reasoning and math with competitive pricing.",
			"benchmark_scores": {
				"aime": 53.3,
				"arc_agi_2": null,
				"bfcl": null,
				"frontiermath": null,
				"gpqa_diamond": 43.9,
				"humanity_last_exam": null,
				"live_code_bench": null,
				"livebench": null,
				"lmarena_coding_elo": 1468,
				"lmarena_creative_elo": 1366,
				"lmarena_en_elo": 1431,
				"lmarena_hard_elo": 1430,
				"lmarena_if_elo": 1404,
				"lmarena_math_elo": 1407,
				"lmarena_vision_elo": null,
				"lmarena_zh_elo": 1426,
				"math_500": 93.6,
				"mathvista": null,
				"mmlu_pro": 73.1,
				"mmmlu": 85.5,
				"simpleqa": 12,
				"mmmu": null,
				"mmmu_pro": null,
				"osworld": null,
				"swe_bench": 55,
				"tau_bench": null,
				"terminal_bench": 23.8,
				"webdev_arena_elo": 1222,
				"livebench_reasoning": null,
				"livebench_coding": null,
				"livebench_agentic_coding": null,
				"livebench_math": null,
				"livebench_data_analysis": null,
				"livebench_language": null,
				"livebench_if": null
			},
			"imputed_metadata": {
				"simpleqa": {
					"original_value": null,
					"imputed_date": "2025-12-27",
					"imputed_value": 12,
					"method": "estimated",
					"note": "Estimated for open-source MoE model"
				}
			}
		}
	],
	"categories": [
		{
			"id": "coding",
			"name": "Coding",
			"emoji": "",
			"weight": 0.25,
			"description": "Measures ability to write, edit, and debug code across multiple programming languages. Includes real-world GitHub issue resolution and competitive programming.",
			"benchmarks": [
				{
					"id": "swe_bench",
					"name": "SWE-Bench Verified",
					"type": "percentage",
					"weight": 0.46,
					"url": "https://swebench.com",
					"description": "Real-world GitHub issue resolution from popular Python repositories"
				},
				{
					"id": "terminal_bench",
					"name": "Terminal-Bench",
					"type": "percentage",
					"weight": 0.05,
					"url": "https://github.com/terminal-bench/terminal-bench",
					"description": "Terminal and command-line task completion"
				},
				{
					"id": "lmarena_coding_elo",
					"name": "LMArena Coding",
					"type": "elo",
					"weight": 0.22,
					"url": "https://lmarena.ai/leaderboard/text/coding",
					"description": "Human preference votes on coding tasks",
					"elo_range": {
						"min": 1100,
						"max": 1600
					}
				},
				{
					"id": "live_code_bench",
					"name": "LiveCodeBench",
					"type": "percentage",
					"weight": 0.16,
					"url": "https://livecodebench.github.io",
					"description": "Competitive programming problems from recent contests"
				},
				{
					"id": "livebench_coding",
					"name": "LiveBench Coding",
					"type": "percentage",
					"weight": 0.11,
					"url": "https://livebench.ai",
					"description": "Monthly-updated coding tasks resistant to contamination (LeetCode-style problems)"
				}
			]
		},
		{
			"id": "reasoning",
			"name": "Reasoning",
			"emoji": "",
			"weight": 0.25,
			"description": "Complex problem-solving and PhD-level questions in science, mathematics, and logical reasoning.",
			"benchmarks": [
				{
					"id": "gpqa_diamond",
					"name": "GPQA Diamond",
					"type": "percentage",
					"weight": 0.4,
					"url": "https://artificialanalysis.ai/evaluations/gpqa-diamond",
					"description": "Graduate-level physics, chemistry, and biology questions"
				},
				{
					"id": "arc_agi_2",
					"name": "ARC-AGI-2",
					"type": "percentage",
					"weight": 0.3,
					"url": "https://arcprize.org/leaderboard#leaderboard-table",
					"description": "ARC-AGI-2 is a contamination-resistant benchmark that tests abstract reasoning and generalization in AI systems."
				},
				{
					"id": "livebench",
					"name": "LiveBench Global",
					"type": "percentage",
					"weight": 0.05,
					"url": "https://livebench.ai",
					"description": "Continuously updated benchmark with 21 diverse tasks resistant to contamination (fallback)"
				},
				{
					"id": "livebench_reasoning",
					"name": "LiveBench Reasoning",
					"type": "percentage",
					"weight": 0.1,
					"url": "https://livebench.ai",
					"description": "Monthly-updated reasoning tasks: theory of mind, zebra puzzles, spatial reasoning"
				},
				{
					"id": "humanity_last_exam",
					"name": "HLE",
					"type": "percentage",
					"weight": 0.05,
					"url": "https://scale.com/hle",
					"description": "Humanity's Last Exam: PhD-level questions across 100+ domains"
				},
				{
					"id": "lmarena_hard_elo",
					"name": "LMArena Hard Prompts",
					"type": "elo",
					"weight": 0.1,
					"url": "https://lmarena.ai/leaderboard/text/hard-prompts",
					"description": "Human preference on challenging reasoning tasks",
					"elo_range": {
						"min": 1100,
						"max": 1550
					}
				}
			]
		},
		{
			"id": "agents",
			"name": "Agents & Tools",
			"emoji": "",
			"weight": 0.18,
			"description": "Function calling, computer use, and tool integration capabilities for autonomous operation.",
			"benchmarks": [
				{
					"id": "bfcl",
					"name": "Berkeley Function Calling (BFCL)",
					"type": "percentage",
					"weight": 0.25,
					"url": "https://gorilla.cs.berkeley.edu/leaderboard.html",
					"description": "Function calling accuracy across multiple languages"
				},
				{
					"id": "tau_bench",
					"name": "TAU-Bench",
					"type": "percentage",
					"weight": 0.2,
					"url": "https://taubench.com/#leaderboard",
					"description": "-bench evaluates AI agents in dual-control environments where both agent and user actively collaborate using tools to solve shared problems."
				},
				{
					"id": "osworld",
					"name": "OSWorld",
					"type": "percentage",
					"weight": 0.2,
					"url": "https://os-world.github.io",
					"description": "Computer control and GUI interaction tasks"
				},
				{
					"id": "webdev_arena_elo",
					"name": "WebDev Arena",
					"type": "elo",
					"weight": 0.15,
					"url": "https://web.lmarena.ai/leaderboard",
					"description": "Web development task completion",
					"elo_range": {
						"min": 1100,
						"max": 1600
					}
				},
				{
					"id": "livebench_agentic_coding",
					"name": "LiveBench Agentic Coding",
					"type": "percentage",
					"weight": 0.1,
					"url": "https://livebench.ai",
					"description": "Agentic coding with Mini-SWE-Agent for autonomous code completion"
				},
				{
					"id": "livebench_data_analysis",
					"name": "LiveBench Data Analysis",
					"type": "percentage",
					"weight": 0.1,
					"url": "https://livebench.ai",
					"description": "Table operations, joins, and type annotation tasks"
				}
			]
		},
		{
			"id": "conversation",
			"name": "Conversation",
			"emoji": "",
			"weight": 0.12,
			"description": "Creative writing, instruction following, and conversational quality.",
			"benchmarks": [
				{
					"id": "lmarena_creative_elo",
					"name": "LMArena Creative Writing",
					"type": "elo",
					"weight": 0.4375,
					"url": "https://lmarena.ai/leaderboard/text/creative-writing",
					"description": "Human preference on creative writing tasks",
					"elo_range": {
						"min": 1100,
						"max": 1600
					}
				},
				{
					"id": "lmarena_if_elo",
					"name": "LMArena Instruction Following",
					"type": "elo",
					"weight": 0.4375,
					"url": "https://lmarena.ai/leaderboard/text/instruction-following",
					"description": "Human preference on instruction following tasks",
					"elo_range": {
						"min": 1100,
						"max": 1600
					}
				},
				{
					"id": "livebench_if",
					"name": "LiveBench IF",
					"type": "percentage",
					"weight": 0.125,
					"url": "https://livebench.ai",
					"description": "Monthly-updated instruction following evaluation resistant to contamination"
				}
			]
		},
		{
			"id": "math",
			"name": "Math",
			"emoji": "",
			"weight": 0.1,
			"description": "Mathematical problem solving from elementary to competition level.",
			"benchmarks": [
				{
					"id": "math_500",
					"name": "MATH-500",
					"type": "percentage",
					"weight": 0.42,
					"url": "https://github.com/hendrycks/math",
					"description": "Competition mathematics problems"
				},
				{
					"id": "aime",
					"name": "AIME 2025",
					"type": "percentage",
					"weight": 0.2,
					"url": "https://artofproblemsolving.com/wiki/index.php/AIME",
					"description": "American Invitational Mathematics Examination problems"
				},
				{
					"id": "lmarena_math_elo",
					"name": "LMArena Math",
					"type": "elo",
					"weight": 0.2,
					"url": "https://lmarena.ai/leaderboard/text/math",
					"description": "Human preference on math tasks",
					"elo_range": {
						"min": 1100,
						"max": 1600
					}
				},
				{
					"id": "frontiermath",
					"name": "FrontierMath",
					"type": "percentage",
					"weight": 0.08,
					"url": "https://epoch.ai/frontiermath",
					"description": "Research-level mathematics problems designed by experts"
				},
				{
					"id": "livebench_math",
					"name": "LiveBench Math",
					"type": "percentage",
					"weight": 0.1,
					"url": "https://livebench.ai",
					"description": "Monthly-updated math from recent competitions (IMO, USAMO, AMPS)"
				}
			]
		},
		{
			"id": "multimodal",
			"name": "Multimodal",
			"emoji": "",
			"weight": 0.07,
			"description": "Vision and text understanding, including charts, diagrams, and images.",
			"benchmarks": [
				{
					"id": "mathvista",
					"name": "MathVista",
					"type": "percentage",
					"weight": 0.2,
					"url": "https://mathvista.github.io",
					"description": "Math reasoning with visual context"
				},
				{
					"id": "mmmu",
					"name": "MMMU",
					"type": "percentage",
					"weight": 0.2,
					"url": "https://mmmu-benchmark.github.io",
					"description": "Massive multi-discipline multimodal understanding"
				},
				{
					"id": "mmmu_pro",
					"name": "MMMU-Pro",
					"type": "percentage",
					"weight": 0.2,
					"url": "https://mmmu-benchmark.github.io",
					"description": "Enhanced MMMU with more robust evaluation"
				},
				{
					"id": "lmarena_vision_elo",
					"name": "LMArena Vision",
					"type": "elo",
					"weight": 0.4,
					"url": "https://lmarena.ai/leaderboard/vision",
					"description": "Human preference on vision tasks",
					"elo_range": {
						"min": 1100,
						"max": 1450
					}
				}
			]
		},
		{
			"id": "knowledge",
			"name": "Knowledge",
			"emoji": "",
			"weight": 0.03,
			"description": "Knowledge assessment across subjects and languages.",
			"benchmarks": [
				{
					"id": "mmlu_pro",
					"name": "MMLU-Pro",
					"type": "percentage",
					"weight": 0.3,
					"url": "https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro",
					"description": "Enhanced MMLU with 10 answer options and reasoning focus"
				},
				{
					"id": "mmmlu",
					"name": "MMMLU",
					"type": "percentage",
					"weight": 0.3,
					"url": "https://huggingface.co/datasets/openai/mmmlu",
					"description": "Multilingual MMLU across 14 languages"
				},
				{
					"id": "simpleqa",
					"name": "SimpleQA Verified",
					"type": "percentage",
					"weight": 0.2,
					"url": "https://openai.com/index/introducing-simpleqa/",
					"description": "1,000 factoid questions testing factual accuracy and hallucination detection"
				},
				{
					"id": "lmarena_en_elo",
					"name": "LMArena English",
					"type": "elo",
					"weight": 0.05,
					"url": "https://lmarena.ai/leaderboard",
					"description": "Human preference on English tasks",
					"elo_range": {
						"min": 1100,
						"max": 1600
					}
				},
				{
					"id": "lmarena_zh_elo",
					"name": "LMArena Chinese",
					"type": "elo",
					"weight": 0.05,
					"url": "https://lmarena.ai/leaderboard",
					"description": "Human preference on Chinese tasks",
					"elo_range": {
						"min": 1100,
						"max": 1600
					}
				},
				{
					"id": "livebench_language",
					"name": "LiveBench Language",
					"type": "percentage",
					"weight": 0.1,
					"url": "https://livebench.ai",
					"description": "Monthly-updated language understanding tasks across multiple domains"
				}
			]
		}
	]
}
