{
	"meta": {
		"version": "2025.12.10",
		"last_update": "2025-12-10T12:00:00Z",
		"schema_version": "1.0"
	},
	"models": [
		{
			"id": "claude-opus-4-5",
			"name": "Claude Opus 4.5",
			"provider": "Anthropic",
			"type": "proprietary",
			"release_date": "2025-11-24",
			"pricing": {
				"input_per_1m": 5.0,
				"output_per_1m": 25.0,
				"average_per_1m": 15.0
			},
			"performance": {
				"output_speed_tps": 65,
				"latency_ttft_ms": 3750,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Anthropic's most intelligent model. Excels at complex reasoning and advanced coding with extended autonomous operation. New pricing makes it more competitive.",
			"benchmark_scores": {
				"swe_bench": 80.9,
				"terminal_bench": 58.5,
				"lmarena_coding_elo": 1385,
				"live_code_bench": 72.3,
				"gpqa_diamond": 87.0,
				"aime_2024": 78.5,
				"arc_agi_2": 7.8,
				"lmarena_hard_elo": 1420,
				"bfcl": 82.5,
				"tau_bench": 64.2,
				"osworld": 61.4,
				"webdev_arena_elo": 1365,
				"lmarena_creative_elo": 1395,
				"lmarena_if_elo": 1471,
				"math_500": 88.5,
				"gsm8k": 97.2,
				"lmarena_math_elo": 1380,
				"mathvista": 72.8,
				"mmmu": 70.5,
				"lmarena_vision_elo": 1320,
				"mmlu": 90.8,
				"mmmlu": 85.2,
				"lmarena_en_elo": 1410,
				"lmarena_zh_elo": 1355
			}
		},
		{
			"id": "claude-opus-4-5-thinking",
			"name": "Claude Opus 4.5 Thinking",
			"provider": "Anthropic",
			"type": "proprietary",
			"release_date": "2025-11-24",
			"pricing": {
				"input_per_1m": 5.0,
				"output_per_1m": 25.0,
				"average_per_1m": 15.0
			},
			"performance": {
				"output_speed_tps": 35,
				"latency_ttft_ms": 8500,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Extended thinking variant of Opus 4.5. Excels at complex reasoning tasks with visible chain-of-thought. Won LMArena Triple Crown (Expert, WebDev, Math). Higher latency due to deliberation.",
			"benchmark_scores": {
				"swe_bench": 80.9,
				"terminal_bench": 59.3,
				"lmarena_coding_elo": 1537,
				"live_code_bench": 75.5,
				"gpqa_diamond": 85.9,
				"aime_2024": 95.4,
				"arc_agi_2": 37.6,
				"lmarena_hard_elo": 1497,
				"bfcl": 80.9,
				"tau_bench": 68.5,
				"osworld": 66.3,
				"webdev_arena_elo": 1511,
				"lmarena_creative_elo": 1453,
				"lmarena_if_elo": 1475,
				"math_500": 94.5,
				"gsm8k": 98.5,
				"lmarena_math_elo": 1468,
				"mathvista": 78.5,
				"mmmu": 83.0,
				"lmarena_vision_elo": 1210,
				"mmlu": 91.5,
				"mmmlu": 90.8,
				"lmarena_en_elo": 1487,
				"lmarena_zh_elo": 1492
			}
		},
		{
			"id": "claude-sonnet-4-5",
			"name": "Claude Sonnet 4.5",
			"provider": "Anthropic",
			"type": "proprietary",
			"release_date": "2025-09-29",
			"pricing": {
				"input_per_1m": 3.0,
				"output_per_1m": 15.0,
				"average_per_1m": 9.0
			},
			"performance": {
				"output_speed_tps": 77,
				"latency_ttft_ms": 1200,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Best-in-class for software engineering. State-of-the-art on SWE-bench with excellent balance of speed and capability.",
			"benchmark_scores": {
				"swe_bench": 82.0,
				"terminal_bench": 62.1,
				"lmarena_coding_elo": 1392,
				"live_code_bench": 74.5,
				"gpqa_diamond": 78.5,
				"aime_2024": 72.3,
				"arc_agi_2": 3.8,
				"lmarena_hard_elo": 1365,
				"bfcl": 80.8,
				"tau_bench": 58.5,
				"osworld": 44.0,
				"webdev_arena_elo": 1358,
				"lmarena_creative_elo": 1380,
				"lmarena_if_elo": 1452,
				"math_500": 82.3,
				"gsm8k": 95.8,
				"lmarena_math_elo": 1355,
				"mathvista": 68.5,
				"mmmu": 66.2,
				"lmarena_vision_elo": 1295,
				"mmlu": 88.5,
				"mmmlu": 82.8,
				"lmarena_en_elo": 1375,
				"lmarena_zh_elo": 1325
			}
		},
		{
			"id": "claude-sonnet-4-5-thinking",
			"name": "Claude Sonnet 4.5 Thinking",
			"provider": "Anthropic",
			"type": "proprietary",
			"release_date": "2025-09-29",
			"pricing": {
				"input_per_1m": 3.0,
				"output_per_1m": 15.0,
				"average_per_1m": 9.0
			},
			"performance": {
				"output_speed_tps": 45,
				"latency_ttft_ms": 5500,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Extended thinking variant of Sonnet 4.5. Excels at complex reasoning tasks with visible chain-of-thought. Higher latency due to deliberation but improved math and coding benchmarks.",
			"benchmark_scores": {
				"swe_bench": 82.0,
				"terminal_bench": 63.5,
				"lmarena_coding_elo": 1398,
				"live_code_bench": 77.2,
				"gpqa_diamond": 83.4,
				"aime_2024": 88.5,
				"arc_agi_2": 13.6,
				"lmarena_hard_elo": 1431,
				"bfcl": 83.5,
				"tau_bench": 62.5,
				"osworld": 55.8,
				"webdev_arena_elo": 1398,
				"lmarena_creative_elo": 1395,
				"lmarena_if_elo": 1457,
				"math_500": 90.5,
				"gsm8k": 97.5,
				"lmarena_math_elo": 1405,
				"mathvista": 74.5,
				"mmmu": 70.2,
				"lmarena_vision_elo": 1335,
				"mmlu": 89.8,
				"mmmlu": 84.5,
				"lmarena_en_elo": 1431,
				"lmarena_zh_elo": 1375
			}
		},
		{
			"id": "claude-opus-4-1",
			"name": "Claude Opus 4.1",
			"provider": "Anthropic",
			"type": "proprietary",
			"release_date": "2025-08-05",
			"pricing": {
				"input_per_1m": 15.0,
				"output_per_1m": 75.0,
				"average_per_1m": 45.0
			},
			"performance": {
				"output_speed_tps": 52,
				"latency_ttft_ms": 4200,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Previous flagship with excellent sustained autonomous operation up to 30 hours. Strong at complex multi-step reasoning but higher cost.",
			"benchmark_scores": {
				"swe_bench": 74.5,
				"terminal_bench": 54.2,
				"lmarena_coding_elo": 1355,
				"live_code_bench": 68.2,
				"gpqa_diamond": 74.9,
				"aime_2024": 70.5,
				"arc_agi_2": 8.1,
				"lmarena_hard_elo": 1385,
				"bfcl": 78.5,
				"tau_bench": 62.8,
				"osworld": 58.2,
				"webdev_arena_elo": 1335,
				"lmarena_creative_elo": 1365,
				"lmarena_if_elo": 1449,
				"math_500": 80.2,
				"gsm8k": 94.5,
				"lmarena_math_elo": 1345,
				"mathvista": 68.2,
				"mmmu": 65.5,
				"lmarena_vision_elo": 1285,
				"mmlu": 87.2,
				"mmmlu": 81.5,
				"lmarena_en_elo": 1360,
				"lmarena_zh_elo": 1310
			}
		},
		{
			"id": "gpt-4o",
			"name": "GPT-4o",
			"provider": "OpenAI",
			"type": "proprietary",
			"release_date": "2024-05-13",
			"pricing": {
				"input_per_1m": 2.5,
				"output_per_1m": 10.0,
				"average_per_1m": 6.25
			},
			"performance": {
				"output_speed_tps": 110,
				"latency_ttft_ms": 320,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Excellent multimodal capabilities with very fast inference. Good value for general-purpose tasks but superseded by newer models on benchmarks.",
			"benchmark_scores": {
				"swe_bench": 30.8,
				"terminal_bench": 28.5,
				"lmarena_coding_elo": 1285,
				"live_code_bench": 48.2,
				"gpqa_diamond": 53.6,
				"aime_2024": 45.2,
				"arc_agi_2": 5.5,
				"lmarena_hard_elo": 1285,
				"bfcl": 76.2,
				"tau_bench": 45.5,
				"osworld": 32.5,
				"webdev_arena_elo": 1253,
				"lmarena_creative_elo": 1295,
				"lmarena_if_elo": 1340,
				"math_500": 76.6,
				"gsm8k": 92.5,
				"lmarena_math_elo": 1275,
				"mathvista": 63.8,
				"mmmu": 58.5,
				"lmarena_vision_elo": 1285,
				"mmlu": 85.8,
				"mmmlu": 78.2,
				"lmarena_en_elo": 1295,
				"lmarena_zh_elo": 1245
			}
		},
		{
			"id": "gpt-5-1",
			"name": "GPT 5.1",
			"provider": "OpenAI",
			"type": "proprietary",
			"release_date": "2025-11-12",
			"pricing": {
				"input_per_1m": 1.25,
				"output_per_1m": 10.0,
				"average_per_1m": 5.625
			},
			"performance": {
				"output_speed_tps": 95,
				"latency_ttft_ms": 400,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "OpenAI's latest flagship with adaptive reasoning. 30% faster than GPT-4o with improved instruction following. First model to reach 70 on Artificial Analysis Index.",
			"benchmark_scores": {
				"swe_bench": 76.3,
				"terminal_bench": null,
				"lmarena_coding_elo": 1345,
				"live_code_bench": 68.5,
				"gpqa_diamond": 86.6,
				"aime_2024": 85.0,
				"arc_agi_2": 17.6,
				"lmarena_hard_elo": 1385,
				"bfcl": 82.5,
				"tau_bench": null,
				"osworld": null,
				"webdev_arena_elo": 1354,
				"lmarena_creative_elo": 1365,
				"lmarena_if_elo": 1420,
				"math_500": 85.5,
				"gsm8k": 97.1,
				"lmarena_math_elo": 1365,
				"mathvista": 68.5,
				"mmmu": 65.5,
				"lmarena_vision_elo": 1246,
				"mmlu": 92.3,
				"mmmlu": 85.0,
				"lmarena_en_elo": 1385,
				"lmarena_zh_elo": 1325
			}
		},
		{
			"id": "gpt-5-1-high",
			"name": "GPT 5.1 High",
			"provider": "OpenAI",
			"type": "proprietary",
			"release_date": "2025-11-12",
			"pricing": {
				"input_per_1m": 15.0,
				"output_per_1m": 120.0,
				"average_per_1m": 67.5
			},
			"performance": {
				"output_speed_tps": 40,
				"latency_ttft_ms": 1500,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Extended thinking variant with high reasoning effort. Scores 69 on Artificial Analysis Index. Higher latency and cost but excels on complex reasoning tasks.",
			"benchmark_scores": {
				"swe_bench": 76.3,
				"terminal_bench": 47.6,
				"lmarena_coding_elo": 1490,
				"live_code_bench": 78,
				"gpqa_diamond": 88.5,
				"aime_2024": 95.0,
				"arc_agi_2": 20.0,
				"lmarena_hard_elo": 1445,
				"bfcl": 84.5,
				"tau_bench": null,
				"osworld": null,
				"webdev_arena_elo": 1378,
				"lmarena_creative_elo": 1395,
				"lmarena_if_elo": 1446,
				"math_500": 92.0,
				"gsm8k": 98.5,
				"lmarena_math_elo": 1425,
				"mathvista": 72.5,
				"mmmu": 70.0,
				"lmarena_vision_elo": 1285,
				"mmlu": 93.5,
				"mmmlu": 87.5,
				"lmarena_en_elo": 1457,
				"lmarena_zh_elo": 1385
			}
		},
		{
			"id": "gemini-3-pro",
			"name": "Gemini 3 Pro",
			"provider": "Google",
			"type": "proprietary",
			"release_date": "2025-11-18",
			"pricing": {
				"input_per_1m": 2.0,
				"output_per_1m": 12.0,
				"average_per_1m": 7.0
			},
			"performance": {
				"output_speed_tps": 128,
				"latency_ttft_ms": 680,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "First model to break 1500 Elo on LMArena. Exceptional at algorithmic/competitive programming with breakthrough reasoning performance.",
			"benchmark_scores": {
				"swe_bench": 76.2,
				"terminal_bench": 54.2,
				"lmarena_coding_elo": 1521,
				"live_code_bench": 78.5,
				"gpqa_diamond": 91.9,
				"aime_2024": 95.0,
				"arc_agi_2": 31.1,
				"lmarena_hard_elo": 1504,
				"bfcl": 82.8,
				"tau_bench": 85.4,
				"osworld": 48.5,
				"webdev_arena_elo": 1476,
				"lmarena_creative_elo": 1493,
				"lmarena_if_elo": 1474,
				"math_500": 92.8,
				"gsm8k": 98.2,
				"lmarena_math_elo": 1480,
				"mathvista": 78.5,
				"mmmu": 75.2,
				"lmarena_vision_elo": 1314,
				"mmlu": 92.5,
				"mmmlu": 91.8,
				"lmarena_en_elo": 1490,
				"lmarena_zh_elo": 1405
			}
		},
		{
			"id": "gemini-2-5-pro",
			"name": "Gemini 2.5 Pro",
			"provider": "Google",
			"type": "proprietary",
			"release_date": "2025-03-25",
			"pricing": {
				"input_per_1m": 1.25,
				"output_per_1m": 5.0,
				"average_per_1m": 3.13
			},
			"performance": {
				"output_speed_tps": 165,
				"latency_ttft_ms": 520,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Excellent value with 1M token context window. Strong reasoning at lower cost but superseded by Gemini 3 on cutting-edge tasks.",
			"benchmark_scores": {
				"swe_bench": 63.8,
				"terminal_bench": 45.2,
				"lmarena_coding_elo": 1345,
				"live_code_bench": 65.5,
				"gpqa_diamond": 86.4,
				"aime_2024": 78.2,
				"arc_agi_2": 4.9,
				"lmarena_hard_elo": 1450,
				"bfcl": 79.5,
				"tau_bench": 52.8,
				"osworld": 42.5,
				"webdev_arena_elo": 1315,
				"lmarena_creative_elo": 1385,
				"lmarena_if_elo": 1443,
				"math_500": 85.5,
				"gsm8k": 96.5,
				"lmarena_math_elo": 1375,
				"mathvista": 72.5,
				"mmmu": 68.8,
				"lmarena_vision_elo": 1335,
				"mmlu": 89.5,
				"mmmlu": 84.2,
				"lmarena_en_elo": 1425,
				"lmarena_zh_elo": 1355
			}
		},
		{
			"id": "deepseek-v3-1",
			"name": "DeepSeek V3.1",
			"provider": "DeepSeek",
			"type": "open-source",
			"release_date": "2025-08-15",
			"pricing": {
				"input_per_1m": 0.27,
				"output_per_1m": 1.1,
				"average_per_1m": 0.69
			},
			"performance": {
				"output_speed_tps": 120,
				"latency_ttft_ms": 450,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Outstanding value as open-source model. Strong coding performance at fraction of proprietary model costs. Excellent for tool use.",
			"benchmark_scores": {
				"swe_bench": 66.0,
				"terminal_bench": 31.3,
				"lmarena_coding_elo": 1325,
				"live_code_bench": 58.5,
				"gpqa_diamond": 75.2,
				"aime_2024": 68.5,
				"arc_agi_2": 8.5,
				"lmarena_hard_elo": 1355,
				"bfcl": 80.5,
				"tau_bench": 55.2,
				"osworld": 38.5,
				"webdev_arena_elo": 1285,
				"lmarena_creative_elo": 1325,
				"lmarena_if_elo": 1365,
				"math_500": 78.5,
				"gsm8k": 94.2,
				"lmarena_math_elo": 1315,
				"mathvista": 65.5,
				"mmmu": 62.5,
				"lmarena_vision_elo": 1265,
				"mmlu": 88.5,
				"mmmlu": 82.5,
				"lmarena_en_elo": 1335,
				"lmarena_zh_elo": 1385
			}
		},
		{
			"id": "deepseek-r1",
			"name": "DeepSeek R1",
			"provider": "DeepSeek",
			"type": "open-source",
			"release_date": "2025-05-28",
			"pricing": {
				"input_per_1m": 0.55,
				"output_per_1m": 2.19,
				"average_per_1m": 1.37
			},
			"performance": {
				"output_speed_tps": 85,
				"latency_ttft_ms": 1200,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Reasoning-focused model with excellent math performance. Competitive with o3 on GPQA at fraction of cost.",
			"benchmark_scores": {
				"swe_bench": 57.6,
				"terminal_bench": 42.5,
				"lmarena_coding_elo": 1305,
				"live_code_bench": 73.3,
				"gpqa_diamond": 81.0,
				"aime_2024": 87.5,
				"arc_agi_2": 1.3,
				"lmarena_hard_elo": 1395,
				"bfcl": 75.8,
				"tau_bench": 48.5,
				"osworld": 35.2,
				"webdev_arena_elo": 1265,
				"lmarena_creative_elo": 1345,
				"lmarena_if_elo": 1355,
				"math_500": 88.5,
				"gsm8k": 97.5,
				"lmarena_math_elo": 1385,
				"mathvista": 68.2,
				"mmmu": 64.5,
				"lmarena_vision_elo": 1275,
				"mmlu": 90.8,
				"mmmlu": 84.0,
				"lmarena_en_elo": 1355,
				"lmarena_zh_elo": 1395
			}
		},
		{
			"id": "grok-4-1",
			"name": "Grok 4.1",
			"provider": "xAI",
			"type": "proprietary",
			"release_date": "2025-11-17",
			"pricing": {
				"input_per_1m": 3.0,
				"output_per_1m": 15.0,
				"average_per_1m": 9.0
			},
			"performance": {
				"output_speed_tps": 95,
				"latency_ttft_ms": 850,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "xAI's flagship successor to Grok 4. #2 on LMArena Overall (1465 Elo). 3x reduction in hallucinations vs Grok 4. Excels at emotional intelligence and conversation.",
			"benchmark_scores": {
				"swe_bench": 76.5,
				"terminal_bench": 54.0,
				"lmarena_coding_elo": 1375,
				"live_code_bench": 70.5,
				"gpqa_diamond": 89.5,
				"aime_2024": 96.0,
				"arc_agi_2": 15.9,
				"lmarena_hard_elo": 1475,
				"bfcl": 83.5,
				"tau_bench": 58.5,
				"osworld": 48.0,
				"webdev_arena_elo": 1335,
				"lmarena_creative_elo": 1410,
				"lmarena_if_elo": 1435,
				"math_500": 97.2,
				"gsm8k": 98.8,
				"lmarena_math_elo": 1455,
				"mathvista": 76.5,
				"mmmu": 73.0,
				"lmarena_vision_elo": 1365,
				"mmlu": 91.8,
				"mmmlu": 87.5,
				"lmarena_en_elo": 1465,
				"lmarena_zh_elo": 1390
			}
		},
		{
			"id": "grok-4-1-thinking",
			"name": "Grok 4.1 Thinking",
			"provider": "xAI",
			"type": "proprietary",
			"release_date": "2025-11-17",
			"pricing": {
				"input_per_1m": 3.0,
				"output_per_1m": 15.0,
				"average_per_1m": 9.0
			},
			"performance": {
				"output_speed_tps": 45,
				"latency_ttft_ms": 2200,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Extended thinking variant of Grok 4.1. #1 on LMArena Overall (1483 Elo). Deep reasoning with chain-of-thought. Top-tier emotional intelligence (1586 EQ-Bench Elo).",
			"benchmark_scores": {
				"swe_bench": 75,
				"terminal_bench": 56.5,
				"lmarena_coding_elo": 1392,
				"live_code_bench": 74.5,
				"gpqa_diamond": 91.0,
				"aime_2024": 97.5,
				"arc_agi_2": 22.5,
				"lmarena_hard_elo": 1495,
				"bfcl": 85.5,
				"tau_bench": 62.5,
				"osworld": 52.5,
				"webdev_arena_elo": 1358,
				"lmarena_creative_elo": 1430,
				"lmarena_if_elo": 1442,
				"math_500": 98.0,
				"gsm8k": 99.2,
				"lmarena_math_elo": 1475,
				"mathvista": 79.5,
				"mmmu": 76.0,
				"lmarena_vision_elo": 1385,
				"mmlu": 92.5,
				"mmmlu": 88.5,
				"lmarena_en_elo": 1483,
				"lmarena_zh_elo": 1410
			}
		},
		{
			"id": "llama-4-maverick",
			"name": "Llama 4 Maverick",
			"provider": "Meta",
			"type": "open-source",
			"release_date": "2025-04-05",
			"pricing": {
				"input_per_1m": 0.0,
				"output_per_1m": 0.0,
				"average_per_1m": 0.0
			},
			"performance": {
				"output_speed_tps": 155,
				"latency_ttft_ms": 380,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Open-weights model with strong coding. Excellent LiveCodeBench performance. Free for self-hosting, competitive with closed models.",
			"benchmark_scores": {
				"swe_bench": 58.5,
				"terminal_bench": 38.5,
				"lmarena_coding_elo": 1295,
				"live_code_bench": 43.4,
				"gpqa_diamond": 68.5,
				"aime_2024": 62.5,
				"arc_agi_2": 7.8,
				"lmarena_hard_elo": 1315,
				"bfcl": 77.6,
				"tau_bench": 48.5,
				"osworld": 32.5,
				"webdev_arena_elo": 1255,
				"lmarena_creative_elo": 1305,
				"lmarena_if_elo": 1335,
				"math_500": 61.2,
				"gsm8k": 90.5,
				"lmarena_math_elo": 1285,
				"mathvista": 58.5,
				"mmmu": 55.2,
				"lmarena_vision_elo": 1245,
				"mmlu": 85.5,
				"mmmlu": 78.5,
				"lmarena_en_elo": 1305,
				"lmarena_zh_elo": 1255
			}
		},
		{
			"id": "llama-4-scout",
			"name": "Llama 4 Scout",
			"provider": "Meta",
			"type": "open-source",
			"release_date": "2025-04-05",
			"pricing": {
				"input_per_1m": 0.0,
				"output_per_1m": 0.0,
				"average_per_1m": 0.0
			},
			"performance": {
				"output_speed_tps": 2600,
				"latency_ttft_ms": 120,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Blazing fast with 10M token context. Exceptional speed at 2600 tokens/s. Ideal for high-throughput applications.",
			"benchmark_scores": {
				"swe_bench": 42.5,
				"terminal_bench": 28.5,
				"lmarena_coding_elo": 1225,
				"live_code_bench": 35.5,
				"gpqa_diamond": 52.5,
				"aime_2024": 45.5,
				"arc_agi_2": 5.2,
				"lmarena_hard_elo": 1245,
				"bfcl": 72.5,
				"tau_bench": 38.5,
				"osworld": 25.5,
				"webdev_arena_elo": 1205,
				"lmarena_creative_elo": 1255,
				"lmarena_if_elo": 1285,
				"math_500": 52.5,
				"gsm8k": 85.5,
				"lmarena_math_elo": 1225,
				"mathvista": 48.5,
				"mmmu": 45.5,
				"lmarena_vision_elo": 1195,
				"mmlu": 78.5,
				"mmmlu": 72.5,
				"lmarena_en_elo": 1255,
				"lmarena_zh_elo": 1205
			}
		},
		{
			"id": "qwen3-235b",
			"name": "Qwen3 235B",
			"provider": "Alibaba",
			"type": "open-source",
			"release_date": "2025-04-29",
			"pricing": {
				"input_per_1m": 0.0,
				"output_per_1m": 0.0,
				"average_per_1m": 0.0
			},
			"performance": {
				"output_speed_tps": 75,
				"latency_ttft_ms": 850,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Flagship open-source MoE model. Outperforms DeepSeek-R1 on 17/23 benchmarks. State-of-the-art reasoning among open models.",
			"benchmark_scores": {
				"swe_bench": 62.5,
				"terminal_bench": 45.5,
				"lmarena_coding_elo": 1335,
				"live_code_bench": 68.5,
				"gpqa_diamond": 78.5,
				"aime_2024": 82.5,
				"arc_agi_2": 9.5,
				"lmarena_hard_elo": 1385,
				"bfcl": 82.5,
				"tau_bench": 52.5,
				"osworld": 38.5,
				"webdev_arena_elo": 1295,
				"lmarena_creative_elo": 1355,
				"lmarena_if_elo": 1380,
				"math_500": 85.5,
				"gsm8k": 96.5,
				"lmarena_math_elo": 1365,
				"mathvista": 68.5,
				"mmmu": 65.5,
				"lmarena_vision_elo": 1295,
				"mmlu": 89.5,
				"mmmlu": 85.5,
				"lmarena_en_elo": 1365,
				"lmarena_zh_elo": 1425
			}
		},
		{
			"id": "qwen3-32b",
			"name": "Qwen3 32B",
			"provider": "Alibaba",
			"type": "open-source",
			"release_date": "2025-04-29",
			"pricing": {
				"input_per_1m": 0.0,
				"output_per_1m": 0.0,
				"average_per_1m": 0.0
			},
			"performance": {
				"output_speed_tps": 145,
				"latency_ttft_ms": 320,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Largest dense Qwen3 model. Outperforms QwQ-32B while more efficient. Great for local deployment.",
			"benchmark_scores": {
				"swe_bench": 52.5,
				"terminal_bench": 35.5,
				"lmarena_coding_elo": 1275,
				"live_code_bench": 55.5,
				"gpqa_diamond": 65.5,
				"aime_2024": 68.5,
				"arc_agi_2": 6.8,
				"lmarena_hard_elo": 1315,
				"bfcl": 75.5,
				"tau_bench": 45.5,
				"osworld": 32.5,
				"webdev_arena_elo": 1245,
				"lmarena_creative_elo": 1295,
				"lmarena_if_elo": 1345,
				"math_500": 72.5,
				"gsm8k": 92.5,
				"lmarena_math_elo": 1295,
				"mathvista": 58.5,
				"mmmu": 55.5,
				"lmarena_vision_elo": 1245,
				"mmlu": 85.5,
				"mmmlu": 79.5,
				"lmarena_en_elo": 1305,
				"lmarena_zh_elo": 1365
			}
		},
		{
			"id": "qwen3-max-preview",
			"name": "Qwen3 Max Preview",
			"provider": "Alibaba",
			"type": "proprietary",
			"release_date": "2025-12-01",
			"pricing": {
				"input_per_1m": 1.2,
				"output_per_1m": 6.0,
				"average_per_1m": 3.6
			},
			"performance": {
				"output_speed_tps": 85,
				"latency_ttft_ms": 750,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Alibaba's flagship proprietary model in preview. Top 3 on LMArena. Strong coding (SWE-Bench 69.6% verified) and math performance. Thinking variant achieves 100% on AIME 2025.",
			"benchmark_scores": {
				"swe_bench": 69.6,
				"terminal_bench": null,
				"lmarena_coding_elo": 1365,
				"live_code_bench": 66.9,
				"gpqa_diamond": 77.8,
				"aime_2024": 60.7,
				"arc_agi_2": null,
				"lmarena_hard_elo": 1425,
				"bfcl": 80.5,
				"tau_bench": null,
				"osworld": null,
				"webdev_arena_elo": 1305,
				"lmarena_creative_elo": 1375,
				"lmarena_if_elo": 1395,
				"math_500": 82.5,
				"gsm8k": 95.5,
				"lmarena_math_elo": 1385,
				"mathvista": 68.5,
				"mmmu": 65.5,
				"lmarena_vision_elo": 1305,
				"mmlu": 88.5,
				"mmmlu": 83.5,
				"lmarena_en_elo": 1440,
				"lmarena_zh_elo": 1445
			}
		},
		{
			"id": "o3",
			"name": "OpenAI o3",
			"provider": "OpenAI",
			"type": "proprietary",
			"release_date": "2025-04-16",
			"pricing": {
				"input_per_1m": 10.0,
				"output_per_1m": 40.0,
				"average_per_1m": 25.0
			},
			"performance": {
				"output_speed_tps": 35,
				"latency_ttft_ms": 2500,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Premier reasoning model with extended thinking. Excels at complex multi-step problems. Higher latency due to deliberation.",
			"benchmark_scores": {
				"swe_bench": 71.7,
				"terminal_bench": 58.5,
				"lmarena_coding_elo": 1355,
				"live_code_bench": 72.5,
				"gpqa_diamond": 83.3,
				"aime_2024": 86.5,
				"arc_agi_2": 6.5,
				"lmarena_hard_elo": 1485,
				"bfcl": 78.5,
				"tau_bench": 55.5,
				"osworld": 48.5,
				"webdev_arena_elo": 1315,
				"lmarena_creative_elo": 1375,
				"lmarena_if_elo": 1415,
				"math_500": 95.8,
				"gsm8k": 98.8,
				"lmarena_math_elo": 1455,
				"mathvista": 78.5,
				"mmmu": 74.5,
				"lmarena_vision_elo": 1365,
				"mmlu": 90.5,
				"mmmlu": 85.5,
				"lmarena_en_elo": 1455,
				"lmarena_zh_elo": 1385
			}
		},
		{
			"id": "gemini-2-5-flash",
			"name": "Gemini 2.5 Flash",
			"provider": "Google",
			"type": "proprietary",
			"release_date": "2025-04-17",
			"pricing": {
				"input_per_1m": 0.15,
				"output_per_1m": 0.6,
				"average_per_1m": 0.38
			},
			"performance": {
				"output_speed_tps": 372,
				"latency_ttft_ms": 180,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Fastest major model at 372 tokens/s. Incredible value for speed-critical applications. Good reasoning at ultra-low cost.",
			"benchmark_scores": {
				"swe_bench": 48.5,
				"terminal_bench": 32.5,
				"lmarena_coding_elo": 1255,
				"live_code_bench": 52.5,
				"gpqa_diamond": 72.5,
				"aime_2024": 65.5,
				"arc_agi_2": 6.5,
				"lmarena_hard_elo": 1325,
				"bfcl": 74.5,
				"tau_bench": 45.5,
				"osworld": 35.5,
				"webdev_arena_elo": 1313,
				"lmarena_creative_elo": 1315,
				"lmarena_if_elo": 1375,
				"math_500": 78.5,
				"gsm8k": 94.5,
				"lmarena_math_elo": 1325,
				"mathvista": 65.5,
				"mmmu": 62.5,
				"lmarena_vision_elo": 1305,
				"mmlu": 86.5,
				"mmmlu": 80.5,
				"lmarena_en_elo": 1335,
				"lmarena_zh_elo": 1305
			}
		},
		{
			"id": "minimax-m2",
			"name": "MiniMax M2",
			"provider": "MiniMax",
			"type": "open-source",
			"release_date": "2025-10-27",
			"pricing": {
				"input_per_1m": 0.3,
				"output_per_1m": 1.2,
				"average_per_1m": 0.75
			},
			"performance": {
				"output_speed_tps": 100,
				"latency_ttft_ms": 450,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "MIT-licensed 230B MoE model excelling at coding tasks. Tops Multi-SWE-Bench and WebDev Arena. Fast inference at 100 TPS with excellent value for agentic workflows.",
			"benchmark_scores": {
				"swe_bench": 69.4,
				"terminal_bench": null,
				"lmarena_coding_elo": 1346,
				"live_code_bench": null,
				"gpqa_diamond": 78.0,
				"aime_2024": null,
				"arc_agi_2": null,
				"lmarena_hard_elo": 1346,
				"bfcl": null,
				"tau_bench": null,
				"osworld": null,
				"webdev_arena_elo": 1398,
				"lmarena_creative_elo": 1346,
				"lmarena_if_elo": null,
				"math_500": null,
				"gsm8k": null,
				"lmarena_math_elo": 1346,
				"mathvista": null,
				"mmmu": null,
				"lmarena_vision_elo": null,
				"mmlu": 95.0,
				"mmmlu": null,
				"lmarena_en_elo": 1346,
				"lmarena_zh_elo": null
			}
		},
		{
			"id": "kimi-k2",
			"name": "Kimi K2",
			"provider": "Moonshot AI",
			"type": "open-source",
			"release_date": "2025-07-10",
			"pricing": {
				"input_per_1m": 0.6,
				"output_per_1m": 2.5,
				"average_per_1m": 1.55
			},
			"performance": {
				"output_speed_tps": 85,
				"latency_ttft_ms": 650,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Open-source MoE model (1T total, 32B active) with strong agentic performance. Competitive with Claude Sonnet 4 on SWE-bench at fraction of cost. 256K context window.",
			"benchmark_scores": {
				"swe_bench": 65.8,
				"terminal_bench": null,
				"lmarena_coding_elo": null,
				"live_code_bench": 53.7,
				"gpqa_diamond": 75.1,
				"aime_2024": null,
				"arc_agi_2": null,
				"lmarena_hard_elo": null,
				"bfcl": null,
				"tau_bench": null,
				"osworld": null,
				"webdev_arena_elo": null,
				"lmarena_creative_elo": null,
				"lmarena_if_elo": null,
				"math_500": null,
				"gsm8k": null,
				"lmarena_math_elo": null,
				"mathvista": null,
				"mmmu": null,
				"lmarena_vision_elo": null,
				"mmlu": null,
				"mmmlu": null,
				"lmarena_en_elo": null,
				"lmarena_zh_elo": null
			}
		},
		{
			"id": "kimi-k2-thinking",
			"name": "Kimi K2 Thinking",
			"provider": "Moonshot AI",
			"type": "open-source",
			"release_date": "2025-11-06",
			"pricing": {
				"input_per_1m": 0.6,
				"output_per_1m": 2.5,
				"average_per_1m": 1.55
			},
			"performance": {
				"output_speed_tps": 45,
				"latency_ttft_ms": 1800,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Thinking agent with multi-step reasoning and tool use. Sets open-source records on SWE-bench (71.3%) and Humanity's Last Exam (44.9% w/tools). Can execute 200-300 sequential tool calls.",
			"benchmark_scores": {
				"swe_bench": 71.3,
				"terminal_bench": 47.1,
				"lmarena_coding_elo": 1479,
				"live_code_bench": 83.0,
				"gpqa_diamond": 75.1,
				"aime_2024": null,
				"arc_agi_2": null,
				"lmarena_hard_elo": 1448,
				"bfcl": null,
				"tau_bench": 93.0,
				"osworld": null,
				"webdev_arena_elo": 1350,
				"lmarena_creative_elo": 1402,
				"lmarena_if_elo": 1416,
				"math_500": null,
				"gsm8k": null,
				"lmarena_math_elo": 1428,
				"mathvista": null,
				"mmmu": null,
				"lmarena_vision_elo": null,
				"mmlu": null,
				"mmmlu": null,
				"lmarena_en_elo": 1444,
				"lmarena_zh_elo": 1462
			}
		},
		{
			"id": "longcat-flash-chat",
			"name": "Longcat Flash Chat",
			"provider": "Meituan",
			"type": "open-source",
			"release_date": "2025-12-01",
			"pricing": {
				"input_per_1m": 0.15,
				"output_per_1m": 0.75,
				"average_per_1m": 0.45
			},
			"performance": {
				"output_speed_tps": 100,
				"latency_ttft_ms": 350,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Meituan's MoE model (560B total, 27B active) optimized for agentic and conversational tasks. 128K context with strong tool use and Chinese language performance.",
			"benchmark_scores": {
				"swe_bench": 60.4,
				"terminal_bench": 39.51,
				"lmarena_coding_elo": 1474,
				"live_code_bench": 48.02,
				"gpqa_diamond": 73.23,
				"aime_2024": null,
				"arc_agi_2": null,
				"lmarena_hard_elo": null,
				"bfcl": 82.0,
				"tau_bench": 67.7,
				"osworld": null,
				"webdev_arena_elo": null,
				"lmarena_creative_elo": 1331,
				"lmarena_if_elo": 1390,
				"math_500": 96.4,
				"gsm8k": null,
				"lmarena_math_elo": 1429,
				"mathvista": null,
				"mmmu": null,
				"lmarena_vision_elo": null,
				"mmlu": 89.7,
				"mmmlu": null,
				"lmarena_en_elo": 1425,
				"lmarena_zh_elo": 1414
			}
		},
		{
			"id": "mistral-large-3",
			"name": "Mistral Large 3",
			"provider": "Mistral",
			"type": "open-source",
			"release_date": "2025-12-02",
			"pricing": {
				"input_per_1m": 0.5,
				"output_per_1m": 1.5,
				"average_per_1m": 1.0
			},
			"performance": {
				"output_speed_tps": 90,
				"latency_ttft_ms": 450,
				"source": "https://artificialanalysis.ai"
			},
			"editor_notes": "Mistral's flagship MoE model (675B total, 41B active). Top open-source coding model on LMArena. Excels at structured reasoning and math with competitive pricing.",
			"benchmark_scores": {
				"swe_bench": 55.0,
				"terminal_bench": 23.8,
				"lmarena_coding_elo": 1335,
				"live_code_bench": null,
				"gpqa_diamond": 43.9,
				"aime_2024": 53.3,
				"arc_agi_2": null,
				"lmarena_hard_elo": 1345,
				"bfcl": null,
				"tau_bench": null,
				"osworld": null,
				"webdev_arena_elo": null,
				"lmarena_creative_elo": 1335,
				"lmarena_if_elo": null,
				"math_500": 93.6,
				"gsm8k": null,
				"lmarena_math_elo": 1345,
				"mathvista": null,
				"mmmu": null,
				"lmarena_vision_elo": null,
				"mmlu": 73.1,
				"mmmlu": null,
				"lmarena_en_elo": 1418,
				"lmarena_zh_elo": null
			}
		}
	],
	"categories": [
		{
			"id": "coding",
			"name": "Coding",
			"emoji": "üíª",
			"weight": 0.25,
			"description": "Measures ability to write, edit, and debug code across multiple programming languages. Includes real-world GitHub issue resolution and competitive programming.",
			"benchmarks": [
				{
					"id": "swe_bench",
					"name": "SWE-Bench Verified",
					"type": "percentage",
					"weight": 0.5,
					"url": "https://swebench.com",
					"description": "Real-world GitHub issue resolution from popular Python repositories"
				},
				{
					"id": "terminal_bench",
					"name": "Terminal-Bench",
					"type": "percentage",
					"weight": 0.1,
					"url": "https://github.com/terminal-bench/terminal-bench",
					"description": "Terminal and command-line task completion"
				},
				{
					"id": "lmarena_coding_elo",
					"name": "LMArena Coding",
					"type": "elo",
					"weight": 0.3,
					"url": "https://lmarena.ai/leaderboard/text/coding",
					"description": "Human preference votes on coding tasks",
					"elo_range": { "min": 1100, "max": 1600 }
				},
				{
					"id": "live_code_bench",
					"name": "LiveCodeBench",
					"type": "percentage",
					"weight": 0.1,
					"url": "https://livecodebench.github.io",
					"description": "Competitive programming problems from recent contests"
				}
			]
		},
		{
			"id": "reasoning",
			"name": "Reasoning",
			"emoji": "üß†",
			"weight": 0.25,
			"description": "Complex problem-solving and PhD-level questions in science, mathematics, and logical reasoning.",
			"benchmarks": [
				{
					"id": "gpqa_diamond",
					"name": "GPQA Diamond",
					"type": "percentage",
					"weight": 0.4,
					"url": "https://artificialanalysis.ai/evaluations/gpqa-diamond",
					"description": "Graduate-level physics, chemistry, and biology questions"
				},
				{
					"id": "aime_2024",
					"name": "AIME 2024",
					"type": "percentage",
					"weight": 0.1,
					"url": "https://artofproblemsolving.com/wiki/index.php/AIME",
					"description": "American Invitational Mathematics Examination problems"
				},
				{
					"id": "arc_agi_2",
					"name": "ARC-AGI-2",
					"type": "percentage",
					"weight": 0.4,
					"url": "https://arcprize.org/leaderboard#leaderboard-table",
					"description": "ARC-AGI-2 is a contamination-resistant benchmark that tests abstract reasoning and generalization in AI systems."
				},
				{
					"id": "lmarena_hard_elo",
					"name": "LMArena Hard Prompts",
					"type": "elo",
					"weight": 0.2,
					"url": "https://lmarena.ai/pt/leaderboard/text/hard-prompts",
					"description": "Human preference on challenging reasoning tasks",
					"elo_range": { "min": 1100, "max": 1550 }
				}
			]
		},
		{
			"id": "agents",
			"name": "Agents & Tools",
			"emoji": "ü§ñ",
			"weight": 0.18,
			"description": "Function calling, computer use, and tool integration capabilities for autonomous operation.",
			"benchmarks": [
				{
					"id": "bfcl",
					"name": "Berkeley Function Calling (BFCL)",
					"type": "percentage",
					"weight": 0.3,
					"url": "https://gorilla.cs.berkeley.edu/leaderboard.html",
					"description": "Function calling accuracy across multiple languages"
				},
				{
					"id": "tau_bench",
					"name": "TAU-Bench",
					"type": "percentage",
					"weight": 0.25,
					"url": "https://taubench.com/#leaderboard",
					"description": "API orchestration and multi-step tool use"
				},
				{
					"id": "osworld",
					"name": "OSWorld",
					"type": "percentage",
					"weight": 0.25,
					"url": "https://os-world.github.io",
					"description": "Computer control and GUI interaction tasks"
				},
				{
					"id": "webdev_arena_elo",
					"name": "WebDev Arena",
					"type": "elo",
					"weight": 0.2,
					"url": "https://web.lmarena.ai/leaderboard",
					"description": "Web development task completion",
					"elo_range": { "min": 1100, "max": 1450 }
				}
			]
		},
		{
			"id": "conversation",
			"name": "Conversation",
			"emoji": "üí¨",
			"weight": 0.12,
			"description": "Creative writing, instruction following, and conversational quality.",
			"benchmarks": [
				{
					"id": "lmarena_creative_elo",
					"name": "LMArena Creative Writing",
					"type": "elo",
					"weight": 0.5,
					"url": "https://lmarena.ai/leaderboard/text/creative-writing",
					"description": "Human preference on creative writing tasks",
					"elo_range": { "min": 1100, "max": 1500 }
				},
				{
					"id": "lmarena_if_elo",
					"name": "LMArena Instruction Following",
					"type": "elo",
					"weight": 0.5,
					"url": "https://lmarena.ai/leaderboard/text/instruction-following",
					"description": "Human preference on instruction following tasks",
					"elo_range": { "min": 1100, "max": 1500 }
				}
			]
		},
		{
			"id": "math",
			"name": "Math",
			"emoji": "üî¢",
			"weight": 0.1,
			"description": "Mathematical problem solving from elementary to competition level.",
			"benchmarks": [
				{
					"id": "math_500",
					"name": "MATH-500",
					"type": "percentage",
					"weight": 0.5,
					"url": "https://github.com/hendrycks/math",
					"description": "Competition mathematics problems"
				},
				{
					"id": "gsm8k",
					"name": "GSM8K",
					"type": "percentage",
					"weight": 0.3,
					"url": "https://github.com/openai/grade-school-math",
					"description": "Grade school math word problems"
				},
				{
					"id": "lmarena_math_elo",
					"name": "LMArena Math",
					"type": "elo",
					"weight": 0.2,
					"url": "https://lmarena.ai/leaderboard/text/math",
					"description": "Human preference on math tasks",
					"elo_range": { "min": 1100, "max": 1500 }
				}
			]
		},
		{
			"id": "multimodal",
			"name": "Multimodal",
			"emoji": "üëÅÔ∏è",
			"weight": 0.07,
			"description": "Vision and text understanding, including charts, diagrams, and images.",
			"benchmarks": [
				{
					"id": "mathvista",
					"name": "MathVista",
					"type": "percentage",
					"weight": 0.3,
					"url": "https://mathvista.github.io",
					"description": "Math reasoning with visual context"
				},
				{
					"id": "mmmu",
					"name": "MMMU",
					"type": "percentage",
					"weight": 0.3,
					"url": "https://mmmu-benchmark.github.io",
					"description": "Massive multi-discipline multimodal understanding"
				},
				{
					"id": "lmarena_vision_elo",
					"name": "LMArena Vision",
					"type": "elo",
					"weight": 0.4,
					"url": "https://lmarena.ai/leaderboard/vision",
					"description": "Human preference on vision tasks",
					"elo_range": { "min": 1100, "max": 1450 }
				}
			]
		},
		{
			"id": "multilingual",
			"name": "Multilingual",
			"emoji": "üåê",
			"weight": 0.03,
			"description": "Performance across multiple languages beyond English.",
			"benchmarks": [
				{
					"id": "mmlu",
					"name": "MMLU",
					"type": "percentage",
					"weight": 0.4,
					"url": "https://github.com/hendrycks/test",
					"description": "Massive Multitask Language Understanding"
				},
				{
					"id": "mmmlu",
					"name": "MMMLU",
					"type": "percentage",
					"weight": 0.4,
					"url": "https://huggingface.co/datasets/openai/mmmlu",
					"description": "Multilingual MMLU across 14 languages"
				},
				{
					"id": "lmarena_en_elo",
					"name": "LMArena English",
					"type": "elo",
					"weight": 0.1,
					"url": "https://lmarena.ai/leaderboard",
					"description": "Human preference on English tasks",
					"elo_range": { "min": 1100, "max": 1500 }
				},
				{
					"id": "lmarena_zh_elo",
					"name": "LMArena Chinese",
					"type": "elo",
					"weight": 0.1,
					"url": "https://lmarena.ai/leaderboard",
					"description": "Human preference on Chinese tasks",
					"elo_range": { "min": 1100, "max": 1450 }
				}
			]
		}
	]
}
