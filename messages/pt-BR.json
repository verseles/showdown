{
	"$schema": "https://inlang.com/schema/inlang-message-format",
	"site_title": "Showdown - Rankings e comparação completa de LLM",
	"site_description": "Compare os melhores modelos de linguagem AI em programação, raciocínio, agentes e mais. Rankings transparentes de mais de 20 benchmarks.",
	"og_title": "Showdown - Rankings de LLM",
	"og_description": "Comparação transparente de modelos AI agregando SWE-Bench, GPQA, LMArena e mais.",
	"header_title": "Showdown",
	"header_tagline": "Rankings de LLM",
	"header_showing": "Exibindo {count} de {total} modelos",
	"header_updated": "Atualizado: {date}",
	"aria_select_language": "Selecionar idioma",
	"aria_star_github": "Dar estrela no GitHub",
	"aria_switch_light": "Mudar para modo claro",
	"aria_switch_dark": "Mudar para modo escuro",
	"aria_switch_system": "Mudar para modo do sistema",
	"aria_favorite": "Favorito",
	"aria_add_favorite": "Adicionar aos favoritos",
	"aria_remove_favorite": "Remover dos favoritos",
	"aria_show_less": "Mostrar menos",
	"aria_show_more": "Mostrar mais",
	"filter_provider": "Provedor",
	"filter_provider_placeholder": "Selecionar provedores",
	"filter_type": "Tipo",
	"filter_proprietary": "Proprietário",
	"filter_opensource": "Código aberto",
	"filter_favorites_only": "Apenas favoritos",
	"filter_reset": "Redefinir",
	"filter_columns": "Colunas",
	"filter_visible_columns": "Colunas visíveis",
	"column_rank": "Posição",
	"column_rank_score": "Posição e pontuação",
	"column_provider": "Provedor",
	"column_model": "Modelo",
	"column_type": "Tipo",
	"column_price": "Preço",
	"column_speed": "Velocidade",
	"column_latency": "Latência",
	"column_released": "Lançamento",
	"column_release_date": "Data de lançamento",
	"type_proprietary": "Proprietário",
	"type_opensource": "Código aberto",
	"type_open": "Aberto",
	"category_coding": "Programação",
	"category_reasoning": "Raciocínio",
	"category_agents": "Agentes e ferramentas",
	"category_conversation": "Conversação",
	"category_math": "Matemática",
	"category_multimodal": "Multimodal",
	"tooltip_weight": "Peso: {percentage}%",
	"tooltip_benchmarks_available": "{available}/{total} benchmarks disponíveis",
	"tooltip_pricing": "Preços: {model}",
	"tooltip_input": "Entrada",
	"tooltip_output": "Saída",
	"tooltip_average": "Média",
	"tooltip_per_1m_tokens": "/ 1M tokens",
	"tooltip_average_formula": "Média = (Entrada + Saída) / 2",
	"card_price_avg": "Preço (méd.)",
	"card_input": "Entrada",
	"card_output": "Saída",
	"card_speed": "Velocidade",
	"card_latency": "Latência",
	"card_released": "Lançamento",
	"card_show_less": "Mostrar menos",
	"card_show_all": "Mostrar tudo",
	"footer_data_sourced": "Dados obtidos de benchmarks públicos.",
	"footer_contribute": "Contribuir no GitHub",
	"github_star": "Star",
	"aria_toggle_sort_order": "Alternar ordem de classificação",
	"filter_search_placeholder": "Pesquisar modelos...",
	"sort_by": "Ordenar por",
	"category_knowledge": "Conhecimento",
	"bench_swe": "SWE-Bench Verificado",
	"bench_terminal": "Terminal-Bench",
	"bench_lmarena_coding": "LMArena Programação",
	"bench_live_code": "LiveCodeBench",
	"bench_gpqa": "GPQA Diamante",
	"bench_arc": "ARC-AGI-2",
	"bench_livebench": "LiveBench",
	"bench_hle": "Último Exame da Humanidade",
	"bench_lmarena_hard": "LMArena Difícil",
	"bench_bfcl": "BFCL",
	"bench_tau": "TAU-Bench",
	"bench_osworld": "OSWorld",
	"bench_webdev": "WebDev Arena",
	"bench_lmarena_creative": "LMArena Criativo",
	"bench_lmarena_if": "LMArena Instruções",
	"bench_math500": "MATH-500",
	"bench_aime": "AIME",
	"bench_lmarena_math": "LMArena Matemática",
	"bench_frontier": "FrontierMath",
	"bench_mathvista": "MathVista",
	"bench_mmmu": "MMMU",
	"bench_mmmu_pro": "MMMU-Pro",
	"bench_lmarena_vision": "LMArena Visão",
	"bench_mmlu_pro": "MMLU-Pro",
	"bench_mmmlu": "MMMLU",
	"bench_lmarena_en": "LMArena Inglês",
	"bench_lmarena_zh": "LMArena Chinês",
	"category_description_coding": "Mede a capacidade de escrever, editar e depurar código em várias linguagens de programação.",
	"category_description_reasoning": "Resolução de problemas complexos e questões de nível de doutorado em ciências, matemática e raciocínio lógico.",
	"category_description_agents": "Capacidades de chamada de função, uso de computador e integração de ferramentas para operação autônoma.",
	"category_description_conversation": "Escrita criativa, seguimento de instruções e qualidade conversacional.",
	"category_description_math": "Resolução de problemas matemáticos desde o nível elementar até o de competição.",
	"category_description_multimodal": "Compreensão de visão e texto, incluindo gráficos, diagramas e imagens.",
	"category_description_knowledge": "Avaliação de conhecimento em diversos temas e idiomas.",
	"bench_description_swe_bench": "Resolução de problemas reais do GitHub em repositórios Python populares.",
	"bench_description_terminal_bench": "Conclusão de tarefas de terminal e linha de comando.",
	"bench_description_lmarena_coding_elo": "Votos de preferência humana em tarefas de programação.",
	"bench_description_live_code_bench": "Problemas de programação competitiva de concursos recentes.",
	"bench_description_aider_polyglot": "Benchmark de programação multilíngue testando geração e edição de código.",
	"bench_description_gpqa_diamond": "Questões de física, química e biologia de nível de pós-graduação.",
	"bench_description_arc_agi_2": "Teste de raciocínio abstrato e generalização resistente à contaminação.",
	"bench_description_livebench": "Benchmark atualizado continuamente e resistente à contaminação.",
	"bench_description_humanity_last_exam": "Questões de nível de doutorado em mais de 100 domínios.",
	"bench_description_lmarena_hard_elo": "Preferência humana em tarefas de raciocínio desafiadoras.",
	"bench_description_bfcl": "Precisão de chamada de função em vários idiomas.",
	"bench_description_tau_bench": "Avaliação de agentes de IA colaborativos.",
	"bench_description_osworld": "Tarefas de controle de computador e interação GUI.",
	"bench_description_webdev_arena_elo": "Conclusão de tarefas de desenvolvimento web.",
	"bench_description_lmarena_creative_elo": "Preferência humana em tarefas de escrita criativa.",
	"bench_description_lmarena_if_elo": "Preferência humana em seguimento de instruções.",
	"bench_description_math_500": "Problemas matemáticos de competição.",
	"bench_description_aime": "Problemas do American Invitational Mathematics Examination.",
	"bench_description_lmarena_math_elo": "Preferência humana em tarefas de matemática.",
	"bench_description_frontiermath": "Problemas matemáticos de nível de pesquisa.",
	"bench_description_mathvista": "Raciocínio matemático com contexto visual.",
	"bench_description_mmmu": "Compreensão multimodal massiva multi-disciplinar.",
	"bench_description_mmmu_pro": "Avaliação MMMU aprimorada.",
	"bench_description_lmarena_vision_elo": "Preferência humana em tarefas de visão.",
	"bench_description_mmlu_pro": "MMLU aprimorado com foco em raciocínio.",
	"bench_description_mmmlu": "MMLU multilíngue em 14 idiomas.",
	"bench_description_lmarena_en_elo": "Preferência humana em tarefas em inglês.",
	"bench_description_lmarena_zh_elo": "Preferência humana em tarefas em chinês.",
	"imputed_indicator_title": "Contém valores estimados",
	"imputed_notice": "* = Valor estimado (calculado a partir de outros benchmarks nesta categoria)",
	"superior_imputed_notice": "* = Estimado como superior ao modelo base (verde)",
	"confidence_low": "Baixa",
	"confidence_medium": "Média",
	"confidence_high": "Alta",
	"bench_livebench_reasoning": "LiveBench Reasoning",
	"bench_livebench_coding": "LiveBench Coding",
	"bench_livebench_agentic_coding": "LiveBench Agentic Coding",
	"bench_livebench_math": "LiveBench Math",
	"bench_livebench_data_analysis": "LiveBench Data Analysis",
	"bench_livebench_language": "LiveBench Language",
	"bench_livebench_if": "LiveBench IF",
	"filter_date_range": "Data de lançamento",
	"filter_date_all": "Todo o período",
	"filter_date_30d": "Últimos 30 dias",
	"filter_date_90d": "Últimos 90 dias",
	"filter_date_180d": "Últimos 6 meses"
}
